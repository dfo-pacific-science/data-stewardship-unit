[
  {
    "objectID": "tutorials/support_controlled_vocabs.html",
    "href": "tutorials/support_controlled_vocabs.html",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "",
    "text": "Use this workflow when you need to add, revise, or align controlled terms used in salmon datasets, salmon data package metadata, and SPSR uploads.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#canonical-source-and-namespace",
    "href": "tutorials/support_controlled_vocabs.html#canonical-source-and-namespace",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Canonical source and namespace",
    "text": "Canonical source and namespace\n\nCanonical ontology docs: GC DFO Salmon Ontology\nCanonical contributor conventions: Ontology Conventions Guidebook\nCanonical namespace: https://w3id.org/gcdfo/salmon#",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-1-identify-candidate-terms-from-real-data",
    "href": "tutorials/support_controlled_vocabs.html#step-1-identify-candidate-terms-from-real-data",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 1 ‚Äî Identify candidate terms from real data",
    "text": "Step 1 ‚Äî Identify candidate terms from real data\nFor each candidate term, capture:\n\nproposed label\nplain-language definition\nwhere it appears (column name, code list, method field)\nexpected usage context (FSAR, SPSR wizard, reporting)\nsource/provenance reference (document, method spec, report)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-2-check-existing-ontology-and-vocabularies-first",
    "href": "tutorials/support_controlled_vocabs.html#step-2-check-existing-ontology-and-vocabularies-first",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 2 ‚Äî Check existing ontology and vocabularies first",
    "text": "Step 2 ‚Äî Check existing ontology and vocabularies first\nBefore proposing anything new:\n\nSearch WIDOCO for exact or close matches.\nCheck existing concept schemes and synonyms.\nDecide whether your candidate is:\n\nexisting term (reuse as-is)\nsynonym/label variant (add lexical support)\ngenuinely new concept (new term needed)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-3-choose-the-right-modeling-type",
    "href": "tutorials/support_controlled_vocabs.html#step-3-choose-the-right-modeling-type",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 3 ‚Äî Choose the right modeling type",
    "text": "Step 3 ‚Äî Choose the right modeling type\n\nUse OWL classes/properties for domain entities and logical relations.\nUse SKOS concepts for controlled code lists and enumerations.\nDo not model the same concept as both OWL class and SKOS concept unless there is an explicit architecture decision.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-4-prepare-a-term-request-package",
    "href": "tutorials/support_controlled_vocabs.html#step-4-prepare-a-term-request-package",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 4 ‚Äî Prepare a term request package",
    "text": "Step 4 ‚Äî Prepare a term request package\nMinimum required fields:\n\npreferred label\ndefinition (IAO:0000115 for OWL or skos:definition for SKOS)\nprovenance (IAO:0000119 and/or dcterms:source when available)\nowner/theme/module\nexample usage\nrequested placement (parent class, scheme membership, broader term)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-5-review-and-decision",
    "href": "tutorials/support_controlled_vocabs.html#step-5-review-and-decision",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 5 ‚Äî Review and decision",
    "text": "Step 5 ‚Äî Review and decision\nApply a light governance check:\n\nno duplicate meaning\ndefinition is testable and non-circular\nnaming is consistent\naligns with competency questions and downstream workflows\nimpact on salmon data package and SPSR workflows is documented",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-6-implement-and-publish",
    "href": "tutorials/support_controlled_vocabs.html#step-6-implement-and-publish",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 6 ‚Äî Implement and publish",
    "text": "Step 6 ‚Äî Implement and publish\nAfter approval:\n\nAdd/update ontology term(s).\nRegenerate ontology docs and release artifacts.\nUpdate DSU docs if contributor guidance changed.\nUpdate downstream mappings/templates as needed.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#step-7-sync-downstream-systems-including-spsr",
    "href": "tutorials/support_controlled_vocabs.html#step-7-sync-downstream-systems-including-spsr",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Step 7 ‚Äî Sync downstream systems (including SPSR)",
    "text": "Step 7 ‚Äî Sync downstream systems (including SPSR)\nWhen term changes affect intake:\n\nupdate salmon data package templates (column_dictionary.csv, codes.csv examples)\nupdate SPSR upload guidance and controlled-value references\nrun one end-to-end test through SPSR wizard with the updated term usage\n\nUseful SPSR references:\n\nRepo: https://github.com/dfo-pacific-science/salmon-population-summary-repository\nIn-app documentation hub: https://spsr.dfo-mpo.gc.ca/documentation/\nUpload wizard: https://spsr.dfo-mpo.gc.ca/wizard/1/",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#quick-checklist",
    "href": "tutorials/support_controlled_vocabs.html#quick-checklist",
    "title": "Controlled Vocabulary Governance Workflow",
    "section": "Quick checklist",
    "text": "Quick checklist\n\nExisting term reuse checked first\nCorrect OWL vs SKOS modeling choice\nDefinition + provenance complete\nDownstream salmon data package/SPSR impacts assessed\nDocs and examples updated",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Controlled Vocabulary Governance Workflow"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools for Data Stewardship",
    "section": "",
    "text": "This page lists practical tools used across the FADS Open Science Hub workflows.",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Tools"
    ]
  },
  {
    "objectID": "tools.html#core-platforms",
    "href": "tools.html#core-platforms",
    "title": "Tools for Data Stewardship",
    "section": "Core platforms",
    "text": "Core platforms\n\nSalmon Population Summary Repository (SPSR)\nOperational intake and exploration platform for salmon population data, including wizard-based contribution flows.\n\nDocs hub: https://spsr.dfo-mpo.gc.ca/documentation/\nUpload wizard: https://spsr.dfo-mpo.gc.ca/wizard/1/\nRepo: https://github.com/dfo-pacific-science/salmon-population-summary-repository\n\n\n\nDFO Salmon Ontology\nCanonical semantic source for terms and identifiers.\n\nWIDOCO docs: https://w3id.org/gcdfo/salmon\nRepo: https://github.com/dfo-pacific-science/dfo-salmon-ontology",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Tools"
    ]
  },
  {
    "objectID": "tools.html#packaging-and-validation",
    "href": "tools.html#packaging-and-validation",
    "title": "Tools for Data Stewardship",
    "section": "Packaging and validation",
    "text": "Packaging and validation\n\nSalmon data package (SDP)\nMetadata package structure used across standards and intake workflows.\n\nSpec overview: Salmon data package specification\nCanonical markdown spec: https://github.com/dfo-pacific-science/smn-data-pkg/blob/main/SPECIFICATION.md\nIntake guidance: Salmon data package + SPSR intake path\n\n\n\nmetasalmon\nR-based tooling for validating and working with salmon data package metadata.\n\nHub context: R Packages",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Tools"
    ]
  },
  {
    "objectID": "tools.html#internal-coordination-and-support",
    "href": "tools.html#internal-coordination-and-support",
    "title": "Tools for Data Stewardship",
    "section": "Internal coordination and support",
    "text": "Internal coordination and support\n\nDSU standards hub: Salmon Data Standards Hub\nContact: mailto:FADSDataStewardship-GestiondesdonneesSFDA@dfo-mpo.gc.ca\nInternal wiki: https://086gc.sharepoint.com/sites/FisheryAssessmentDataSectionWiki/SitePages/Data-Stewardship-Unit(1).aspx",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Tools"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html",
    "href": "reference_info/publishing_data_externally.html",
    "title": "Hosting Data and Code",
    "section": "",
    "text": "This guide is intended for scientists and technical staff in the DFO Pacific Region Science Branch. It outlines what is allowed, restricted, or prohibited when using internal and external platforms (e.g., SharePoint, GitHub, Zenodo, Google Drive) to manage and share DFO Science data, code, and metadata, including during Fisheries Science Advisory Report (FSAR) workflows.\nFor FSAR workflows, we recommend authors store data in the SharePoint site for the Pacific Salmon Data MS Team. All data should be accompanied by a data dictionary and README. GitHub can be used to host analysis code.\nTable 1. External Platform Use Guidelines for three data classifications of both data and code: Unclassified, Protected A, and Protected B. Note that this table does not cover Protected C or Classified data which are not permitted on external platforms.",
    "crumbs": [
      "Home",
      "Publishing & Open Science",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#data-classification-quick-reference",
    "href": "reference_info/publishing_data_externally.html#data-classification-quick-reference",
    "title": "Hosting Data and Code",
    "section": "üîê Data Classification Quick Reference",
    "text": "üîê Data Classification Quick Reference\n\nUnclassified: No expected harm from disclosure. Includes open data, public metadata, and code that contains no sensitive logic or credentials.\n\nExamples: Published R packages, fish counts aggregated by region and year, habitat model source code without raw data. Note, however, even unclassified data has to be approved for release by the Regional Director of Science.\n\nProtected A: Could cause low-level injury if disclosed (e.g., minor confidentiality breach, operational disruption).\n\nExamples: Unpublished but non-sensitive biological metrics tied to location or project; fishing effort by vessel class if potentially re-identifiable; unreviewed site-specific model outputs.\n\nProtected B: Could cause serious injury if disclosed (e.g., violation of legal obligations, significant economic harm).\n\nExamples: Commercial catch by vessel or license number; sensitive species occurrences; data provided under confidentiality agreements.\n\n\n\nüîó Refer to the Policy on Government Security for classification definitions and examples.\nüîé Unsure if data is protected? Consult your divisional data contact or ATIP office.",
    "crumbs": [
      "Home",
      "Publishing & Open Science",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#platform-by-platform-guidance",
    "href": "reference_info/publishing_data_externally.html#platform-by-platform-guidance",
    "title": "Hosting Data and Code",
    "section": "üß≠ Platform-by-Platform Guidance",
    "text": "üß≠ Platform-by-Platform Guidance\n\n‚úÖ GitHub (Public and Private)\n\nRecommended for: Non-sensitive, unclassified code, metadata schemas, documentation, workflows.\nAllowed: Unclassified data/code only. Data must go through the approvals and internal publishing process before being posted publicly.\nPrivate repos: Use for drafting and collaboration before public release. Do not use for long-term data storage.\nFSAR Guidance: Releasing data in a public GitHub repo before approval by CSAS and before the release has been approved by the Regional Director of Science is a breach of the Access to Information Act and/or Privacy Act.\n\n\nüîó TBS Directive on Open Government, Section 6.1 ‚Äì Open information by default, subject to security/privacy. üîó DFO Policy for Scientific Data - ‚Äúit is the responsibility of the Regional Director of Science to designate specific Data as classified for preventing its open sharing.‚Äù\n\n\n\n\nüö´ GitHub Codespaces / Copilot\n\nCodespaces: Cloud-hosted dev environments. Not GC-approved for sensitive data. Fine for non-sensitive code.\nCopilot: Do not enter Protected data/code. Avoid unless using for generic logic.\n\n\nüîó TBS Guide on Generative AI\n\n\n\n\n‚úÖ Canadian Integrated Ocean Observing System, Zenodo, Dryad, Figshare\n\nUse after FSAR or publication approval.\nGood for creating DOIs and enabling citation and disseminating broadly.\nData must be fully unclassified, reviewed, and approved.\n\n\n\n\n‚úÖ GC Open Data Portal\n\nOfficial repository for open DFO datasets.\nRequires metadata, data steward approval, and Open Government Licence.\n\n\nüîó Open Government Licence\nüîó Open Data Publishing Guidance",
    "crumbs": [
      "Home",
      "Publishing & Open Science",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#internal-dfo-requirements",
    "href": "reference_info/publishing_data_externally.html#internal-dfo-requirements",
    "title": "Hosting Data and Code",
    "section": "üßæ Internal DFO Requirements",
    "text": "üßæ Internal DFO Requirements\n\nüì§ Internal release process\n\nManaged by the Marine Spatial Data Infrastructure System and the Enterprise Data Hub Follow the official steps here\n\nFor steps specific to approvals and publishing, see Step 4 of the Release Criteria Checklist.\n\n\n‚è≥ When to Release?\n\nFor FSAR-related data: Only after advice is finalized and report is approved.\nFor code/metadata tools: May be shared publicly once confirmed to contain no sensitive content.\n\n\nüîó CSAS Procedural Manuals ‚Äì Supporting material must not be released until advice is finalized.",
    "crumbs": [
      "Home",
      "Publishing & Open Science",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#need-help",
    "href": "reference_info/publishing_data_externally.html#need-help",
    "title": "Hosting Data and Code",
    "section": "üì¨ Need Help?",
    "text": "üì¨ Need Help?\n\nData Classification or Release Questions: DFO.PACIFIC.SCIENCE.DATA@dfo-mpo.gc.ca\nCSAS Advice Process Questions: Contact your regional CSAS Coordinator\nOpen Science & GitHub Use: Data Stewardship Unit\n\n\n\n‚ÑπÔ∏è This document will be updated periodically. Please check back for revised guidance as policies evolve.",
    "crumbs": [
      "Home",
      "Publishing & Open Science",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html",
    "href": "reference_info/ontology/request-new-term.html",
    "title": "How to Request a New Ontology Term",
    "section": "",
    "text": "Use this process when you cannot find a suitable existing term in the DFO Salmon Ontology.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html#before-requesting",
    "href": "reference_info/ontology/request-new-term.html#before-requesting",
    "title": "How to Request a New Ontology Term",
    "section": "Before requesting",
    "text": "Before requesting\n\nSearch existing ontology docs: GC DFO Salmon Ontology\nCheck conventions: Ontology Conventions Guidebook\nConfirm the concept is not already represented under another label/synonym.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html#minimum-request-package",
    "href": "reference_info/ontology/request-new-term.html#minimum-request-package",
    "title": "How to Request a New Ontology Term",
    "section": "Minimum request package",
    "text": "Minimum request package\nInclude:\n\nproposed label\nplain-language definition\nconcept type (OWL class/property or SKOS concept)\nexpected parent class or concept scheme\nusage example from a real workflow (FSAR/SPSR/salmon data package)\nprovenance/source reference (if available)\nwhy existing terms are insufficient",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html#decision-criteria",
    "href": "reference_info/ontology/request-new-term.html#decision-criteria",
    "title": "How to Request a New Ontology Term",
    "section": "Decision criteria",
    "text": "Decision criteria\nA strong request should:\n\nsolve a real recurring use case\navoid semantic duplication\nfit existing naming and modeling conventions\nimprove interoperability or data quality",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html#suggested-issue-template",
    "href": "reference_info/ontology/request-new-term.html#suggested-issue-template",
    "title": "How to Request a New Ontology Term",
    "section": "Suggested issue template",
    "text": "Suggested issue template\n## Proposed term\nLabel: ...\nType: OWL class / OWL property / SKOS concept\n\n## Definition\n...\n\n## Placement\nParent class or concept scheme: ...\n\n## Example usage\n...\n\n## Why existing terms do not work\n...\n\n## Source/provenance\n...",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/request-new-term.html#after-approval",
    "href": "reference_info/ontology/request-new-term.html#after-approval",
    "title": "How to Request a New Ontology Term",
    "section": "After approval",
    "text": "After approval\n\nAdd term in ontology repository.\nRegenerate docs/release artifacts.\nUpdate DSU guidance pages that reference this area.\nUpdate salmon data package/SPSR mappings if intake workflows are affected.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "How to Request a New Ontology Term"
    ]
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html",
    "title": "SKOS Vocabularies",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html#when-to-use-skos",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html#when-to-use-skos",
    "title": "SKOS Vocabularies",
    "section": "When to use SKOS",
    "text": "When to use SKOS\nUse SKOS for controlled values, code lists, and terminology schemes where you need consistent labels and lightweight hierarchy.\nUse OWL (not SKOS) when you need class-level logical reasoning."
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html#required-pattern-for-a-skos-concept",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html#required-pattern-for-a-skos-concept",
    "title": "SKOS Vocabularies",
    "section": "Required pattern for a SKOS concept",
    "text": "Required pattern for a SKOS concept\n\na skos:Concept\nskos:prefLabel \"...\"@en\nskos:inScheme :YourScheme\nrecommended: skos:definition \"...\"@en\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt;\n\nOptional:\n\nskos:altLabel\nskos:broader / skos:narrower / skos:related\nskos:notation for machine code (typed literal)\nIAO:0000119 and dcterms:source for provenance"
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html#template-example",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html#template-example",
    "title": "SKOS Vocabularies",
    "section": "Template example",
    "text": "Template example\nex:DFOCode a rdfs:Datatype .\n\n:SonarCounting a skos:Concept ;\n  skos:inScheme :EnumerationMethodScheme ;\n  skos:prefLabel \"Sonar counting\"@en ;\n  skos:definition \"Counting fish using active acoustic methods.\"@en ;\n  skos:notation \"ESC-001\"^^ex:DFOCode ;\n  rdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; ."
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html#common-mistakes-to-avoid",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html#common-mistakes-to-avoid",
    "title": "SKOS Vocabularies",
    "section": "Common mistakes to avoid",
    "text": "Common mistakes to avoid\n\nputting machine codes into skos:prefLabel\nforgetting skos:inScheme\nusing SKOS concepts as OWL classes in logical axioms\nadding multiple skos:prefLabel values for the same language"
  },
  {
    "objectID": "reference_info/ontology/conventions/skos-vocabularies.html#quick-checks",
    "href": "reference_info/ontology/conventions/skos-vocabularies.html#quick-checks",
    "title": "SKOS Vocabularies",
    "section": "Quick checks",
    "text": "Quick checks\n\none preferred label per language\nbelongs to the right scheme\ndefinition is present for non-obvious concepts\nnotation datatype is explicit when codes are used"
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html",
    "href": "reference_info/ontology/conventions/quick-reference.html",
    "title": "Quick Reference",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html#essential-elements-minimum",
    "href": "reference_info/ontology/conventions/quick-reference.html#essential-elements-minimum",
    "title": "Quick Reference",
    "section": "Essential elements (minimum)",
    "text": "Essential elements (minimum)\n\nOWL terms (classes and properties)\n\nrdfs:label \"...\"@en\nIAO:0000115 \"...\"@en\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt;\noptional provenance: IAO:0000119 and/or dcterms:source\n\n\n\nSKOS concepts (controlled vocabulary terms)\n\nskos:prefLabel \"...\"@en\nskos:inScheme :YourScheme\nrecommended: skos:definition \"...\"@en\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt;\noptional code: skos:notation \"CODE\"^^ex:CodeDatatype"
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html#naming-conventions",
    "href": "reference_info/ontology/conventions/quick-reference.html#naming-conventions",
    "title": "Quick Reference",
    "section": "Naming conventions",
    "text": "Naming conventions\n\nClasses: PascalCase (e.g., EscapementMeasurement)\nProperties: lowerCamelCase (e.g., aboutStock)\nSKOS concepts: PascalCase (e.g., SonarCounting)"
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html#key-decision-rule-skos-vs-owl",
    "href": "reference_info/ontology/conventions/quick-reference.html#key-decision-rule-skos-vs-owl",
    "title": "Quick Reference",
    "section": "Key decision rule: SKOS vs OWL",
    "text": "Key decision rule: SKOS vs OWL\n\nuse SKOS for code lists, picklists, and lexical hierarchies\nuse OWL for logical class models and inferencing\navoid modeling the same concept as both unless explicitly approved"
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html#non-negotiables",
    "href": "reference_info/ontology/conventions/quick-reference.html#non-negotiables",
    "title": "Quick Reference",
    "section": "Non-negotiables",
    "text": "Non-negotiables\n\nkeep ontology schema separate from instance data\nuse full canonical IRIs\ndo not put codes in labels\nuse provenance fields consistently"
  },
  {
    "objectID": "reference_info/ontology/conventions/quick-reference.html#quality-gate-checklist",
    "href": "reference_info/ontology/conventions/quick-reference.html#quality-gate-checklist",
    "title": "Quick Reference",
    "section": "Quality gate checklist",
    "text": "Quality gate checklist\n\nrequired annotations present\ndefinition is clear and non-circular\nno duplicate concept introduced\nplacement in hierarchy/scheme is correct\nexamples and provenance included where useful"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html",
    "href": "reference_info/ontology/conventions/owl-properties.html",
    "title": "OWL Properties",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#choose-property-type-correctly",
    "href": "reference_info/ontology/conventions/owl-properties.html#choose-property-type-correctly",
    "title": "OWL Properties",
    "section": "Choose property type correctly",
    "text": "Choose property type correctly\n\nowl:ObjectProperty for links to another entity\nowl:DatatypeProperty for literal values (numbers, strings, dates)"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#required-pattern",
    "href": "reference_info/ontology/conventions/owl-properties.html#required-pattern",
    "title": "OWL Properties",
    "section": "Required pattern",
    "text": "Required pattern\nFor either property type:\n\ndeclaration (a owl:ObjectProperty or a owl:DatatypeProperty)\nrdfs:label \"...\"@en\nIAO:0000115 \"...\"@en\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt;\n\nOptional but recommended:\n\nIAO:0000119, dcterms:source, IAO:0000112"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#domainrange-guidance",
    "href": "reference_info/ontology/conventions/owl-properties.html#domainrange-guidance",
    "title": "OWL Properties",
    "section": "Domain/range guidance",
    "text": "Domain/range guidance\nUse rdfs:domain and rdfs:range conservatively.\nGlobal domain/range statements propagate broadly in OWL. If a constraint is contextual rather than universal, prefer class restrictions and/or SHACL validation."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#naming-convention",
    "href": "reference_info/ontology/conventions/owl-properties.html#naming-convention",
    "title": "OWL Properties",
    "section": "Naming convention",
    "text": "Naming convention\nUse lowerCamelCase from the subject perspective:\n\naboutStock\nusesMethod\nhasMember"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#alignment-guidance",
    "href": "reference_info/ontology/conventions/owl-properties.html#alignment-guidance",
    "title": "OWL Properties",
    "section": "Alignment guidance",
    "text": "Alignment guidance\nBefore creating a new relation, check standard relation vocabularies (e.g., RO). Prefer reuse or alignment via:\n\nrdfs:subPropertyOf\nowl:equivalentProperty\n\nAvoid using rdfs:seeAlso as a semantic alignment substitute."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-properties.html#template-examples",
    "href": "reference_info/ontology/conventions/owl-properties.html#template-examples",
    "title": "OWL Properties",
    "section": "Template examples",
    "text": "Template examples\n:aboutStock a owl:ObjectProperty ;\n  rdfs:label \"about stock\"@en ;\n  IAO:0000115 \"Links a measurement or observation to the stock it describes.\"@en ;\n  rdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; .\n\n:measurementValue a owl:DatatypeProperty ;\n  rdfs:label \"measurement value\"@en ;\n  IAO:0000115 \"The recorded numeric value for a measurement.\"@en ;\n  rdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; ."
  },
  {
    "objectID": "reference_info/ontology/conventions/index.html",
    "href": "reference_info/ontology/conventions/index.html",
    "title": "Ontology Conventions Guidebook",
    "section": "",
    "text": "This guidebook is the user-first conventions guide for contributors working in the GC DFO Salmon Ontology ecosystem.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Ontology Conventions Guidebook"
    ]
  },
  {
    "objectID": "reference_info/ontology/conventions/index.html#start-here",
    "href": "reference_info/ontology/conventions/index.html#start-here",
    "title": "Ontology Conventions Guidebook",
    "section": "Start here",
    "text": "Start here\n\nQuick reference",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Ontology Conventions Guidebook"
    ]
  },
  {
    "objectID": "reference_info/ontology/conventions/index.html#core-modeling-guidance",
    "href": "reference_info/ontology/conventions/index.html#core-modeling-guidance",
    "title": "Ontology Conventions Guidebook",
    "section": "Core modeling guidance",
    "text": "Core modeling guidance\n\nSchema vs data (TBox vs ABox)\nOWL classes\nOWL properties\nSKOS vocabularies\nProvenance and maintenance",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Ontology Conventions Guidebook"
    ]
  },
  {
    "objectID": "reference_info/ontology/conventions/index.html#full-single-page-version",
    "href": "reference_info/ontology/conventions/index.html#full-single-page-version",
    "title": "Ontology Conventions Guidebook",
    "section": "Full single-page version",
    "text": "Full single-page version\n\nFull conventions (canonical source)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Ontology Conventions Guidebook"
    ]
  },
  {
    "objectID": "reference_info/ontology/class-vs-skos-decision-tree.html",
    "href": "reference_info/ontology/class-vs-skos-decision-tree.html",
    "title": "Class vs SKOS Concept: Decision Tree",
    "section": "",
    "text": "Use this quick decision tree when modeling a new term.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Class vs SKOS Decision Tree"
    ]
  },
  {
    "objectID": "reference_info/ontology/class-vs-skos-decision-tree.html#decision-tree",
    "href": "reference_info/ontology/class-vs-skos-decision-tree.html#decision-tree",
    "title": "Class vs SKOS Concept: Decision Tree",
    "section": "Decision tree",
    "text": "Decision tree\n\nIs this a controlled value/code list item (picklist term)?\n\nYes ‚Üí model as SKOS concept\nNo ‚Üí continue\n\nDoes it need logical class semantics (subclassing, restrictions, reasoning)?\n\nYes ‚Üí model as OWL class/property\nNo ‚Üí continue\n\nIs it mostly lexical terminology with broader/narrower relationships?\n\nYes ‚Üí model as SKOS concept\nNo ‚Üí model as OWL class/property",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Class vs SKOS Decision Tree"
    ]
  },
  {
    "objectID": "reference_info/ontology/class-vs-skos-decision-tree.html#practical-examples",
    "href": "reference_info/ontology/class-vs-skos-decision-tree.html#practical-examples",
    "title": "Class vs SKOS Concept: Decision Tree",
    "section": "Practical examples",
    "text": "Practical examples\n\n‚ÄúSonar counting‚Äù as method term in a code list ‚Üí SKOS concept\n‚ÄúConservation Unit‚Äù as domain entity class ‚Üí OWL class\n‚ÄúaboutStock‚Äù relation between measurement and stock ‚Üí OWL object property",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Class vs SKOS Decision Tree"
    ]
  },
  {
    "objectID": "reference_info/ontology/class-vs-skos-decision-tree.html#guardrails",
    "href": "reference_info/ontology/class-vs-skos-decision-tree.html#guardrails",
    "title": "Class vs SKOS Concept: Decision Tree",
    "section": "Guardrails",
    "text": "Guardrails\n\navoid modeling the same meaning as both SKOS concept and OWL class by default\nkeep SKOS for terminology governance and controlled values\nkeep OWL for logical model structure and inferencing\n\nSee also:\n\nOntology Conventions Guidebook\nHow to Request a New Ontology Term",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Class vs SKOS Decision Tree"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html",
    "href": "reference_info/dwc-obis-gbif.html",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "This guide explains how Darwin Core supports interoperability for Fisheries and Oceans Canada (DFO) salmon data, and how GBIF/OBIS publication models connect to that workflow.\n\n\nDarwin Core (DwC) is a biodiversity data standard maintained by TDWG. It provides common terms for recording occurrences, events, measurements, and related metadata.\n\n\n\nIn DFO Pacific science workflows, teams use different local schemas. Darwin Core acts as a translation layer so those local datasets can be aligned and shared without forcing one internal database design.\n\n\n\nThe GBIF Unified Data Model extends DwC to support linked structures such as:\n\nEvents\nOccurrences\nMeasurements/Facts\n\nThis is useful for salmon workflows where one program may generate survey events, biological observations, and derived indicators.\n\n\n\nOBIS applies Darwin Core patterns to marine biodiversity exchange, including marine-focused extensions and publication pathways.\n\n\n\nFor salmon-specific semantics in this ecosystem:\n\ntreat the DFO Salmon Ontology as canonical for term meaning and identifiers\nuse Darwin Core terms for interoperability and publication packaging\nmap outward to GBIF/OBIS as needed\n\nCanonical ontology entry point:\n\nGC DFO Salmon Ontology documentation\n\n\n\n\n\n\n\n\n\n\n\n\nDFO Field / Concept\nDarwin Core Term\nNotes\n\n\n\n\nSpecies_Code or Species_Name\nscientificName\nResolve against accepted taxonomic services\n\n\nSample_ID\noccurrenceID\nUse stable unique identifiers\n\n\nCatch_Date\neventDate\nPrefer ISO 8601\n\n\nSurvey_Name\neventID\nUse hierarchical event IDs where needed\n\n\nLatitude, Longitude\ndecimalLatitude, decimalLongitude\nUse WGS84\n\n\nDepth_m\nminimumDepthInMeters, maximumDepthInMeters\nInclude both when possible\n\n\nbiological metrics\nmeasurementType, measurementValue, measurementUnit\nUse controlled value strategy\n\n\nmethod/protocol\nsamplingProtocol\nKeep method terms consistent\n\n\n\n\n\n\n\nstart semantic mapping in DFO ontology terms\nmap to DwC publication terms for external interoperability\nkeep provenance explicit for transformations and crosswalks\n\n\nDarwin Core is not your internal data model. It is your interoperability contract.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#what-is-darwin-core",
    "href": "reference_info/dwc-obis-gbif.html#what-is-darwin-core",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "Darwin Core (DwC) is a biodiversity data standard maintained by TDWG. It provides common terms for recording occurrences, events, measurements, and related metadata.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#darwin-core-as-an-interoperability-layer",
    "href": "reference_info/dwc-obis-gbif.html#darwin-core-as-an-interoperability-layer",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "In DFO Pacific science workflows, teams use different local schemas. Darwin Core acts as a translation layer so those local datasets can be aligned and shared without forcing one internal database design.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#the-unified-gbif-data-model",
    "href": "reference_info/dwc-obis-gbif.html#the-unified-gbif-data-model",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "The GBIF Unified Data Model extends DwC to support linked structures such as:\n\nEvents\nOccurrences\nMeasurements/Facts\n\nThis is useful for salmon workflows where one program may generate survey events, biological observations, and derived indicators.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#obis-context",
    "href": "reference_info/dwc-obis-gbif.html#obis-context",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "OBIS applies Darwin Core patterns to marine biodiversity exchange, including marine-focused extensions and publication pathways.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#dfo-salmon-semantic-position",
    "href": "reference_info/dwc-obis-gbif.html#dfo-salmon-semantic-position",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "For salmon-specific semantics in this ecosystem:\n\ntreat the DFO Salmon Ontology as canonical for term meaning and identifiers\nuse Darwin Core terms for interoperability and publication packaging\nmap outward to GBIF/OBIS as needed\n\nCanonical ontology entry point:\n\nGC DFO Salmon Ontology documentation",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#example-field-alignment",
    "href": "reference_info/dwc-obis-gbif.html#example-field-alignment",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "DFO Field / Concept\nDarwin Core Term\nNotes\n\n\n\n\nSpecies_Code or Species_Name\nscientificName\nResolve against accepted taxonomic services\n\n\nSample_ID\noccurrenceID\nUse stable unique identifiers\n\n\nCatch_Date\neventDate\nPrefer ISO 8601\n\n\nSurvey_Name\neventID\nUse hierarchical event IDs where needed\n\n\nLatitude, Longitude\ndecimalLatitude, decimalLongitude\nUse WGS84\n\n\nDepth_m\nminimumDepthInMeters, maximumDepthInMeters\nInclude both when possible\n\n\nbiological metrics\nmeasurementType, measurementValue, measurementUnit\nUse controlled value strategy\n\n\nmethod/protocol\nsamplingProtocol\nKeep method terms consistent",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#practical-guidance",
    "href": "reference_info/dwc-obis-gbif.html#practical-guidance",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "start semantic mapping in DFO ontology terms\nmap to DwC publication terms for external interoperability\nkeep provenance explicit for transformations and crosswalks\n\n\nDarwin Core is not your internal data model. It is your interoperability contract.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Darwin Core / GBIF / OBIS"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-system.html",
    "href": "reference_info/data_standards/semantic-salmon-system.html",
    "title": "Semantic Salmon System ‚Äî Start Here",
    "section": "",
    "text": "Canonical specification (Markdown): Salmon data package specification\nThis page is a high-level overview of the workflow. For complete package details and field requirements, use the canonical specification above.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon System ‚Äî Start Here"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-system.html#the-system-flow",
    "href": "reference_info/data_standards/semantic-salmon-system.html#the-system-flow",
    "title": "Semantic Salmon System ‚Äî Start Here",
    "section": "The system flow",
    "text": "The system flow\n\n  \n    1) DFO Salmon Ontology\n    Use canonical term definitions and IRIs.\n    Open ontology docs\n  \n  ‚Üí\n  \n    2) Salmon data package\n    Package dataset metadata in a standard structure.\n    Open SDP spec\n  \n  ‚Üí\n  \n    3) metasalmon\n    Run quality and validation checks before intake.\n    Open tools page\n  \n  ‚Üí\n  \n    4) Salmon Data GPT (optional)\n    Draft mappings and package metadata faster.\n    See intake guidance\n  \n  ‚Üí\n  \n    5) SPSR Intake\n    Upload through the operational FSAR-oriented pathway.\n    Open SPSR upload wizard",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon System ‚Äî Start Here"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-system.html#salmon-data-package-overview-diagram",
    "href": "reference_info/data_standards/semantic-salmon-system.html#salmon-data-package-overview-diagram",
    "title": "Semantic Salmon System ‚Äî Start Here",
    "section": "Salmon data package overview (diagram)",
    "text": "Salmon data package overview (diagram)\n\n\n\nSalmon data package diagram showing metadata files (dataset.csv, tables.csv, columns.csv, codes.csv) linked to package data tables.\n\n\nFor complete structure and field-level requirements, use the canonical salmon data package specification.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon System ‚Äî Start Here"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-system.html#what-to-use-this-page-for",
    "href": "reference_info/data_standards/semantic-salmon-system.html#what-to-use-this-page-for",
    "title": "Semantic Salmon System ‚Äî Start Here",
    "section": "What to use this page for",
    "text": "What to use this page for\n\nnew contributors who need the shortest correct path\nanalysts preparing FSAR datasets for submission\nreviewers checking if a package is intake-ready",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon System ‚Äî Start Here"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-system.html#quick-launch-paths",
    "href": "reference_info/data_standards/semantic-salmon-system.html#quick-launch-paths",
    "title": "Semantic Salmon System ‚Äî Start Here",
    "section": "Quick launch paths",
    "text": "Quick launch paths\n\nStart my FSAR intake path\nFSAR Data Standardization Workflow\nSalmon data package + SPSR intake path\nThe Semantic Salmon Data Ecosystem (deeper architecture)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon System ‚Äî Start Here"
    ]
  },
  {
    "objectID": "reference_info/data_standards/salmon-data-exchange-package.html",
    "href": "reference_info/data_standards/salmon-data-exchange-package.html",
    "title": "Salmon data package specification",
    "section": "",
    "text": "Canonical markdown source: https://github.com/dfo-pacific-science/smn-data-pkg/blob/main/SPECIFICATION.md\nThis page provides a DSU-hosted copy/overview. For canonical field definitions and version history, use the GitHub source above.\n\n1 Overview\nThe salmon data package (SDP) is a lightweight, frictionless-style specification for exchanging salmon datasets between scientists, assessment biologists, and data stewards.\nIt is designed to:\n\nBe simple to adopt with Excel and CSV files.\nBe tool-friendly for R/Python packages and custom GPT assistants.\nBe ontology-aware, linking columns and codes to the DFO Salmon Ontology and related vocabularies.\nBe compatible with frictionless data packages and Darwin Core‚Äìstyle semantic layers without forcing a single rigid schema.\n\nEach salmon data package instance is a small directory of CSV data files plus a set of metadata CSVs that describe the dataset, tables, columns, and controlled codes.\n\n\n2 Design Goals\n\nInteroperable but flexible: support multiple schemas (FSAR, SPSR-like, project-specific) while providing shared semantics via URIs.\nFRICTIONLESS-compatible: align conceptually with frictionless/datapackage ideas so existing tooling can be reused or extended.\nOntology-linked: allow columns and codes to reference DFO Salmon Ontology and SKOS vocab IRIs.\nIncremental adoption: existing tables can be wrapped in a package with minimal changes (no need to refactor everything at once).\nMachine- and human-readable: scientists can work in spreadsheets; machines can consume the same metadata.\n\n\n\n3 Package Layout\nA salmon data package consists of:\n\nOne root directory (e.g., sdp_myproject/) containing:\n\ndataset.csv ‚Äì dataset-level metadata.\ntables.csv ‚Äì one row per logical table.\ncolumn_dictionary.csv ‚Äì one row per column in each table.\ncodes.csv ‚Äì optional: controlled value lists and SKOS links.\n\nOne or more data files (typically CSV) referenced from tables.csv, usually in a data/ subdirectory.\n\nMinimal example layout:\n\ndataset.csv\ntables.csv\ncolumn_dictionary.csv\ncodes.csv (optional for fully numeric or unconstrained columns)\ndata/your_table_1.csv\ndata/your_table_2.csv\netc.\n\nDownloadable starter assets:\n\nMinimal salmon data package example (zip)\nBrowse example package files\n\nThe salmon data package specification does not prescribe a single canonical salmon schema. Instead, it standardizes the metadata about whatever schema a project uses and ties it to ontology and vocabularies.\n\n\n4 Identifiers and Conventions\n\ndataset_id\n\nShort string identifier unique within your organization or project (e.g., fsar_spsr_chinook_2025).\nUsed to link rows across dataset.csv, tables.csv, column_dictionary.csv, and codes.csv.\n\ntable_id\n\nShort ID unique within a dataset (e.g., cu_year_index, survey_events).\n\ncolumn_name\n\nExact column name as it appears in the data file (e.g., TOTAL_SPAWNERS, run_size_total).\n\nURIs / IRIs\n\nFields such as dataset_iri, entity_iri, term_iri, concept_scheme_iri, unit_iri should use persistent HTTP IRIs where available (e.g., w3id for DFO Salmon Ontology terms, vocabulary concepts, and units).\n\n\n\n\n5 dataset.csv Schema\nOne row per logical dataset (often one per salmon data package directory). Can describe multiple related tables.\nRequired columns:\n\n\n\n\n\n\n\n\n\nColumn\nType\nRequired\nDescription\n\n\n\n\ndataset_id\nstring\nyes\nStable identifier used to join to other metadata files.\n\n\ntitle\nstring\nyes\nHuman-readable dataset title.\n\n\ndescription\nstring\nyes\nShort description of the dataset contents and purpose.\n\n\ncreator\nstring\nyes\nName(s) of primary creator(s) or project.\n\n\ncontact_name\nstring\nyes\nPrimary contact person.\n\n\ncontact_email\nstring\nyes\nContact email address.\n\n\nlicense\nstring\nyes\nLicense name or URL (e.g., CC-BY-4.0).\n\n\ntemporal_start\ndate\nno\nStart date or year covered by the dataset (ISO 8601 where possible).\n\n\ntemporal_end\ndate\nno\nEnd date or year covered by the dataset.\n\n\nspatial_extent\nstring\nno\nTextual description of spatial coverage (e.g., CUs, regions, coordinates).\n\n\ndataset_type\nstring\nno\nHigh-level type (e.g., cu_year_index, survey_timeseries, benchmark).\n\n\ndataset_iri\nstring\nno\nIRI for this dataset in a catalog or knowledge graph.\n\n\nsource_citation\nstring\nno\nFree-text citation for publications, reports, or internal docs.\n\n\n\nOptional additional columns (implementation-specific):\n\nprovenance_note ‚Äì narrative about data lineage.\ncreated / modified ‚Äì timestamps.\n\n\n\n6 tables.csv Schema\nOne row per table in the package.\nRequired columns:\n\n\n\n\n\n\n\n\n\nColumn\nType\nRequired\nDescription\n\n\n\n\ndataset_id\nstring\nyes\nReferences dataset_id in dataset.csv.\n\n\ntable_id\nstring\nyes\nShort ID for the table (e.g., cu_year_index).\n\n\nfile_name\nstring\nyes\nRelative path to the data file (e.g., data/cu_year_index.csv).\n\n\ntable_label\nstring\nyes\nHuman-readable label for the table.\n\n\ndescription\nstring\nyes\nDescription of what each row represents and how the table is used.\n\n\nentity_type\nstring\nno\nHuman-readable entity type (e.g., CU-year index, survey event).\n\n\nentity_iri\nstring\nno\nIRI of ontology class representing the row-level entity, if applicable.\n\n\nprimary_key\nstring\nno\nComma-separated list of columns forming a primary key (e.g., cu_id,year).\n\n\n\nNotes:\n\nentity_iri should reference a class in the DFO Salmon Ontology (e.g., a CU-year index class, survey event class) when available.\nprimary_key is advisory; it guides downstream validation and graph loading.\n\n\n\n7 column_dictionary.csv Schema\nOne row per column in each table. This is the core of how the salmon data package links data columns to ontology and vocabularies.\nRequired columns:\n\n\n\n\n\n\n\n\n\nColumn\nType\nRequired\nDescription\n\n\n\n\ndataset_id\nstring\nyes\nReferences dataset_id in dataset.csv.\n\n\ntable_id\nstring\nyes\nReferences table_id in tables.csv.\n\n\ncolumn_name\nstring\nyes\nExact column name in the data file.\n\n\ncolumn_label\nstring\nyes\nShort human-readable label.\n\n\ncolumn_description\nstring\nyes\nClear definition of the column‚Äôs meaning.\n\n\ncolumn_role\nstring\nyes\nOne of: identifier, attribute, temporal, categorical, measurement.\n\n\nvalue_type\nstring\nyes\nBasic type: integer, double, string, boolean, date, datetime.\n\n\nrequired\nboolean\nno\nTRUE if the column is required for each row, otherwise FALSE or blank.\n\n\nunit_label\nstring\nno\nHuman-readable unit label (e.g., number of fish, proportion).\n\n\nunit_iri\nstring\nno\nIRI for the unit (e.g., UCUM or other unit ontology).\n\n\nterm_iri\nstring\nno\nIRI for the ontology term (e.g., measurement type, dimension, concept) from the DFO Salmon Ontology.\n\n\nterm_type\nstring\nno\nType of ontology term (e.g., owl_class, owl_object_property, skos_concept).\n\n\n\ncolumn_role quick definitions:\n\n\n\n\n\n\n\n\nRole\nWhen to use it\nExample columns\n\n\n\n\nidentifier\nKeys and IDs that identify rows/entities\ncu_id, survey_event_id, record_id\n\n\nmeasurement\nQuantitative or measured values\nspawner_count, run_size, exploitation_rate\n\n\ntemporal\nTime and date fields\nreturn_year, sample_date, datetime\n\n\ncategorical\nControlled codes or enumerated classes\nstatus_code, species_code, run_timing\n\n\nattribute\nDescriptive attributes that are not IDs/time/measurements\nwaterbody_name, observer_name, population_name\n\n\n\nGuidance:\n\nUse column_role = \"identifier\" for identifiers such as cu_id, survey_event_id, POP_ID.\nUse column_role = \"measurement\" for numeric quantities (e.g., escapement, run size, exploitation rate).\nUse column_role = \"temporal\" for time-related columns (e.g., year, date, datetime).\nUse column_role = \"categorical\" for columns with controlled vocabularies or enumerated values (e.g., species, run type, status codes).\nUse column_role = \"attribute\" for descriptive attributes that are not identifiers, measurements, temporal, or categorical (e.g., population name, waterbody name).\nterm_iri should reference a term in the DFO Salmon Ontology (e.g., measurement types, dimensions, concepts) when available.\nterm_type indicates the type of ontology term (e.g., owl_class for classes, owl_object_property for properties, skos_concept for SKOS concepts).\n\n\n\n8 codes.csv Schema\nOptional file used for columns that have controlled enumerated values (status codes, methods, gear, etc.). This is where SKOS vocabularies plug in.\nEach row corresponds to one code value for one column.\nRequired columns:\n\n\n\n\n\n\n\n\n\nColumn\nType\nRequired\nDescription\n\n\n\n\ndataset_id\nstring\nyes\nReferences dataset_id in dataset.csv.\n\n\ntable_id\nstring\nyes\nReferences table_id in tables.csv.\n\n\ncolumn_name\nstring\nyes\nName of the column in the data file that uses this code.\n\n\ncode_value\nstring\nyes\nStored value in the data (e.g., R, Y, G, critical, adipose_intact).\n\n\ncode_label\nstring\nno\nHuman-readable label corresponding to the code.\n\n\ncode_description\nstring\nno\nLonger description of what the code means.\n\n\nconcept_scheme_iri\nstring\nno\nIRI of the SKOS concept scheme (e.g., the DFO Salmon Status scheme).\n\n\nterm_iri\nstring\nno\nIRI of the specific ontology term (SKOS concept or other) that code_value represents.\n\n\nterm_type\nstring\nno\nType of ontology term (e.g., skos_concept, owl_class).\n\n\n\nGuidance:\n\ncode_value must match exactly the values present in the data table.\ncode_label and code_description can be used to auto-generate documentation.\nconcept_scheme_iri identifies the SKOS concept scheme containing the controlled vocabulary.\nterm_iri and term_type enable alignment with the larger salmon ontology and SKOS vocabularies, linking each code value to its corresponding ontology term.\n\n\n\n9 Relationship to Ontology, R Package, and GPT\n\nThe ontology defines the formal semantics for:\n\nmeasurement types, entities, dimensions, and concepts (via term_iri and term_type),\nentities (entity_iri in tables.csv),\nconcept schemes (concept_scheme_iri in codes.csv).\n\nThe R package will:\n\nread salmon data package metadata and data files,\nvalidate packages against this specification,\njoin in ontology/vocabulary information,\nhelp reshape data into analysis-friendly tidy formats.\n\nThe custom GPT will:\n\nhelp users draft column_dictionary.csv and codes.csv,\nsuggest ontology-aligned IRIs and term types,\nassist in decomposing compound terms into multiple columns with appropriate roles and IRIs,\npropose package structures compliant with the salmon data package specification for new projects.\n\n\n\n\n10 Validation Workflow\nRun validation locally before publishing.\nTypical local workflow (R):\nlibrary(metasalmon)\n\npkg_path &lt;- \"path/to/salmon-data-package\"\nvalidate_dictionary(file.path(pkg_path, \"column_dictionary.csv\"))\nvalidate_semantics(file.path(pkg_path, \"column_dictionary.csv\"))\nRelated implementation guidance:\n\nFSAR Data Standardization Workflow\nSalmon data package + SPSR intake path\n\n\n\n11 Versioning and Backwards Compatibility\n\nInclude a spec_version field in dataset.csv in future iterations if needed (e.g., sdp-0.1.0).\nNew columns should be added in a backwards-compatible way:\n\nexisting tooling must continue to work when extra columns are present.\n\nBreaking changes to required columns or semantics should bump the major version.\n\n\n\n12 Implementation Notes\n\nCSV files should use UTF-8 encoding and a comma delimiter by default.\nDates should follow ISO 8601 where possible (e.g., YYYY-MM-DD); years can be encoded as integers.\nR and Python tooling should treat missing optional columns as NA/null without failing.\nThe spec is intentionally minimal; domain-specific extensions (e.g., FSAR/SPSR templates) can be layered on top as recommended profiles.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package specification"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html",
    "title": "Salmon data package + SPSR intake path",
    "section": "",
    "text": "Use this page when your goal is to move from mapped data to a submission-ready package and upload path for SPSR.\nCanonical package specification (Markdown): https://github.com/dfo-pacific-science/smn-data-pkg/blob/main/SPECIFICATION.md",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#quick-summary",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#quick-summary",
    "title": "Salmon data package + SPSR intake path",
    "section": "Quick summary",
    "text": "Quick summary\n\nThe DFO Salmon Ontology provides canonical term IRIs.\nThe salmon data package (SDP) provides package metadata structure (dataset.csv, tables.csv, column_dictionary.csv, codes.csv).\nmetasalmon supports package validation.\nSalmon Data GPT can help draft mappings/metadata.\nSPSR is a primary operational destination for FSAR-oriented uploads.\n\nCurrent SPSR direction is CUYear-first intake with WSP-aligned status fields and FSAR fishery fields coexisting in one upload pathway; use current template downloads and avoid relying on stale local copies.\nGuided assistants can help draft content, but validation remains the quality gate.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#practical-workflow",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#practical-workflow",
    "title": "Salmon data package + SPSR intake path",
    "section": "Practical workflow",
    "text": "Practical workflow\n\nFind canonical terms in GC DFO Salmon Ontology documentation.\nBuild column_dictionary.csv with full IRIs (no shortened/partial URIs).\nAssemble package files using Salmon data package specification.\nValidate locally (e.g., metasalmon checks).\nPrepare SPSR-compatible upload CSV(s).\nUpload through SPSR wizard and review validation feedback.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#spsr-touchpoints",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#spsr-touchpoints",
    "title": "Salmon data package + SPSR intake path",
    "section": "SPSR touchpoints",
    "text": "SPSR touchpoints\n\nApp documentation: https://spsr.dfo-mpo.gc.ca/documentation/\nUpload wizard: https://spsr.dfo-mpo.gc.ca/wizard/1/\nTemplate package endpoint: https://spsr.dfo-mpo.gc.ca/download_sdp_templates\nRepo: https://github.com/dfo-pacific-science/salmon-population-summary-repository",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#recommended-artifacts",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#recommended-artifacts",
    "title": "Salmon data package + SPSR intake path",
    "section": "Recommended artifacts",
    "text": "Recommended artifacts\n\ndataset.csv for dataset-level metadata\ntables.csv for table inventory and entities\ncolumn_dictionary.csv for column semantics + term links\ncodes.csv for controlled value sets\nupload-ready data CSV(s) matching your selected intake profile",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#validation-gates",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#validation-gates",
    "title": "Salmon data package + SPSR intake path",
    "section": "Validation gates",
    "text": "Validation gates\nBefore upload, confirm:\n\nall required metadata fields are present\nall semantic links use canonical full IRIs\ncategorical values are mapped to controlled concepts\nprovenance notes are present for major transformations",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/ontology-data-package-gpt.html#references",
    "href": "reference_info/data_standards/ontology-data-package-gpt.html#references",
    "title": "Salmon data package + SPSR intake path",
    "section": "References",
    "text": "References\n\nSemantic Salmon Data Ecosystem\nFSAR Data Standardization Workflow\nControlled Vocabulary Governance Workflow",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon data package + SPSR intake path"
    ]
  },
  {
    "objectID": "reference_info/data_standards/integrate-data.html",
    "href": "reference_info/data_standards/integrate-data.html",
    "title": "Integrate Data",
    "section": "",
    "text": "Document each source dataset:\n\nowner/source system\ntemporal scope\ngeographic scope\nkey identifiers\nknown caveats\n\nExample inventory:\n\n\n\n\n\n\n\n\n\n\nDataset\nSource\nKey Columns\nTemporal Scope\nGeographic Scope\n\n\n\n\nEscapement series\nSurvey program\nCU_code, BY, Esc\n2010‚Äì2024\nFraser CUs\n\n\nCatch series\nFOS\nCU, Year, Catch\n2010‚Äì2024\nMultiple areas\n\n\nStatus outputs\nSPSR\nCU, BY, STATUS\n2010‚Äì2024\nAll CUs\n\n\n\n\n\n\n\n\nCreate a crosswalk from source fields to target standard fields and canonical IRIs.\nExample (illustrative ‚Äî verify exact term IRIs in WIDOCO):\n\n\n\n\n\n\n\n\n\n\nSource Dataset\nSource Column\nStandard Label\nStandard IRI\nTarget Column\n\n\n\n\nEscapement\nCU_code\nConservation Unit\nhttps://w3id.org/gcdfo/salmon#ConservationUnit\nCU\n\n\nEscapement\nBY\nBrood Year\nhttps://w3id.org/gcdfo/salmon#BroodYear\nBroodYear\n\n\nCatch\nYear\nReturn Year\nhttps://w3id.org/gcdfo/salmon#ReturnYear\nReturnYear\n\n\n\nUse full IRIs only.\n\n\n\n\n\nNormalize categorical values before joining.\nsource_dataset,source_column,source_value,standard_value,concept_iri\nEscapement,run_type,Spring,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\nEscapement,run_type,S,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\nCatch,run_type,SPRING,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\n\n\n\n\n\n\nalign column names\nalign data types\nalign temporal semantics (brood year vs return year)\ndocument all transformation assumptions\n\n\n\n\n\n\n\ndefine join keys and expected cardinality\nrun joins in script\ncheck for dropped/unmatched records\ncompute derived fields only after validating joins\n\n\n\n\n\n\nValidation checks:\n\nsource totals vs integrated totals\nrow counts by key strata (CU/year/etc.)\nderived metric sanity checks (ranges, null rate)\nunresolved mapping exceptions list\n\n\n\n\n\n\nPackage the integrated output with:\n\nupdated column_dictionary.csv\ncode mappings (codes.csv)\nintegration README (logic + assumptions)\n\nThen continue to Salmon data package + SPSR intake path if this dataset is moving into SPSR/FSAR workflows.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Integrate Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/integrate-data.html#cookbook-guide",
    "href": "reference_info/data_standards/integrate-data.html#cookbook-guide",
    "title": "Integrate Data",
    "section": "",
    "text": "Document each source dataset:\n\nowner/source system\ntemporal scope\ngeographic scope\nkey identifiers\nknown caveats\n\nExample inventory:\n\n\n\n\n\n\n\n\n\n\nDataset\nSource\nKey Columns\nTemporal Scope\nGeographic Scope\n\n\n\n\nEscapement series\nSurvey program\nCU_code, BY, Esc\n2010‚Äì2024\nFraser CUs\n\n\nCatch series\nFOS\nCU, Year, Catch\n2010‚Äì2024\nMultiple areas\n\n\nStatus outputs\nSPSR\nCU, BY, STATUS\n2010‚Äì2024\nAll CUs\n\n\n\n\n\n\n\n\nCreate a crosswalk from source fields to target standard fields and canonical IRIs.\nExample (illustrative ‚Äî verify exact term IRIs in WIDOCO):\n\n\n\n\n\n\n\n\n\n\nSource Dataset\nSource Column\nStandard Label\nStandard IRI\nTarget Column\n\n\n\n\nEscapement\nCU_code\nConservation Unit\nhttps://w3id.org/gcdfo/salmon#ConservationUnit\nCU\n\n\nEscapement\nBY\nBrood Year\nhttps://w3id.org/gcdfo/salmon#BroodYear\nBroodYear\n\n\nCatch\nYear\nReturn Year\nhttps://w3id.org/gcdfo/salmon#ReturnYear\nReturnYear\n\n\n\nUse full IRIs only.\n\n\n\n\n\nNormalize categorical values before joining.\nsource_dataset,source_column,source_value,standard_value,concept_iri\nEscapement,run_type,Spring,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\nEscapement,run_type,S,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\nCatch,run_type,SPRING,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\n\n\n\n\n\n\nalign column names\nalign data types\nalign temporal semantics (brood year vs return year)\ndocument all transformation assumptions\n\n\n\n\n\n\n\ndefine join keys and expected cardinality\nrun joins in script\ncheck for dropped/unmatched records\ncompute derived fields only after validating joins\n\n\n\n\n\n\nValidation checks:\n\nsource totals vs integrated totals\nrow counts by key strata (CU/year/etc.)\nderived metric sanity checks (ranges, null rate)\nunresolved mapping exceptions list\n\n\n\n\n\n\nPackage the integrated output with:\n\nupdated column_dictionary.csv\ncode mappings (codes.csv)\nintegration README (logic + assumptions)\n\nThen continue to Salmon data package + SPSR intake path if this dataset is moving into SPSR/FSAR workflows.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Integrate Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/integrate-data.html#next-steps",
    "href": "reference_info/data_standards/integrate-data.html#next-steps",
    "title": "Integrate Data",
    "section": "Next Steps",
    "text": "Next Steps\n\nGC DFO Salmon Ontology documentation\nSalmon data package specification\nFSAR workflow",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Integrate Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/changelog.html",
    "href": "reference_info/data_standards/changelog.html",
    "title": "Standards & Workflow Changelog",
    "section": "",
    "text": "This changelog tracks user-facing changes to standards guidance, ontology workflow docs, and FSAR/SPSR integration paths.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Standards & Workflow Changelog"
    ]
  },
  {
    "objectID": "reference_info/data_standards/changelog.html#entry-template",
    "href": "reference_info/data_standards/changelog.html#entry-template",
    "title": "Standards & Workflow Changelog",
    "section": "Entry template",
    "text": "Entry template\nUse this structure for each changelog item:\n\nDate: YYYY-MM-DD\nArea: ontology / salmon data package / FSAR workflow / SPSR intake / publishing\nChange: what changed\nReason: why it changed\nImpact: who should care and what to do next\nLinks: PR/issue/pages",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Standards & Workflow Changelog"
    ]
  },
  {
    "objectID": "reference_info/data_standards/changelog.html#section",
    "href": "reference_info/data_standards/changelog.html#section",
    "title": "Standards & Workflow Changelog",
    "section": "2026-02",
    "text": "2026-02\n\n2026-02-25 ‚Äî terminology normalization + FSAR workflow refresh\n\nArea: salmon data package + FSAR workflow + SPSR intake\nChange: replaced user-facing SDEP wording with ‚Äúsalmon data package‚Äù/‚ÄúSDP‚Äù; added canonical markdown spec links to smn-data-pkg; updated FSAR workflow guidance to reflect current SPSR CUYear-first intake direction and template-first upload prep.\nReason: keep terminology aligned with current naming and reduce confusion between overview pages and canonical spec sources.\nImpact: contributors should use ‚Äúsalmon data package‚Äù wording in docs and rely on SPSR template endpoint + canonical GitHub specification for field-level implementation details.\nLinks:\n\nFSAR Data Standardization Workflow\nSalmon data package specification\nhttps://github.com/dfo-pacific-science/smn-data-pkg/blob/main/SPECIFICATION.md\nhttps://github.com/dfo-pacific-science/salmon-population-summary-repository/blob/master/docs/plans/2026-02-18-spsr-production-ready-unified-execplan.md\n\n\n\n\n2026-02-20 ‚Äî ontology docs consolidation + FSAR/SPSR alignment\n\nArea: ontology + standards hub + workflows\nChange: integrated ontology conventions guidebook into DSU docs; retired duplicate term explorer path; rewired standards pages around canonical ontology docs + salmon data package + SPSR intake.\nReason: reduce duplication and keep one formal source of truth.\nImpact: contributors should use WIDOCO + conventions guidebook for term semantics and use updated FSAR/SPSR workflow pages for intake.\nLinks:\n\nPR #151\nGC DFO Salmon Ontology\nSalmon data package + SPSR intake path\n\n\n\n\n2026-02-20 ‚Äî semantic system entry path made explicit\n\nArea: standardize/publish workflow pages\nChange: made DFO Ontology ‚Üí salmon data package ‚Üí metasalmon ‚Üí Salmon Data GPT ‚Üí SPSR path prominent at top of key workflow pages.\nReason: make the intended system flow obvious during standardization and publishing.\nImpact: data producers should follow the explicit entry path before upload or external release.\nLinks:\n\nStandardize Your Data\nPublish Your Data\nThe Semantic Salmon Data Ecosystem\n\n\n\n\n2026-02-20 ‚Äî branded semantic start page added\n\nArea: standards IA + homepage/navigation\nChange: added a branded front-door page for the full path, with one primary CTA: Start my FSAR intake path.\nReason: provide a single obvious entry point before users branch into detailed pages.\nImpact: new users can start from one page and follow the canonical sequence without guessing navigation order.\nLinks:\n\nSemantic Salmon System ‚Äî Start Here\nFSAR to SPSR: End-to-End Example\nSalmon Data Standards Hub",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Standards & Workflow Changelog"
    ]
  },
  {
    "objectID": "reference_info/data_standards/changelog.html#quality-checks-ci",
    "href": "reference_info/data_standards/changelog.html#quality-checks-ci",
    "title": "Standards & Workflow Changelog",
    "section": "Quality checks (CI)",
    "text": "Quality checks (CI)\nAutomated checks are configured to catch:\n\nlegacy namespace/content patterns in active docs\nbroken internal local links in active .qmd pages\n\nCI workflow file: .github/workflows/docs-quality.yml",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Standards & Workflow Changelog"
    ]
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html",
    "href": "how_to_guides/sharepoint_r.html",
    "title": "Access Data in SharePoint from R",
    "section": "",
    "text": "Connecting R to Sharepoint enables secure, programmatic data access for reproducible analysis workflows. This is especially useful to data stewards and scientists who want to move away from email and personal hard-drive based data storage and sharing and instead use a cloud service as a central repository for data.\nThis is particularly useful for data management tasks in Sharepoint, such as:\n\nDownloading data files for analysis\nUploading processed data files\nManaging document libraries and folders\nAutomating data workflows\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind, to access the DFO Sharepoint from R using the methods below you need to be on the DFO network or using the VPN.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html#conclusion",
    "href": "how_to_guides/sharepoint_r.html#conclusion",
    "title": "Access Data in SharePoint from R",
    "section": "Conclusion",
    "text": "Conclusion\nThis enables secure, programmatic data access for reproducible analysis workflows. Using the Microsoft365R package, you can easily access and manage data in SharePoint from R. This allows for a more streamlined and efficient workflow, especially for data stewards and scientists who work with large datasets and need to collaborate with others.\nThis approach can help you automate data management tasks, reduce reliance on email and personal storage, and improve collaboration within your team.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html#additional-resources",
    "href": "how_to_guides/sharepoint_r.html#additional-resources",
    "title": "Access Data in SharePoint from R",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMicrosoft365R Documentation",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "",
    "text": "This example shows one practical path from a local FSAR dataset to an SPSR upload-ready package.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#scenario",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#scenario",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Scenario",
    "text": "Scenario\nYou have a CU-level escapement time series and supporting metadata for an FSAR update.\nGoal: produce a validated package and submit through SPSR wizard.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-1-start-with-source-data-and-assumptions",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-1-start-with-source-data-and-assumptions",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 1 ‚Äî Start with source data and assumptions",
    "text": "Step 1 ‚Äî Start with source data and assumptions\nRecord:\n\nsource dataset file(s)\ntemporal scope\nCU/SMU scope\nkey transformations already applied\nknown caveats",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-2-map-fields-to-canonical-terms",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-2-map-fields-to-canonical-terms",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 2 ‚Äî Map fields to canonical terms",
    "text": "Step 2 ‚Äî Map fields to canonical terms\nUse GC DFO Salmon Ontology documentation to map core columns to canonical IRIs.\nCreate a simple mapping table:\nsource_column,standard_label,standard_term_iri\nCU_code,Conservation Unit,https://w3id.org/gcdfo/salmon#ConservationUnit\nBY,Brood Year,https://w3id.org/gcdfo/salmon#BroodYear\nEsc,Escapement,https://w3id.org/gcdfo/salmon#Escapement",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-3-build-minimal-salmon-data-package-metadata",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-3-build-minimal-salmon-data-package-metadata",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 3 ‚Äî Build minimal salmon data package metadata",
    "text": "Step 3 ‚Äî Build minimal salmon data package metadata\nCreate:\n\ndataset.csv\ntables.csv\ncolumn_dictionary.csv\ncodes.csv (if categorical values are present)\n\nReference: Salmon data package specification",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-4-validate-before-upload",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-4-validate-before-upload",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 4 ‚Äî Validate before upload",
    "text": "Step 4 ‚Äî Validate before upload\nRun local checks (structure + semantics) and fix issues before submission.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-5-prepare-spsr-upload-files",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-5-prepare-spsr-upload-files",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 5 ‚Äî Prepare SPSR upload files",
    "text": "Step 5 ‚Äî Prepare SPSR upload files\nConfirm that your upload file(s):\n\nuse the expected column naming/profile for your intake mode\ninclude required metadata context\npreserve traceability to source values\n\nSPSR resources:\n\ndocs: https://spsr.dfo-mpo.gc.ca/documentation/\nwizard: https://spsr.dfo-mpo.gc.ca/wizard/1/\ntemplates: https://spsr.dfo-mpo.gc.ca/download_sdp_templates",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-6-submit-and-iterate",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#step-6-submit-and-iterate",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Step 6 ‚Äî Submit and iterate",
    "text": "Step 6 ‚Äî Submit and iterate\n\nUpload through wizard.\nReview validation output.\nCorrect and resubmit as needed.\nRecord accepted version and intake date.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-to-spsr-end-to-end-example.html#deliverables-checklist",
    "href": "how_to_guides/fsar-to-spsr-end-to-end-example.html#deliverables-checklist",
    "title": "FSAR to SPSR: End-to-End Example",
    "section": "Deliverables checklist",
    "text": "Deliverables checklist\n\nmapped field table with canonical IRIs\ncomplete minimal salmon data package metadata files\nvalidated upload file(s)\nSPSR submission completed\nversion/provenance notes archived",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR to SPSR: End-to-End Example"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html",
    "href": "how_to_guides/databricks_r_studio.html",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "",
    "text": "DFO‚Äôs shift to the Azure Lakehouse model integrates Azure Databricks, Data Lake Storage, Delta Lake, and Synapse Analytics, replacing siloed data storage and scattered code repositories. This transition centralizes data, standardizes workflows, and improves access to shared datasets and intermediate data products.\nWith Personal Access Tokens (PATs), analysts can now connect directly to the Lakehouse from RStudio (or other IDEs like VSCode or PyCharm), working seamlessly with cloud-stored data in their preferred environment. This guide explains how to set up that connection using RStudio for analysis, GitHub for code management, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#tldr",
    "href": "how_to_guides/databricks_r_studio.html#tldr",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "",
    "text": "DFO‚Äôs shift to the Azure Lakehouse model integrates Azure Databricks, Data Lake Storage, Delta Lake, and Synapse Analytics, replacing siloed data storage and scattered code repositories. This transition centralizes data, standardizes workflows, and improves access to shared datasets and intermediate data products.\nWith Personal Access Tokens (PATs), analysts can now connect directly to the Lakehouse from RStudio (or other IDEs like VSCode or PyCharm), working seamlessly with cloud-stored data in their preferred environment. This guide explains how to set up that connection using RStudio for analysis, GitHub for code management, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#azure-data-bricks-and-the-azure-lakehouse",
    "href": "how_to_guides/databricks_r_studio.html#azure-data-bricks-and-the-azure-lakehouse",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Azure Data Bricks and the Azure Lakehouse",
    "text": "Azure Data Bricks and the Azure Lakehouse\nAzure Databricks is a cloud-based analytics platform for big data processing, machine learning, and collaborative analysis. Built on Apache Spark, it integrates with Azure Data Lake Storage (ADLS), Delta Lake, and Synapse Analytics, the core components of the Azure Lakehouse.\nIn this model, Databricks acts as the compute engine, running code while Data Lake Storage and Delta Lake handle scalable, secure data storage. Synapse Analytics adds SQL-based analytics and data warehousing.\nFor DFO, adopting the Lakehouse model eliminates siloed data storage and fragmented code management. Previously, datasets were scattered across systems, and essential data cleaning and analysis code was split across multiple GitHub repositories, making it hard to track, reuse, or standardize. Valuable intermediate data products were often inaccessible. The Lakehouse consolidates data, transformations, and curated datasets, improving collaboration, reproducibility, and transparency.\nNow, analysts can connect directly to the Lakehouse from RStudio (or any IDE) using PATs, gaining seamless, secure access to centralized data with scalable compute. This shift removes past limitations on data access, permissions, and technology use, significantly enhancing DFO‚Äôs analytical capabilities.\nThis guide walks through the setup process, enabling you to connect to Azure Databricks and analyze Lakehouse-stored data using RStudio for code execution, GitHub for version control, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#quick-start-guide-to-accessing-azure-lakehouse-from-r-studio",
    "href": "how_to_guides/databricks_r_studio.html#quick-start-guide-to-accessing-azure-lakehouse-from-r-studio",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Quick Start Guide to Accessing Azure Lakehouse from R Studio",
    "text": "Quick Start Guide to Accessing Azure Lakehouse from R Studio\n\nAccess Token\nCreate your user access token in Databricks WebUI.\nIMPORTANT: Treat this token as your personal password and do not share with anybody\n\nRequest an account on the DFO Azure Databricks workspace (EDH Subscription) by submitting an email to DFO.PACStrategicData-DonneesStrategiquesPAC.MPO@dfo-mpo.gc.ca Copy, Paste this template request and replace the [placeholders] with your information:\n\n\nHi Team,\n\nI'm following the guide at https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html and I'd like to be added to the DFO Azure Databricks workspace (EDH Subscription) to access the DFO Pacific Lakehouse for my work which is [briefly describe your work here so that the administrators know which types of permissions to give you: Data Analyst, Data Scientist, Data Engineer (described in section 1.1 of https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html)]. \n\n\nIn Databricks Web UI &gt; go to your profile page and navigate to\nUser &gt; Developer\nAccess tokens &gt; Manage\nGenerate new token as required. Ensure to set the expiration of the token to a suitable date (1 year from now)\nCopy and save this token somewhere safe such as an encrypted password manager\n\n\n\n\nDatabricks Token\n\n\n\n\nJDBC Driver\n\nDownload JDBC (java database connectivity) driver from https://www.databricks.com/spark/jdbc-drivers-download. Save and extract the .jar file to an accessible location on your computer (C:Users\\Your username is fine).\nSet up your Databricks JDBC connection URL. You can copy this directly from Databricks this way\nIn the Databricks Web UI, navigate to SQL Warehouses\nClick on the SQL Warehouse compute resource that you started ¬†\nGo to ‚ÄòConnection details‚Äô &gt; JDBC URL and copy the string\n\n\n\n\nJDBC Driver\n\n\n\n\nEstablishing a Connection to Databricks from R Studio (or any other IDE)\n\nEnsure you are on the DFO VPN\n\n\nlibrary(RJDBC)\n\njdbc_driver &lt;- JDBC(\"com.databricks.client.jdbc.Driver\", \"C:/Users/JOHNSONBRE/DatabricksJDBC42.jar\", \"\")\n\nurlsql &lt;- \"jdbc:databricks://adb....;\"\n\n# Run the code below here once to store your personal access token in your R Environment. \n# IMPORTANT! Never hard code your PAT in directly in your code script since your script will likely be shared, exposing your secret personal access token¬†\n\nfile.edit(\"~/.Renviron\") # add DATABRICKS_PAT=\"your_personal_access_token\" to your .Renviron file\n\npat &lt;- Sys.getenv(\"DATABRICKS_PAT\")\n\nconnsql &lt;- RJDBC::dbConnect(jdbc_driver, urlsql, \"token\", pat)\n\nTest your connection to Databricks by running the following code:\n\ndbGetQuery(connsql, \"SELECT 1\")\n\nIf you see a table with a single row and a single column with the value 1, then your connection is successful\n\n\nExploring the Lakehouse\nNext, we will query the available catalogs of databases, their schemas, and their tables in the Lakehouse.\n\nCatalogs\nExplore the available catalogs in the Databricks environment. Catalogs are the top-level containers in Databricks that store databases.\nIn the DFO Pacific Lakehouse, catalogs are used to organize the medallion architecture.\nYou may be able to see multiple catalogs in your Databricks environment.\n\n# See what catalogs are available\ndbGetQuery(connsql, \"SHOW CATALOGS\")\n\n\n\nSchemas\nCheck available schemas in bronze catalogue\n\ndbGetQuery(connsql, \"SHOW SCHEMAS IN bronze_pacific_prod\")\n\n# check available schemas in bronze nuseds\ndbGetQuery(connsql, \"SHOW TABLES IN bronze_pacific_prod.nuseds_v2_0\")\n\n# Query bronze nuseds_v2_0 table\nnuseds_activity_types &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.nuseds_v2_0.activity_types LIMIT 10\")\n\n\n\nTables\nCheck out the tables in FOS_v1_1\n\ndbGetQuery(connsql, \"SHOW TABLES IN bronze_pacific_prod.FOS_v1_1\")\n\n\n\n\nDescribe Tables\nDescribe a FOS_v1_1 table to extract metadata about a specific table, like what columns are available and their data types.\n\ndbGetQuery(connsql, \"DESCRIBE bronze_pacific_prod.FOS_v1_1.stock\")\n\n\n\nRunning Queries\nRun a query to extract data from a table in the Lakehouse.\n\n# Query bronze nuseds_v2_0 smu_cu_lookup table\nsmu_cu_lookup &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.nuseds_v2_0.smu_cu_lookup LIMIT 10\")\n\nsmu_cu_lookup\n\nConnect to the FOS data and aggregrate:\n\n# Query bronze FOS_v1_1 stock table\nstock &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.FOS_v1_1.stock\")\n\nTry using an aggregation function such as count and group by to get a count of the number of rows in the table.\n\n# Count the number of rows in the stock table\ndbGetQuery(connsql, \"SELECT COUNT(*) FROM bronze_pacific_prod.FOS_v1_1.stock\")",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#local-compute-versus-databricks-compute",
    "href": "how_to_guides/databricks_r_studio.html#local-compute-versus-databricks-compute",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Local Compute versus Databricks Compute",
    "text": "Local Compute versus Databricks Compute\nUsing Databricks compute should be minimized to avoid unnecessary costs. Instead, use your local computer for data exploration and analysis. Only use Databricks compute when you need to access new data, write data back to the Lakehouse, or run large-scale computations.\n\nWhen Databricks Compute is Used vs.¬†Local Compute\nDatabricks Compute:\n\nRunning SQL queries and data processing tasks on the data stored in the Azure Lakehouse.\nAny operations that involve querying the data, such as dbGetQuery(connsql, ‚ÄúSELECT * FROM ‚Ä¶‚Äù), are executed on Databricks compute resources.\n\nLocal Compute:\n\nWriting and executing R scripts that establish the connection to Databricks.\nProcessing the results returned from Databricks.\nManaging code with Git and GitHub from your local machine.\n\n\n\nTips to Minimize Databricks Compute Costs\nOptimize Queries: Write efficient SQL queries to minimize the amount of data processed and transferred to your local machine.\nUse Caching: Save data locally and don‚Äôt run dbGetQuery commands unless you are intentionally trying to get new data. Cache intermediate results when performing complex transformations to avoid redundant computations.\nFor example in R:\n\n# Query the data and save it to a variable\ndata &lt;- dbGetQuery(connsql, \"SELECT * FROM ...\") # Run the query only once or as needed for new data\n \n# Perform transformations on the data\ntransformed_data &lt;- data %&gt;% ...\n\n# Cache the transformed data\nsaveRDS(transformed_data, \"transformed_data.rds\")\n\n# Next time you need the transformed data, don't run the query again, instead load it from the cache or from your local copy\n\n# Load the transformed data from the cache\ntransformed_data &lt;- readRDS(\"transformed_data.rds\")\n\n# Continue working with the transformed data",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#using-a-github-repo-in-databricks",
    "href": "how_to_guides/databricks_r_studio.html#using-a-github-repo-in-databricks",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Using a GitHub Repo in Databricks",
    "text": "Using a GitHub Repo in Databricks\nOne of the advantages of using GitHub with Databricks is that you can easily import code from a GitHub repository into a Databricks notebook. This allows you to leverage the version control and collaboration features of GitHub while working in Databricks.\nHowever, there are a few things to keep in mind when using a GitHub repo in Databricks:\n\nImporting Code: You can import code from a GitHub repository into a Databricks notebook by specifying the URL of the GitHub repository. Databricks will clone the repository and import the code into the notebook.\nCode Execution: When you import code from a GitHub repository into a Databricks notebook, the code is executed in the Databricks environment. This means that any data access or processing tasks will be performed on Databricks compute resources.\nCode Management: While you can import code from a GitHub repository into a Databricks notebook, you are not able to push code changes back to the repository directly from Databricks. If you need to make changes to the code, you will need to do so in the GitHub repository and then re-import the code into the Databricks notebook.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "dsu_documentation.html",
    "href": "dsu_documentation.html",
    "title": "FADS Open Science Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/index.html"
  },
  {
    "objectID": "documentation_hub/how_to_guides/sharepoint_r.html",
    "href": "documentation_hub/how_to_guides/sharepoint_r.html",
    "title": "FADS Open Science Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/sharepoint_r.html"
  },
  {
    "objectID": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html",
    "href": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html",
    "title": "Fisheries Science Reports ‚Äì Data and Stats 101",
    "section": "",
    "text": "The 2025 Salmon Spring Week meetings reviewing the biological status of three stocks underscored the complex convergence of three frameworks: Canada‚Äôs Fish Stock Provisions (FSP), the Wild Salmon Policy (WSP), and the Precautionary Approach (PA).\n\n\nUnder the Fisheries Act, the Fish Stock Provisions mandate the identification of Limit Reference Points (LRPs) and the assignment of status (healthy, cautious, critical) for major stocks. This legal obligation ties directly into harvest control rules and rebuilding plan requirements when stocks fall below LRPs.\n\n\n\nThe WSP focuses on maintaining biological diversity and genetic distinctiveness by assessing stocks at the level of Conservation Units (CUs). It relies on the identification of biological benchmarks, including lower and upper benchmarks derived from stock-recruitment or percentile approaches, to classify CU status into red, amber, or green zones.\n\n\n\nThe PA emphasizes erring on the side of caution in the face of uncertainty. In assessments, this is operationalized via conservative assumptions (e.g., using lower bounds of estimates, assigning ‚Äúdata deficient‚Äù status) and requires clearly articulated risk statements and uncertainty quantification.\n\n\n\nKey terms‚ÄîLimit Reference Point (LRP) under FSP and Lower Benchmark under WSP‚Äîare conceptually aligned but contextually distinct (Table 1). Likewise, USR (Upper Stock Reference) in PA guidance shares conceptual space with the WSP Upper Benchmark. The challenge is reconciling population-specific CUs with stock management units (SMUs) required by FSP, especially when CUs differ in status or data availability. Several inconsistencies, such as those seen in Stikine, highlight the need for an integrated data architecture and harmonized terminology.\nTable 1. Relation of Terms Across Frameworks\n\n\n\n\n\n\n\n\n\n\nWSP Term\nPA/Fisheries Act Equivalent\nPurpose\nStatistical Approach Used\nEg. Quantitative Benchmarks / Model\n\n\n\n\nLower Benchmark\nLimit Reference Point (LRP)\nAvoid serious harm\nSR-based (Ricker), Risk-based, Percentile\n\\(S_{GEN}\\), 40% \\(S_{MSY}\\), 25th percentile\n\n\nUpper Benchmark\nUpper Stock Reference (USR)\nTrigger caution, reduce removals\nSR-based (Ricker), Percentile\n80% \\(S_{MSY}\\), 50th/75th percentile\n\n\nGreen Zone\nHealthy Zone\nFull exploitation allowed\nComposite scoring, quantitative SR input\nStatus \\(\\geq\\) USR or Upper Benchmark\n\n\nAmber Zone\nCautious Zone\nManagement action to avoid critical\nComposite integration, trend analysis\nBetween LRP and USR\n\n\nRed Zone\nCritical Zone\nMinimize removals, enable rebuilding\nEscapement + trend + SR benchmarks\nStatus &lt; LRP or Lower Benchmark\n\n\nStatus Assessment\nStock Assessment\nDetermine location relative to LRP/USR\nIntegrated Indicators + SR + Escapement\nCombined benchmark comparison",
    "crumbs": [
      "Home",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html#high-level-overview-stock-assessment-and-policy-alignment",
    "href": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html#high-level-overview-stock-assessment-and-policy-alignment",
    "title": "Fisheries Science Reports ‚Äì Data and Stats 101",
    "section": "",
    "text": "The 2025 Salmon Spring Week meetings reviewing the biological status of three stocks underscored the complex convergence of three frameworks: Canada‚Äôs Fish Stock Provisions (FSP), the Wild Salmon Policy (WSP), and the Precautionary Approach (PA).\n\n\nUnder the Fisheries Act, the Fish Stock Provisions mandate the identification of Limit Reference Points (LRPs) and the assignment of status (healthy, cautious, critical) for major stocks. This legal obligation ties directly into harvest control rules and rebuilding plan requirements when stocks fall below LRPs.\n\n\n\nThe WSP focuses on maintaining biological diversity and genetic distinctiveness by assessing stocks at the level of Conservation Units (CUs). It relies on the identification of biological benchmarks, including lower and upper benchmarks derived from stock-recruitment or percentile approaches, to classify CU status into red, amber, or green zones.\n\n\n\nThe PA emphasizes erring on the side of caution in the face of uncertainty. In assessments, this is operationalized via conservative assumptions (e.g., using lower bounds of estimates, assigning ‚Äúdata deficient‚Äù status) and requires clearly articulated risk statements and uncertainty quantification.\n\n\n\nKey terms‚ÄîLimit Reference Point (LRP) under FSP and Lower Benchmark under WSP‚Äîare conceptually aligned but contextually distinct (Table 1). Likewise, USR (Upper Stock Reference) in PA guidance shares conceptual space with the WSP Upper Benchmark. The challenge is reconciling population-specific CUs with stock management units (SMUs) required by FSP, especially when CUs differ in status or data availability. Several inconsistencies, such as those seen in Stikine, highlight the need for an integrated data architecture and harmonized terminology.\nTable 1. Relation of Terms Across Frameworks\n\n\n\n\n\n\n\n\n\n\nWSP Term\nPA/Fisheries Act Equivalent\nPurpose\nStatistical Approach Used\nEg. Quantitative Benchmarks / Model\n\n\n\n\nLower Benchmark\nLimit Reference Point (LRP)\nAvoid serious harm\nSR-based (Ricker), Risk-based, Percentile\n\\(S_{GEN}\\), 40% \\(S_{MSY}\\), 25th percentile\n\n\nUpper Benchmark\nUpper Stock Reference (USR)\nTrigger caution, reduce removals\nSR-based (Ricker), Percentile\n80% \\(S_{MSY}\\), 50th/75th percentile\n\n\nGreen Zone\nHealthy Zone\nFull exploitation allowed\nComposite scoring, quantitative SR input\nStatus \\(\\geq\\) USR or Upper Benchmark\n\n\nAmber Zone\nCautious Zone\nManagement action to avoid critical\nComposite integration, trend analysis\nBetween LRP and USR\n\n\nRed Zone\nCritical Zone\nMinimize removals, enable rebuilding\nEscapement + trend + SR benchmarks\nStatus &lt; LRP or Lower Benchmark\n\n\nStatus Assessment\nStock Assessment\nDetermine location relative to LRP/USR\nIntegrated Indicators + SR + Escapement\nCombined benchmark comparison",
    "crumbs": [
      "Home",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html#deep-dive-into-reference-points-and-benchmarks-in-stock-assessment",
    "href": "deep_dives/stock-assessment-data-101-for-fisheries-science-reports.html#deep-dive-into-reference-points-and-benchmarks-in-stock-assessment",
    "title": "Fisheries Science Reports ‚Äì Data and Stats 101",
    "section": "2. Deep Dive into Reference Points and Benchmarks in Stock Assessment",
    "text": "2. Deep Dive into Reference Points and Benchmarks in Stock Assessment\n\nBenchmarks vs Reference Points\n\nWSP Benchmarks (UBM, LBM): Derived from productivity models or empirical data. They are biological in nature and specific to a Conservation Unit (CU).\n\nUBM often corresponds to spawner abundance giving Maximum Sustainable Yield (MSY) or similar.\nLBM often relates to serious harm thresholds, such as a 50% probability of recovery under management actionHolt et al.¬†2009 Indica‚Ä¶.\n\nPrecautionary Approach Reference Points (USR, LRP): Defined at the Stock Management Unit (SMU) level, which aggregates multiple CUs. The LRP is legally required under the amended Fisheries Act.\n\nThe SMU-level LRP is often derived using the probability that all component CUs are above their respective LBM‚Äîcommonly at ‚â•50% probability (Holt et al.¬†2023)‚Ä¶.\nHow a Lower Benchmark Relates to the LRP\nThe CU-level LBM becomes the statistical input for deriving the SMU-level LRP. Specifically:\n\nLRPs can be derived via logistic regression models estimating the probability that all CUs within an SMU exceed their LBMs given a level of aggregate abundance (Holt et al.¬†2023)\nThis integrates the WSP framework into the Fisheries Act mandate for setting LRPs.\n\nIn essence, LRP ‚âà aggregate abundance where there‚Äôs ‚â•50% chance all CUs are above their LBM (Holt et al.¬†2023)\n\n\nSpawner-Recruitment (SR) Based Approaches\nThe gold standard for benchmark estimation when sufficient data are available.\n\na. Ricker Model\nThe most commonly used model for Pacific salmon: - Equation:\n\\[\nR = S \\cdot e^{(a - bS + \\varepsilon)}\n\\]\n\nWhere‚Ä¶\n\n\\(R\\): Number of recruits (offspring that survive to a specific life stage)\n\\(S\\): Number of spawners (parents that produced the recruits)\n\\(a\\): Log-scale productivity parameter; reflects the average number of recruits per spawner at low population density\n\\(b\\): Density-dependence parameter; determines how rapidly recruitment decreases as spawner abundance increases\n\\(\\varepsilon\\): Process error term accounting for environmental or stochastic variation not explained by the model\n\n\n\nb. Benchmarks Derived From SR Models\n\nLower Benchmark (LRP):\n\nOften set at ( S_{GEN} ): spawner abundance that leads to the LRP (e.g., ( S_{MSY} )) in one generation.\nOr, alternatively, a % of ( S_{MSY} ) (e.g., 40%).\n\nUpper Benchmark (USR):\n\nOften set at 80% of ( S_{MSY} ) or similar.\n\nTarget (TRP):\n\nUsually near ( S_{MSY} ), or the escapement yielding maximum sustainable yield.\n\n\n\n\n\nAlternative Methods\n\nc.¬†Percentile-Based Approaches\nUsed when data is limited: - Lower Benchmark: 25th percentile historical abundance. - Upper Benchmark: 50th or 75th percentile historical abundance.\n\n\nd.¬†Risk-Based Approaches\n\nUse Monte Carlo or stochastic simulations to assess risk thresholds by repeatedly sampling from uncertainty distributions.\nSimulations quantify the likelihood of stocks falling below defined critical thresholds (LRP).\nBenchmarks are selected to ensure a low probability (e.g., ‚â§5%) of breaching critical thresholds under various plausible scenarios.\n\n\n\ne. Closed-Loop Simulations (Management Strategy Evaluation - MSE)\n\nUtilize simulation frameworks to test effectiveness and robustness of different management strategies and Harvest Control Rules (HCRs).\nIncorporate uncertainties explicitly:\n\nObservation errors: uncertainties in monitoring and data collection.\nImplementation variability: differences between planned and actual management actions.\nEnvironmental stochasticity: variability due to environmental factors and climate change.\n\nOutcomes evaluated include biological sustainability, economic viability, and compliance with conservation objectives.",
    "crumbs": [
      "Home",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "News & Updates",
    "section": "",
    "text": "This section tracks meaningful changes to standards, workflows, and documentation.",
    "crumbs": [
      "Home",
      "Getting Started",
      "News & Updates"
    ]
  },
  {
    "objectID": "blog/index.html#recent-updates",
    "href": "blog/index.html#recent-updates",
    "title": "News & Updates",
    "section": "Recent updates",
    "text": "Recent updates\n\n2026-02 ‚Äî Ontology documentation consolidation\n\nintegrated ontology conventions guidebook into DSU docs\nremoved duplicate in-site term explorer path\nset WIDOCO as formal canonical reference path\n\n\n\n2026-02 ‚Äî FSAR workflow alignment for SPSR\n\nclarified SPSR as a primary intake pathway for FSAR-oriented datasets\nadded SPSR links in standards/workflow pages\naligned package/intake guidance around salmon data package + wizard flow",
    "crumbs": [
      "Home",
      "Getting Started",
      "News & Updates"
    ]
  },
  {
    "objectID": "blog/index.html#where-to-monitor-ongoing-changes",
    "href": "blog/index.html#where-to-monitor-ongoing-changes",
    "title": "News & Updates",
    "section": "Where to monitor ongoing changes",
    "text": "Where to monitor ongoing changes\n\nGitHub commits and PRs: https://github.com/dfo-pacific-science/data-stewardship-unit\nIssues and enhancement requests: https://github.com/dfo-pacific-science/data-stewardship-unit/issues",
    "crumbs": [
      "Home",
      "Getting Started",
      "News & Updates"
    ]
  },
  {
    "objectID": "blog/index.html#suggest-an-update",
    "href": "blog/index.html#suggest-an-update",
    "title": "News & Updates",
    "section": "Suggest an update",
    "text": "Suggest an update\nIf a workflow or reference page is out of date, open an issue and include:\n\npage URL\nwhat is incorrect\nproposed replacement text or links",
    "crumbs": [
      "Home",
      "Getting Started",
      "News & Updates"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About FADS Data Stewardship",
    "section": "",
    "text": "The Data Stewardship Unit (DSU) is situated within the Fisheries and Assessment Data Section (FADS), of the Stock Assessment and Research Division of the Pacific Region Science Branch of Fisheries and Oceans Canada (DFO).\nWe are a dedicated team committed to enhancing data management and stewardship practices within DFO. Our primary focus is on data related to Pacific Salmon within the DFO Pacific Region.\nOur goal is to provide guidance and assistance throughout the entire lifecycle of data management‚Äîfrom planning to publication. We strive to ensure our practices align with modern data standards, such as those outlined in the DFO Data Strategy and the Government of Canada‚Äôs digital data strategy. Our work is guided by the principles of making data findable, accessible, interoperable, and reusable (FAIR). Additionally, we highly value the principles of Indigenous data governance, such as ownership, control, access, and possession (OCAP), and we are committed to helping our collaborators respect and implement these principles in their work.",
    "crumbs": [
      "Home",
      "Getting Started",
      "About FADS Data Stewardship"
    ]
  },
  {
    "objectID": "cookbook/index.html",
    "href": "cookbook/index.html",
    "title": "Cookbook",
    "section": "",
    "text": "Task-focused recipes for recurring data stewardship workflows.",
    "crumbs": [
      "Home",
      "Cookbook",
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#current-recipes",
    "href": "cookbook/index.html#current-recipes",
    "title": "Cookbook",
    "section": "Current recipes",
    "text": "Current recipes\n\nStandardize Your Data\nIntegrate Data\nPublish Your Data\nFSAR Data Standardization Workflow",
    "crumbs": [
      "Home",
      "Cookbook",
      "Cookbook"
    ]
  },
  {
    "objectID": "cookbook/index.html#coming-next",
    "href": "cookbook/index.html#coming-next",
    "title": "Cookbook",
    "section": "Coming next",
    "text": "Coming next\n\nsalmon data package validation troubleshooting\nControlled vocabulary request walkthrough\nPublication checklists by data class",
    "crumbs": [
      "Home",
      "Cookbook",
      "Cookbook"
    ]
  },
  {
    "objectID": "documentation_hub/how_to_guides/databricks_r_studio.html",
    "href": "documentation_hub/how_to_guides/databricks_r_studio.html",
    "title": "FADS Open Science Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html"
  },
  {
    "objectID": "documentation_hub/how_to_guides/support_controlled_vocabs.html",
    "href": "documentation_hub/how_to_guides/support_controlled_vocabs.html",
    "title": "FADS Open Science Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/tutorials/support_controlled_vocabs.html"
  },
  {
    "objectID": "fads-portfolio.html",
    "href": "fads-portfolio.html",
    "title": "Data Services",
    "section": "",
    "text": "The Data Stewardship Unit (DSU) has developed a number of data services to support DFO's salmon data management needs. Our services include both internal DFO platforms that streamline data access and analysis for staff, and open access services that make salmon data and knowledge publicly available to researchers, stakeholders, and the broader community.\n    \n  \n\n\n\n\n\nOpen Access Services\n\n\n\n  \n    \n      Salmon Data Standards\n      \n    \n    Controlled vocabularies and ontology so salmon terms, code lists, and metadata stay aligned across projects.\n    \n      Ontology (WIDOCO)\n      Contributor Guidebook\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon Data Standards\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Salmon data arrives with inconsistent terms and code lists, making cross-project integration and analysis slow and error-prone.\n      Solution: A maintained set of SKOS controlled vocabularies plus the GC DFO Salmon Ontology that links concepts, code lists, and metadata fields.\n      Outcome: Biologists and stewards can align columns, codes, and documentation quickly, improving interoperability and trust.\n      Key Features:\n      \n        Published term lists with definitions for salmon entities and attributes\n        Ontology that maps relationships between terms and code lists\n        Versioned HTML site and GitHub sources for transparency\n        Supports ontology-aware data packages and code lists\n      \n      \n        Ontology (WIDOCO)\n        ¬†|¬†\n        Contributor Guidebook\n      \n      \n        \n      \n    \n  \n\n\n\n\n  \n    \n      Salmon Outlook Enhancements\n      \n    \n    Structured data collection and automated products for the annual Salmon Outlook.\n    \n      GitHub Repository\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon Outlook Enhancements\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Outlook data arrives quickly from many biologists and must be turned into tables, slides, and reports with little lead time.\n      Solution: Survey123 forms to collect Outlook inputs plus R workflows that auto-generate presentations and reports.\n      Outcome: Faster, repeatable Outlook products with clearer tables and less manual copy/paste.\n      Key Features:\n      \n        Survey123 forms for Outlook inputs and follow-up metrics (targets, reference points)\n        R code to build tailored slide decks and automated reports\n        Versioned data and code storage with planned publication to Open Government Portal\n      \n      \n        GitHub Repository\n      \n      \n        \n      \n    \n  \n\n\n\n\n  \n    \n      Salmon SMU-CU-DU Crosswalk\n      \n    \n    Open data linking SMUs, CUs, and DUs so assessments and reporting stay aligned.\n    \n      GitHub Repository\n      Data Access (link pending)\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon SMU-CU-DU Crosswalk\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Salmon groups are managed at SMU, CU, and DU levels, making it hard to join data and keep revisions straight.\n      Solution: A published crosswalk that links SMU, CU, and DU identifiers, tracks CU revisions, and provides a common vocabulary.\n      Outcome: Consistent assessments and communication across DFO and partners, with versioned releases every three months.\n      Key Features:\n      \n        Linkages between SMU, CU, and DU identifiers\n        CU revision tracking and life-history info\n        Version-controlled releases with changelog\n        Data dictionary for consistent interpretation\n      \n      \n        Preview image not included\n      \n    \n  \n\n\n\n\n  \n    \n      Salmon Escapement Estimates Toolkit\n      \n    \n    Shiny app to classify and review salmon escapement estimates for stock assessment.\n    \n      Access Toolkit\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon Escapement Estimates Toolkit\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Escapement estimates need consistent classification and review to support stock assessments.\n      Solution: A Shiny-based toolkit that walks users through classification and provides reference visuals.\n      Outcome: More consistent escapement inputs and transparency for assessment teams.\n      Key Features:\n      \n        Guided classification workflow for escapement estimates\n        Hosted Shiny app for easy access\n        Screenshots and examples for training\n      \n      \n        \n          Access the Salmon Escapement Estimates Toolkit\n        \n      \n      \n        \n      \n    \n  \n\n\n\n\n  \n    \n      metasalmon R package\n      \n    \n    R package for ontology-aware salmon data packages with unified semantic fields and code lists.\n    \n      Documentation\n      GitHub Repository\n    \n  \n\n\n\n\n  \n  \n    \n      metasalmon R package\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Salmon data arrives as messy spreadsheets without standardized semantics, making it hard for biologists and stewards to align columns, code lists, and metadata to the evolving DFO Salmon Ontology.\n      Solution: The Metasalmon 0.0.2 update unifies semantic fields (term_iri/term_type), streamlines code-list handling, and ships clear GPT/R workflows plus schemas and pkgdown docs to build frictionless-style CSV data packages.\n      Outcome: Teams can rapidly produce ontology-aware, validated data packages and code lists that are ready for analysis, sharing, and future knowledge-graph ingestion‚Äîwithout inventing IRIs or guessing schemas.\n      Key Features:\n      \n        Unified semantic fields (term_iri, term_type) across dictionary and codes schemas\n        SKOS concept schemes confined to code lists (concept_scheme_iri in codes.csv)\n        Updated GPT collaboration prompts and Custom GPT template to avoid invented IRIs\n        Validated R workflows (infer/validate/apply/package) with refreshed pkgdown docs and NEWS for 0.0.2\n        Clear ontology-gap handling with proposed terms and relationships (broader/narrower or superclass)\n      \n      \n        \n          View Package\n        \n      \n      \n        \n      \n    \n  \n\n\n\nInternal DFO Services\n\n\n\n  \n    \n      Qualark Data System\n      \n    \n    Shared PSC‚ÄìDFO data system to move Qualark sonar counts quickly for in-season run size estimates.\n    \n      [URL placeholder]\n    \n  \n\n\n\n\n  \n  \n    \n      Qualark Data System\n      √ó\n    \n    \n      \n        \n      \n      Challenge: In-season Qualark sonar counts need to move quickly and reliably between PSC and DFO teams to support forecasting.\n      Solution: A shared data system for Qualark test fishing that standardizes ingest, validation, and sharing of sonar-derived counts.\n      Outcome: Faster access to trusted counts for biologists and stewards during the season, reducing manual transfers.\n      Key Features:\n      \n        Shared PSC‚ÄìDFO data access for Qualark sonar counts\n        Standardized ingest and validation steps (details pending)\n        Planned delivery for in-season forecasting workflows\n      \n      More detail needed: workflows, tech stack, and access URL once available.\n      \n        Preview image not included\n      \n    \n  \n\n\n\n\n  \n    \n      Salmon Population Summary Repository\n      \n    \n    \n      A regional database that centralizes key salmon population index data to support stock assessments and improve transparency.\n    \n    \n      Open Application\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon Population Summary Repository\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Core salmon population index data (spawner, catch, recruitment, exploitation, age) sit in many spreadsheets and local systems, slowing FSAR production and reducing transparency.\n      Solution: A regional SPSR database that centralizes derived indices and metadata without replacing source systems, making data FAIR and consistent.\n      Outcome: Faster, more transparent assessments with versioned index data, documented methods, and better inter-branch communication.\n      Key Features:\n      \n        Regional compilation of spawner, catch, recruitment, exploitation, and hatchery metrics\n        Metadata on methods, assumptions, infilling, and data quality\n        Standardized reference data (benchmarks, limit/removal references)\n        Feeds for FSAR outputs and monitoring reviews\n        Version-controlled releases with changelog\n      \n      \n        \n          Access the Salmon Population Summary Repository\n        \n      \n      \n        \n      \n    \n  \n\n\n\n\n  \n    \n      Fishery Operations System Data Explorer\n      \n    \n    Self-service Power BI access to commercial salmon catch, openings, and vessel activity.\n    \n      Open Application\n      Documentation (pending)\n    \n  \n\n\n\n\n  \n  \n    \n      Fishery Operations System Data Explorer\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Staff needed FOS summary statistics but access required SQL accounts or manual data pulls from custodians.\n      Solution: A self-service Power BI app with pre-computed commercial salmon catch, openings, and vessel activity summaries.\n      Outcome: Faster access to trusted FOS summaries with less load on data custodians.\n      Key Features:\n      \n        Pre-computed catch, openings, and vessel activity summaries\n        Self-service Power BI access for authorized staff\n        Downloadable outputs for analysis and reporting\n      \n      \n        \n          Open the FOS Data Explorer\n        \n      \n      \n        \n      \n    \n  \n\n\n\n\n  \n    \n      Genetic Results Database\n      \n    \n    \n      A centralized repository for genetic stock identification and parentage-based tagging results to improve consistency and integration across DFO programs.\n    \n    \n      [URL placeholder]\n    \n  \n\n\n\n\n  \n  \n    \n      Genetic Results Database\n      √ó\n    \n\n      \n        \n      \n      Challenge: GSI and PBT results are scattered across teams with differing formats and vocabularies, making reuse and integration difficult‚Äîespecially as baselines and methods evolve.\n      Solution: A centralized Genetic Results Database that ingests, validates, and standardizes GSI/PBT outputs with common metadata and terminology.\n      Outcome: A single, traceable source for genetic results that supports mixed-stock analyses and interoperability with other systems.\n      Key Features:\n      \n        Authoritative repository for GSI and PBT results\n        Standardized data structures and controlled vocabulary\n        Automated ingest and validation pipelines\n        Versioned metadata capturing baselines, biomarkers, methods, and processing history\n        Planned interoperability with systems like STAMP\n      \n      Access URL and architecture details pending.\n      \n        Preview image not included\n      \n    \n  \n\n\n\n\n  \n    \n      Salmon Data Wiki\n      \n    \n    Collaborative wiki for salmon data sources, methods, and best practices.\n    \n      [URL placeholder]\n      Contribute (pending)\n    \n  \n\n\n\n\n  \n  \n    \n      Salmon Data Wiki\n      √ó\n    \n    \n      \n        \n      \n      Challenge: Knowledge about salmon data sources and methods is scattered across documents and teams.\n      Solution: A collaborative wiki to collect data source guides, methods, and best practices for DFO staff.\n      Outcome: Faster onboarding and more consistent practices for biologists, analysts, and stewards.\n      Key Features (planned):\n      \n        Pages for data sources, methods, and stewardship practices\n        Contribution workflow for teams to add and update content\n        Searchable, linkable entries across the site\n      \n      Needs: final URL, contribution process, and initial page list.\n      \n        Preview image not included",
    "crumbs": [
      "Home",
      "Getting Started",
      "Data Services"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html",
    "href": "how_to_guides/fsar-data-standardization-workflow.html",
    "title": "FSAR Data Standardization Workflow",
    "section": "",
    "text": "Use this workflow when preparing salmon datasets for Fisheries Science Advisory Report (FSAR) analysis, review, and operational SPSR intake.\nCanonical package specification (Markdown): Salmon data package specification\nIf you want a concrete walkthrough, use the companion page: FSAR to SPSR: End-to-End Example.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-1-define-scope-classification-and-destination",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-1-define-scope-classification-and-destination",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 1 ‚Äî Define scope, classification, and destination",
    "text": "Step 1 ‚Äî Define scope, classification, and destination\n\nConfirm dataset scope (what, where, when).\nConfirm data classification and sharing constraints.\nConfirm whether this package is intended for SPSR intake, external publication, or both.\nPolicy guidance: Publishing Data Externally",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-2-map-columns-to-canonical-ontology-terms",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-2-map-columns-to-canonical-ontology-terms",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 2 ‚Äî Map columns to canonical ontology terms",
    "text": "Step 2 ‚Äî Map columns to canonical ontology terms\n\nOpen GC DFO Salmon Ontology documentation.\nMap core variables (CU identifiers, year fields, key measurements).\nRecord full canonical IRIs for mapped terms.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-3-build-a-salmon-data-package",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-3-build-a-salmon-data-package",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 3 ‚Äî Build a salmon data package",
    "text": "Step 3 ‚Äî Build a salmon data package\nUse the salmon data package specification.\nAt minimum:\n\ndataset.csv\ntables.csv\ncolumn_dictionary.csv\ndata/*.csv\ncodes.csv (when categorical code lists apply)\n\nStarter assets:\n\nDownload minimal salmon data package example (zip)\nBrowse template package files",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-4-validate-package-quality",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-4-validate-package-quality",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 4 ‚Äî Validate package quality",
    "text": "Step 4 ‚Äî Validate package quality\nRun structural and semantic checks before submission.\nExample R checks:\nlibrary(metasalmon)\n\npkg_path &lt;- \"path/to/your/salmon-data-package\"\n\nvalidate_dictionary(file.path(pkg_path, \"column_dictionary.csv\"))\nvalidate_semantics(file.path(pkg_path, \"column_dictionary.csv\"))",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-5-align-to-current-spsr-intake-direction-cuyear-first",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-5-align-to-current-spsr-intake-direction-cuyear-first",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 5 ‚Äî Align to current SPSR intake direction (CUYear-first)",
    "text": "Step 5 ‚Äî Align to current SPSR intake direction (CUYear-first)\nBased on current SPSR execution direction, prepare uploads as one unified CU-year intake path:\n\nStart from current SPSR templates: https://spsr.dfo-mpo.gc.ca/download_sdp_templates\nTreat WSP CU assessment fields as the canonical status/benchmark layer.\nKeep FSAR fishery fields (catch, age, exploitation, etc.) as complementary data on the same CU-year pathway.\nDo not assume equivalence for similarly named fields (for example, FSAR TOTAL_SPAWNERS vs WSP SpnForAbd_Total); document mapping assumptions explicitly.\nKeep provenance explicit for transformed/derived values (for example, data source, year type, and method notes where applicable).\nKeep package metadata and upload CSV(s) versioned together.\n\nUseful SPSR references:\n\nRepo: https://github.com/dfo-pacific-science/salmon-population-summary-repository\nApp docs: https://spsr.dfo-mpo.gc.ca/documentation/\nUpload wizard: https://spsr.dfo-mpo.gc.ca/wizard/1/",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-6-upload-and-review-in-spsr",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-6-upload-and-review-in-spsr",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 6 ‚Äî Upload and review in SPSR",
    "text": "Step 6 ‚Äî Upload and review in SPSR\n\nUpload via SPSR wizard.\nResolve validation feedback.\nRe-upload if needed until intake checks are clean.\nRecord accepted upload version, intake date, and any profile-specific notes.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#step-7-publish-externally-if-in-scope-and-approved",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#step-7-publish-externally-if-in-scope-and-approved",
    "title": "FSAR Data Standardization Workflow",
    "section": "Step 7 ‚Äî Publish externally (if in scope and approved)",
    "text": "Step 7 ‚Äî Publish externally (if in scope and approved)\n\nFollow policy + approvals for external release.\nInclude citation/version metadata and release notes.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/fsar-data-standardization-workflow.html#fsar-readiness-checklist",
    "href": "how_to_guides/fsar-data-standardization-workflow.html#fsar-readiness-checklist",
    "title": "FSAR Data Standardization Workflow",
    "section": "FSAR readiness checklist",
    "text": "FSAR readiness checklist\n\nScope and classification confirmed\nVocabulary mappings documented with full IRIs\nSalmon data package files completed and validated\nSPSR template-aligned upload files prepared and submitted\nIntake assumptions/provenance notes archived\nExternal publication requirements completed (if applicable)",
    "crumbs": [
      "Home",
      "Common Workflows",
      "FSAR Data Standardization Workflow"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html",
    "href": "how_to_guides/openGovernmentPortalR.html",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "",
    "text": "This tutorial provides an overview for how to extract spatial data from the Open Government Portal, commonly referred to as Open Data. OGP is the Government of Canada‚Äôs official platform for publishing federal datasets, and DFO Science staff publish eligible data products there to meet open data policy requirements, ensure visibility of our work, and support reuse by partners and the public.\nThe code below shows how to leverage the resources available on OGP, making it easier to meet FAIR (Findable, Accessible, Interoperable, Reusable) principles and support reproducible science.\nThis tutorial reviews:\n\nQuerying ArcGIS REST services\nExtracting data from file geodatabases\nUsing ckanr to build more robust code\n\nThe examples cover the basics of mapping in R, including plotting both vector and raster data with ggplot(), and creating leaflet maps. Many online tutorials provide further detail for specifics of mapping in R if you need more background. For example, this tutorial from the University of Toronto provides a very good overview.\nNote that much of this content has been expanded from a similar tutorial created in the DFO Maritimes Region, but with DFO Pacific examples. Other clarifying comments have also been added.\n\n\nThis tutorial uses several spatial packages that require system libraries. If those dependencies are not installed locally, code chunks are skipped so site rendering can still complete. Static map outputs are cached under images/openDataSpatial/rendered/ so published pages keep map images even when spatial dependencies are unavailable.\n\nspatial_packages &lt;- c(\n  \"arcpullr\", \"dplyr\", \"ggplot2\", \"ggspatial\",\n  \"leaflet\", \"rnaturalearth\", \"sf\", \"terra\"\n)\noptional_packages &lt;- c(\"ckanr\")\n\nspatial_ready &lt;- all(vapply(spatial_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE))\nckan_ready &lt;- all(vapply(optional_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE))\n\nif (!spatial_ready) {\n  missing &lt;- spatial_packages[!vapply(spatial_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE)]\n  message(\"Skipping spatial execution; missing packages: \", paste(missing, collapse = \", \"))\n}\nif (!ckan_ready) {\n  message(\"Skipping CKAN examples; package 'ckanr' is not installed.\")\n}\n\n\nNote: Spatial code examples are skipped in this render because required system dependencies are not installed locally. Note: CKAN API examples are skipped in this render because ckanr is not installed.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#background",
    "href": "how_to_guides/openGovernmentPortalR.html#background",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "",
    "text": "This tutorial provides an overview for how to extract spatial data from the Open Government Portal, commonly referred to as Open Data. OGP is the Government of Canada‚Äôs official platform for publishing federal datasets, and DFO Science staff publish eligible data products there to meet open data policy requirements, ensure visibility of our work, and support reuse by partners and the public.\nThe code below shows how to leverage the resources available on OGP, making it easier to meet FAIR (Findable, Accessible, Interoperable, Reusable) principles and support reproducible science.\nThis tutorial reviews:\n\nQuerying ArcGIS REST services\nExtracting data from file geodatabases\nUsing ckanr to build more robust code\n\nThe examples cover the basics of mapping in R, including plotting both vector and raster data with ggplot(), and creating leaflet maps. Many online tutorials provide further detail for specifics of mapping in R if you need more background. For example, this tutorial from the University of Toronto provides a very good overview.\nNote that much of this content has been expanded from a similar tutorial created in the DFO Maritimes Region, but with DFO Pacific examples. Other clarifying comments have also been added.\n\n\nThis tutorial uses several spatial packages that require system libraries. If those dependencies are not installed locally, code chunks are skipped so site rendering can still complete. Static map outputs are cached under images/openDataSpatial/rendered/ so published pages keep map images even when spatial dependencies are unavailable.\n\nspatial_packages &lt;- c(\n  \"arcpullr\", \"dplyr\", \"ggplot2\", \"ggspatial\",\n  \"leaflet\", \"rnaturalearth\", \"sf\", \"terra\"\n)\noptional_packages &lt;- c(\"ckanr\")\n\nspatial_ready &lt;- all(vapply(spatial_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE))\nckan_ready &lt;- all(vapply(optional_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE))\n\nif (!spatial_ready) {\n  missing &lt;- spatial_packages[!vapply(spatial_packages, requireNamespace, FUN.VALUE = logical(1), quietly = TRUE)]\n  message(\"Skipping spatial execution; missing packages: \", paste(missing, collapse = \", \"))\n}\nif (!ckan_ready) {\n  message(\"Skipping CKAN examples; package 'ckanr' is not installed.\")\n}\n\n\nNote: Spatial code examples are skipped in this render because required system dependencies are not installed locally. Note: CKAN API examples are skipped in this render because ckanr is not installed.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#accessing-open-data-via-arcgis-rest",
    "href": "how_to_guides/openGovernmentPortalR.html#accessing-open-data-via-arcgis-rest",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Accessing Open Data via ARCGIS REST",
    "text": "Accessing Open Data via ARCGIS REST\nThe arcpullr package is a great way to access ArcGIS REST services, which are often used for storing spatial data on Open Data. You can use the get_spatial_layer() function to retrieve a spatial layer from an ArcGIS server.\n\nExample: Pacific Recreational Fishery Salmon Head Depots\nThere can many URLs associated with each dataset. The ‚ÄúArcGIS Rest URL‚Äù we need to feed into the get_spatial_layer() function should be the one that includes ‚Äúarcgis/rest/services‚Äù.\nFor example, with the Pacific Recreational Fishery Salmon Head dataset, the URL we are interested in is https://egisp.dfo-mpo.gc.ca/arcgis/rest/services/open_data_donnees_ouvertes/pacific_recreational_fishery_salmon_head_depots/MapServer/.\nIf you have trouble tracking this down, go to the appropriate dataset page on Open Data.\nThen, under the Data and Resources section, find the item labelled ESRI REST (there is both an English and French example). Left click on the Explore dropdown item, then right click on Go to resource, and then left click on Copy link. This then copies the link to the URL you need.\n\n\n\ngoToResource\n\n\nIt is important to ensure the appropriate layer is specified. In this example, the ‚Äú0‚Äù at the end of the address denotes the English version, while a 1 represents French (see layers within the MapServer page). They also do not always correspond to language. For example, in the Pacific Commercial Salmon In Season Catch Estimates dataset, layers 0 to 3 represent gill nets, troll, seine and Pacific Fishery Management Areas, respectively.\n\nsalmonDepots = arcpullr::get_spatial_layer(\"https://egisp.dfo-mpo.gc.ca/arcgis/rest/services/open_data_donnees_ouvertes/pacific_recreational_fishery_salmon_head_depots/MapServer/0\")\n\n\n\nCreating a map\nThere are some great packages to get basemaps in R. The rnaturalearth package is a good option for getting country borders and other geographical features. You can use the ne_countries() function to get country borders.\nIn British Columbia, rnaturalearth may not have the level of detail required (e.g., some islands are missing). The bcmaps package has a lot of detailed basemaps and spatial data for the BC coast, and the bc_bound_hres() function is particularly useful for mapping coastline.\nSince the data used in the tutorial are for the entire BC coast, rnaturalearth is used in the examples below.\n\n# Get coastline for North America \ncoast = ne_countries(scale = 10, returnclass = c(\"sf\"), continent = \"North America\") \n\nBecause the coastline basemap data span all of North America, it may be useful to crop the plot to the bounding box of the data (BC coast). The st_bbox() function from the sf package allows you to get the bounding box of a spatial object. You can use the xlim and ylim arguments in the coord_sf() function to set the limits of the x and y axes.",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#accessing-file-geodatabases",
    "href": "how_to_guides/openGovernmentPortalR.html#accessing-file-geodatabases",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Accessing file geodatabases",
    "text": "Accessing file geodatabases\nFile geodatabases are a proprietary file format developed by ESRI for spatial and non-spatial data. Many spatial datasets are uploaded to Open Data as file geodatabases, and it is possible to download and extract them with R!\nThe example below shows how to access the file geodatabase from the floating infrastructure dataset on Open Data\n\nCreating temporary directories\nFor exploring data, it is often helpful to create temporary directories to store and extract data. This can be easier than downloading the data directly and setting your directories.\n\n# Create a temporary directory to store the downloaded zip file\ntemp_dir = tempdir()\n\n# Define the path for the downloaded zip file inside the temp directory\nzip_file = file.path(temp_dir, \"zipPath.gdb.zip\")\n\n# Define the directory where the zip contents will be extracted\n# This is a relative path, so files will be extracted into the current working directory\nunzip_dir = file.path(\"extracted_fgdb\")\n\n# Download the dataset zip file. To get the correct link, follow the same steps as in the Pacific Recreational Fishery Salmon Head Depots example above, but with the \"FDGB/GDB\" item in Data and resources \ndownload.file(\n  \"https://api-proxy.edh-cde.dfo-mpo.gc.ca/catalogue/records/049770ef-6cb3-44ee-afc8-5d77d6200a12/attachments/floating_infrastructure-docks.gdb.zip\",\n  destfile = zip_file,\n  mode = \"wb\"\n)\n\n# Create the extraction directory if it doesn't already exist\ndir.create(unzip_dir, showWarnings = FALSE)\n\n# Unzip the downloaded file into the extraction directory\nunzip(zip_file, exdir = unzip_dir)\n\n# List the files extracted to verify contents\nlist.files(unzip_dir)\n\n# Load the layers available in the extracted .gdb file\n# Copy the name from the list.files() command. This should match the name of the gdb from the URL above\nlayers = st_layers(file.path(unzip_dir, \"floating_infrastructure-docks.gdb\"))\n\n# Turn layers from a list into a dataframe\n# layers has 1 entry called docks, so now a df called 'docks' will be created\nfor(l in layers$name){\n  message(\"Reading layer: \", l)\n  assign(l, st_read(file.path(unzip_dir, \"floating_infrastructure-docks.gdb\"), layer = l, quiet = TRUE))\n}\n\n\n\nMap the data",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#plotting-raster-data",
    "href": "how_to_guides/openGovernmentPortalR.html#plotting-raster-data",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Plotting raster data",
    "text": "Plotting raster data\nAn example is shown here for accessing and plotting raster data stored in a geodatabase. You can follow the example in the Maritimes tutorial for accessing raster data from tiff files, if you are interested.\nThe example I‚Äôm using is from the recreational vessel traffic model on the BC coast.\n\n# Follow steps above to create a temporary folder to download & extract geodatabase info\ntemp_dir = tempdir()\nzip_file = file.path(temp_dir, \"bc_boating_model.zip\")\nunzip_dir = file.path(temp_dir, \"bc_boating_data\")\n\n# Download the dataset zip file. To get the correct link, follow the same steps as in the Pacific Recreational Fishery Salmon Head Depots example above, but with the \"FDGB/GDB\" item in Data and resources\ndownload.file(\n  url = \"https://api-proxy.edh-cde.dfo-mpo.gc.ca/catalogue/records/fed5f00f-7b17-4ac2-95d6-f1a73858dac0/attachments/Recreational_Boating_Data_Model.gdb.zip\",\n  destfile = zip_file,\n  mode = \"wb\"\n)\n\n# Unzip the contents\ndir.create(unzip_dir, showWarnings = FALSE)\nunzip(zip_file, exdir = unzip_dir)\n\n# List the file names you just downloaded \nboat_file = list.files(unzip_dir, full.names = TRUE)",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#mapping-the-survey-effort-data-with-ggplot",
    "href": "how_to_guides/openGovernmentPortalR.html#mapping-the-survey-effort-data-with-ggplot",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Mapping the survey effort data with ggplot",
    "text": "Mapping the survey effort data with ggplot\nUnfortunately, raster data in this format cannot be directly mapped with ggplot. But, you can convert it to a data frame and keep the coordinate positions.\nIf you read through the metadata, you‚Äôll see that that many types of data are available in this geodatabase (e.g., point data, raster, vector grid). The data being plotted in this example below is the surveyeffort raster dataset, which is specified using the sub argument in the rast() function.\n\n\n\n\n\n\n\n\n\n\nMaking Leaflet Maps\nLeaflet maps are interactive maps that can be a great way to explore data. Leaflet maps can also be great for point data, since you can create popup labels where you can click and view your attribute data.\n\n# Convert raster to leaflet-compatible format\npal = colorNumeric(palette = \"magma\", domain = values(rast_proj), na.color = \"transparent\")\n\n# Create leaflet map\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.OceanBasemap) %&gt;%\n  addRasterImage(rast_proj, colors = pal, opacity = 0.7, project = T) %&gt;%\n  addLegend(pal = pal, values = values(rast_proj),\n            title = \"Survey Effort\",\n            position = \"bottomright\")",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#using-ckanr-to-build-more-robust-code",
    "href": "how_to_guides/openGovernmentPortalR.html#using-ckanr-to-build-more-robust-code",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Using ckanr to build more robust code",
    "text": "Using ckanr to build more robust code\nSometimes, IT needs to upgrade server hardware or software, which can result in changes to the ArcGIS REST service URLs (i.e., the portion like this: https://egisp.dfo-mpo.gc.ca/arcgis/rest/services). However, the Universally Unique Identifier (UUID) associated with each dataset remains unchanged.\nIn the Open Data portal, the UUID is the final part of the URL after ‚Äúdataset‚Äù. For example, in the Pacific Recreational Salmon Head Depots dataset (https://open.canada.ca/data/dataset/3cc03bbf-d59f-4812-996e-ddc52e0ba99e), the UUID is 3cc03bbf-d59f-4812-996e-ddc52e0ba99e.\nTo make our script more robust, we can use the CKAN API. CKAN is a data management system that supports publishing, sharing, and discovering datasets. The ckanr package in R allows us to interact with CKAN-based portals, such as Open Data. You can use the ckanr_setup() function to establish a connection.\nInstead of hardcoding the ArcGIS REST URL in our script (which may change) we can use the CKAN API to reference the stable UUID. This allows us to dynamically retrieve the current ArcGIS REST URL associated with that UUID, ensuring our code remains functional even if updates occur.\n\nDownloading data\nWe‚Äôll use the Pacific Recreational Fishery Salmon Head Depots data as an example. We can use the ckanr package to extract the ArcGIS REST URL by referencing the UUID. This may seem like an extra step while you‚Äôre writing your code, but it will save you time in the long run if the URLs change (it can happen!!)\n\nckanr::ckanr_setup(\"https://open.canada.ca/data\")\nuuid = \"3cc03bbf-d59f-4812-996e-ddc52e0ba99e\"\npkg = ckanr::package_show(uuid)\nresources = pkg$resources\ndf_resources = as.data.frame(do.call(rbind, resources))\n\n# Now you can grab the correct ESRI Rest URL (\"url\"). Make sure to search for that format, and also specify you want the English (\"En\") version and not French\nsalmon_url = unlist(df_resources[df_resources$format == \"ESRI REST\" & sapply(df_resources$language, function(x) \"en\" %in% x), \"url\"])\n\n\n\nSearching for data\nYou can also use the ckanr package to search for data using the package_search() function. You can use the q argument to search for key words, and the fq argument to specify file types.\n\nckanr::ckanr_setup(\"https://open.canada.ca/data\")\nsearch_results = ckanr::package_search(q = \"Pacific salmon\")\nsearch_results # Note that UUID also shows up here after the &lt;CKAN Package&gt; text\n\n# You  can also search specifically for spatial data (e.g., shapefiles, geodatabases, etc.) by specifying the fq argument\nsearch_results = ckanr::package_search(\n  q = \"Pacific salmon\",\n  fq = 'res_format:(SHP OR GDB OR GeoJSON OR KML OR CSV)' # CSVs sometimes contain spatial data, sometimes not\n)",
    "crumbs": [
      "Home",
      "Data Access & Platforms",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "Fisheries & Assessment Data Section\n  \n  \n    Use this hub to standardize data, package it correctly, and publish with confidence.\n  \n  \n    Start with the Semantic Salmon System\n    Run the FSAR workflow\n    Browse ontology documentation\n  \n\n\n\n\n  \n    \n    Follow the canonical sequence: ontology ‚Üí salmon data package ‚Üí metasalmon ‚Üí Salmon Data GPT ‚Üí SPSR.\n  \n  \n    \n    Follow the end-to-end workflow with validation and packaging checkpoints.\n  \n  \n    \n    Use the canonical WIDOCO site for classes, properties, and term identifiers.\n  \n  \n    \n    Use the salmon data package specification and starter artifacts.\n  \n  \n    \n    Review publishing pathways and open science guidance for external release.\n  \n\n\n\n\n\n  \n    Start Here\n    \n    Get the full end-to-end sequence front and center before diving into details.\n  \n  \n    Workflow\n    \n    Move from source data through validation and packaging with a practical, end-to-end checklist.\n  \n  \n    Publishing\n    \n    Choose publication routes and prepare outputs aligned with open science constraints.\n  \n\n\n\n\n\nFSAR to SPSR: End-to-End ExampleHow-to ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readStandards & Workflow ChangelogReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 2 min readIRI and Identifier Policy (Practical)Reference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readSemantic Salmon System ‚Äî Start HereReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readProvenance and MaintenanceReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readSchema vs Data (TBox vs ABox)Reference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min read\n\n\n\n\n\n\nGC DFO Salmon Ontology88 views\n\n\nFSAR Data Standardization Workflow79 views\n\n\nSalmon data package specification65 views\n\n\nPublishing Data Externally57 views\n\n\nOpen Government Portal with R44 views\n\n\n\n\n\nScroll and scan everything in one place, or filter by title/type with the quick search box.\n\n  \n  \n    All\n    How-to\n    Reference\n    Tutorial\n    Deep dive\n    Cookbook\n    News\n  \n\n\n\n  \n    \n      \n        Title\n        Type\n        Published\n        Updated\n        Read time\n      \n    \n    \n\n\nFSAR to SPSR: End-to-End Example\n\n\nHow-to\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nStandards & Workflow Changelog\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nIRI and Identifier Policy (Practical)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nSemantic Salmon System ‚Äî Start Here\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nProvenance and Maintenance\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nSchema vs Data (TBox vs ABox)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nHow to Request a New Ontology Term\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nFSAR Data Standardization Workflow\n\n\nHow-to\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon data package + SPSR intake path\n\n\nReference\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nGlossary & Acronyms\n\n\nReference\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nCookbook\n\n\nCookbook\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nNews & Updates\n\n\nNews\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nThe Semantic Salmon Data Ecosystem\n\n\nReference\n\n\n2026-02-11\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nIntegrate Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nPublish Your Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nStandardize Your Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon Data Standards Hub\n\n\nReference\n\n\n2025-12-16\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon data package specification\n\n\nReference\n\n\n2025-12-08\n\n\n2026-02-25\n\n\n8 min\n\n\n\n\nControlled Vocabulary Governance Workflow\n\n\nTutorial\n\n\n2025-04-10\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nClass vs SKOS Concept: Decision Tree\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nFull Conventions (Canonical Source)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOntology Conventions Guidebook\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOWL Classes\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOWL Properties\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nQuick Reference\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nSKOS Vocabularies\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nGC DFO Salmon Ontology\n\n\nReference\n\n\n2025-11-04\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nDarwin Core, GBIF, and OBIS Reference Guide\n\n\nReference\n\n\n2025-04-10\n\n\n2026-02-20\n\n\n2 min\n\n\n\n\nR Packages\n\n\nReference\n\n\n2025-06-06\n\n\n2026-02-18\n\n\n1 min\n\n\n\n\nFisheries Science Reports ‚Äì Data and Stats 101\n\n\nDeep dive\n\n\n2025-06-16\n\n\n2026-02-09\n\n\n5 min\n\n\n\n\nConnect to the Azure Lakehouse from R\n\n\nHow-to\n\n\n2025-03-13\n\n\n2025-10-03\n\n\n7 min\n\n\n\n\nAccessing Spatial Data from the Open Government Portal\n\n\nHow-to\n\n\n2025-08-11\n\n\n2025-08-11\n\n\n6 min\n\n\n\n\nData Practices for Open Science ‚Äì 3 Quick Tips\n\n\nReference\n\n\n2025-07-23\n\n\n2025-07-23\n\n\n3 min\n\n\n\n\nAccess Data in SharePoint from R\n\n\nHow-to\n\n\n2025-03-31\n\n\n2025-06-09\n\n\n2 min\n\n\n\n\nHosting Data and Code\n\n\nReference\n\n\n2025-04-04\n\n\n2025-04-04\n\n\n4 min\n\n\n    \n  \n\n\n\n\n‚Ü©Ô∏éÔ∏è Return to the Internal FADS SharePoint Wiki",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#start-by-goal",
    "href": "index.html#start-by-goal",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "Follow the canonical sequence: ontology ‚Üí salmon data package ‚Üí metasalmon ‚Üí Salmon Data GPT ‚Üí SPSR.\n  \n  \n    \n    Follow the end-to-end workflow with validation and packaging checkpoints.\n  \n  \n    \n    Use the canonical WIDOCO site for classes, properties, and term identifiers.\n  \n  \n    \n    Use the salmon data package specification and starter artifacts.\n  \n  \n    \n    Review publishing pathways and open science guidance for external release.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#featured-pathways",
    "href": "index.html#featured-pathways",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "Start Here\n    \n    Get the full end-to-end sequence front and center before diving into details.\n  \n  \n    Workflow\n    \n    Move from source data through validation and packaging with a practical, end-to-end checklist.\n  \n  \n    Publishing\n    \n    Choose publication routes and prepare outputs aligned with open science constraints.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#latest-updates",
    "href": "index.html#latest-updates",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "FSAR to SPSR: End-to-End ExampleHow-to ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readStandards & Workflow ChangelogReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 2 min readIRI and Identifier Policy (Practical)Reference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readSemantic Salmon System ‚Äî Start HereReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readProvenance and MaintenanceReference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min readSchema vs Data (TBox vs ABox)Reference ¬∑ Published 2026-02-20 ¬∑ Updated 2026-02-25 ¬∑ 1 min read",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#most-read-this-week",
    "href": "index.html#most-read-this-week",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "GC DFO Salmon Ontology88 views\n\n\nFSAR Data Standardization Workflow79 views\n\n\nSalmon data package specification65 views\n\n\nPublishing Data Externally57 views\n\n\nOpen Government Portal with R44 views",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#browse-all-articles",
    "href": "index.html#browse-all-articles",
    "title": "FADS Open Science Hub üêüüíæ",
    "section": "",
    "text": "Scroll and scan everything in one place, or filter by title/type with the quick search box.\n\n  \n  \n    All\n    How-to\n    Reference\n    Tutorial\n    Deep dive\n    Cookbook\n    News\n  \n\n\n\n  \n    \n      \n        Title\n        Type\n        Published\n        Updated\n        Read time\n      \n    \n    \n\n\nFSAR to SPSR: End-to-End Example\n\n\nHow-to\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nStandards & Workflow Changelog\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nIRI and Identifier Policy (Practical)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nSemantic Salmon System ‚Äî Start Here\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nProvenance and Maintenance\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nSchema vs Data (TBox vs ABox)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nHow to Request a New Ontology Term\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nFSAR Data Standardization Workflow\n\n\nHow-to\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon data package + SPSR intake path\n\n\nReference\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nGlossary & Acronyms\n\n\nReference\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nCookbook\n\n\nCookbook\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nNews & Updates\n\n\nNews\n\n\n2026-02-17\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nThe Semantic Salmon Data Ecosystem\n\n\nReference\n\n\n2026-02-11\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nIntegrate Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nPublish Your Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n1 min\n\n\n\n\nStandardize Your Data\n\n\nReference\n\n\n2026-02-09\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon Data Standards Hub\n\n\nReference\n\n\n2025-12-16\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nSalmon data package specification\n\n\nReference\n\n\n2025-12-08\n\n\n2026-02-25\n\n\n8 min\n\n\n\n\nControlled Vocabulary Governance Workflow\n\n\nTutorial\n\n\n2025-04-10\n\n\n2026-02-25\n\n\n2 min\n\n\n\n\nClass vs SKOS Concept: Decision Tree\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nFull Conventions (Canonical Source)\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOntology Conventions Guidebook\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOWL Classes\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nOWL Properties\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nQuick Reference\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nSKOS Vocabularies\n\n\nReference\n\n\n2026-02-20\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nGC DFO Salmon Ontology\n\n\nReference\n\n\n2025-11-04\n\n\n2026-02-20\n\n\n1 min\n\n\n\n\nDarwin Core, GBIF, and OBIS Reference Guide\n\n\nReference\n\n\n2025-04-10\n\n\n2026-02-20\n\n\n2 min\n\n\n\n\nR Packages\n\n\nReference\n\n\n2025-06-06\n\n\n2026-02-18\n\n\n1 min\n\n\n\n\nFisheries Science Reports ‚Äì Data and Stats 101\n\n\nDeep dive\n\n\n2025-06-16\n\n\n2026-02-09\n\n\n5 min\n\n\n\n\nConnect to the Azure Lakehouse from R\n\n\nHow-to\n\n\n2025-03-13\n\n\n2025-10-03\n\n\n7 min\n\n\n\n\nAccessing Spatial Data from the Open Government Portal\n\n\nHow-to\n\n\n2025-08-11\n\n\n2025-08-11\n\n\n6 min\n\n\n\n\nData Practices for Open Science ‚Äì 3 Quick Tips\n\n\nReference\n\n\n2025-07-23\n\n\n2025-07-23\n\n\n3 min\n\n\n\n\nAccess Data in SharePoint from R\n\n\nHow-to\n\n\n2025-03-31\n\n\n2025-06-09\n\n\n2 min\n\n\n\n\nHosting Data and Code\n\n\nReference\n\n\n2025-04-04\n\n\n2025-04-04\n\n\n4 min\n\n\n    \n  \n\n\n\n\n‚Ü©Ô∏éÔ∏è Return to the Internal FADS SharePoint Wiki",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html",
    "href": "reference_info/data_standards/index.html",
    "title": "Salmon Data Standards Hub",
    "section": "",
    "text": "This hub is the starting point for standardizing, integrating, and publishing salmon data with shared semantics.\nFor the branded front-door view of the full system path, start here:",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html#start-here-common-tasks",
    "href": "reference_info/data_standards/index.html#start-here-common-tasks",
    "title": "Salmon Data Standards Hub",
    "section": "Start here (common tasks)",
    "text": "Start here (common tasks)\n\nI need the full system path first ‚Üí Semantic Salmon System ‚Äî Start Here\nI need to prepare data for an FSAR ‚Üí FSAR Data Standardization Workflow\nI need to submit FSAR data to SPSR ‚Üí FSAR to SPSR: End-to-End Example and Salmon data package + SPSR intake path\nI need standard terms and definitions ‚Üí GC DFO Salmon Ontology documentation\nI need package structure/spec details ‚Üí Salmon data package specification\nI need canonical markdown spec source ‚Üí https://github.com/dfo-pacific-science/smn-data-pkg/blob/main/SPECIFICATION.md\nI need practical identifier rules ‚Üí IRI and Identifier Policy\nI need to understand how ontology + salmon data package + tools fit together ‚Üí Semantic Salmon Data Ecosystem\nI need to publish data ‚Üí Publish Your Data and Publishing Data Externally\nI need recent changes/status ‚Üí Standards & Workflow Changelog",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html#start-by-role",
    "href": "reference_info/data_standards/index.html#start-by-role",
    "title": "Salmon Data Standards Hub",
    "section": "Start by role",
    "text": "Start by role\n\nData producer (scientist/analyst)\n\nStandardize Your Data\nIntegrate Data\nFSAR to SPSR: End-to-End Example\n\nOntology contributor\n\nOntology Conventions Guidebook\nClass vs SKOS Decision Tree\nHow to Request a New Ontology Term\n\nData steward/reviewer\n\nSalmon data package + SPSR intake path\nIRI and Identifier Policy\nPublishing Data Externally",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html#standards-map",
    "href": "reference_info/data_standards/index.html#standards-map",
    "title": "Salmon Data Standards Hub",
    "section": "Standards map",
    "text": "Standards map\n\n1) Controlled vocabularies and ontology\nUse these to anchor variable meaning to shared definitions and stable IRIs.\n\nGC DFO Salmon Ontology documentation\nOntology Conventions Guidebook\nClass vs SKOS Decision Tree\nHow to Request a New Ontology Term\n\n\n\n2) Package your dataset with a salmon data package\nUse salmon data package metadata files to describe your dataset, tables, columns, and controlled codes.\n\nSalmon data package specification\nIRI and Identifier Policy\nSalmon data package + SPSR intake path\n\n\n\n3) Use implementation workflows\n\nStandardize Your Data\nIntegrate Data\nFSAR Data Standardization Workflow\nFSAR to SPSR: End-to-End Example\n\n\n\n4) Track what changed\n\nStandards & Workflow Changelog",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html#downloadable-starter-template",
    "href": "reference_info/data_standards/index.html#downloadable-starter-template",
    "title": "Salmon Data Standards Hub",
    "section": "Downloadable starter template",
    "text": "Downloadable starter template\nA minimal starter package is available here:\n\nDownload minimal salmon data package example (zip)\nBrowse example files in repo",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/index.html#need-help",
    "href": "reference_info/data_standards/index.html#need-help",
    "title": "Salmon Data Standards Hub",
    "section": "Need help?",
    "text": "Need help?\n\nOpen an issue: https://github.com/dfo-pacific-science/data-stewardship-unit/issues\nContact DSU: mailto:FADSDataStewardship-GestiondesdonneesSFDA@dfo-mpo.gc.ca",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Salmon Data Standards Hub"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html",
    "href": "reference_info/data_standards/iri-identifier-policy.html",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "",
    "text": "This page defines practical rules for identifiers across DSU standards and workflows.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html#core-rule",
    "href": "reference_info/data_standards/iri-identifier-policy.html#core-rule",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "Core rule",
    "text": "Core rule\nUse full canonical IRIs for semantic term links.\nCanonical base: https://w3id.org/gcdfo/salmon#",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html#what-goes-where",
    "href": "reference_info/data_standards/iri-identifier-policy.html#what-goes-where",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "What goes where",
    "text": "What goes where\n\nHuman-facing name ‚Üí label fields (rdfs:label, skos:prefLabel, or local label columns)\nMachine code value ‚Üí code fields (e.g., skos:notation, coded columns)\nStable term identifier ‚Üí full IRI in semantic link columns (e.g., standard_term_iri)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html#do-dont",
    "href": "reference_info/data_standards/iri-identifier-policy.html#do-dont",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "Do / Don‚Äôt",
    "text": "Do / Don‚Äôt\n\nDo\n\nstore full https://w3id.org/gcdfo/salmon#... IRIs in mapping metadata\npreserve the exact IRI used at publication/submission time\ndocument crosswalks when terms are replaced\n\n\n\nDon‚Äôt\n\nuse partial/abbreviated IRIs in package metadata\nput machine IDs into display labels\nsilently swap identifiers without change notes",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html#example-mapping-row",
    "href": "reference_info/data_standards/iri-identifier-policy.html#example-mapping-row",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "Example mapping row",
    "text": "Example mapping row\nvariable_name,label,standard_term_iri\nEsc,Escapement,https://w3id.org/gcdfo/salmon#Escapement",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/iri-identifier-policy.html#why-this-matters",
    "href": "reference_info/data_standards/iri-identifier-policy.html#why-this-matters",
    "title": "IRI and Identifier Policy (Practical)",
    "section": "Why this matters",
    "text": "Why this matters\nConsistent full IRIs reduce ambiguity and make salmon data package validation, SPSR intake, and downstream integration more reliable.",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "IRI and Identifier Policy"
    ]
  },
  {
    "objectID": "reference_info/data_standards/publish-data.html",
    "href": "reference_info/data_standards/publish-data.html",
    "title": "Publish Your Data",
    "section": "",
    "text": "Active Workflow\n  Canonical Terms Required\n  Internal + External Paths",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/publish-data.html#semantic-salmon-system-entry-path-front-and-center",
    "href": "reference_info/data_standards/publish-data.html#semantic-salmon-system-entry-path-front-and-center",
    "title": "Publish Your Data",
    "section": "Semantic Salmon System ‚Äî entry path (front and center)",
    "text": "Semantic Salmon System ‚Äî entry path (front and center)\n\n  Before publishing, confirm this chain is complete\n  \n    DFO Salmon Ontology mappings are finalized\n    Salmon data package metadata is complete\n    metasalmon validation is clean\n    Salmon Data GPT (optional) is used only to draft, not to bypass validation\n    SPSR intake is complete for FSAR-oriented workflows\n  \n\nQuick links:\n\nSemantic Salmon System ‚Äî Start Here\nGC DFO Salmon Ontology documentation\nSalmon data package specification\nSalmon data package + SPSR intake path\nFSAR to SPSR: End-to-End Example",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/publish-data.html#cookbook-guide",
    "href": "reference_info/data_standards/publish-data.html#cookbook-guide",
    "title": "Publish Your Data",
    "section": "Cookbook Guide",
    "text": "Cookbook Guide\n\n\nStep 1: Finalize Metadata\n\nEnsure your dataset has:\n\ncomplete data dictionary entries\nclear definitions and units\ncanonical full IRIs for mapped terms\ncontact + provenance + license details\n\n\n\n\nStep 2: Verify Semantic Links\n\nUse GC DFO Salmon Ontology documentation to verify that term IRIs are current and resolvable.\nExample row:\nvariable_name,label,data_type,definition,standard_term_iri\nCU,Conservation Unit,string,The conservation unit identifier,https://w3id.org/gcdfo/salmon#ConservationUnit\nDo not use partial URIs.\n\n\n\nStep 3: Build Release-Ready Package\n\nPrepare the package structure expected by your destination workflow:\n\ndataset.csv\ntables.csv\ncolumn_dictionary.csv\ncodes.csv (if needed)\ndata files in data/\n\nReference: Salmon data package specification\n\n\n\nStep 4: Validate Before Submission\n\nRun validation checks before upload/publication:\n\nrequired fields complete\ndata types and date formats valid\ncode lists consistent\nall IRI links present and syntactically valid\n\n\n\n\nStep 5: Submit to SPSR for FSAR Workflows (when applicable)\n\nFor many FSAR-oriented datasets, SPSR is the operational intake path.\n\nSPSR docs: https://spsr.dfo-mpo.gc.ca/documentation/\nSPSR upload wizard: https://spsr.dfo-mpo.gc.ca/wizard/1/\nSPSR templates endpoint: https://spsr.dfo-mpo.gc.ca/download_sdp_templates\n\nUpload only after internal quality checks are complete.\n\n\n\nStep 6: Publish Externally (if approved)\n\nFor external/public release, follow policy and approval requirements:\n\nPublishing Data Externally\n\nKeep internal intake (e.g., SPSR) and external publication pathways clearly documented.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/publish-data.html#next-steps",
    "href": "reference_info/data_standards/publish-data.html#next-steps",
    "title": "Publish Your Data",
    "section": "Next Steps",
    "text": "Next Steps\n\nFSAR Data Standardization Workflow\nSalmon data package + SPSR intake path\nPublishing Data Externally",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Publish Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-data-ecosystem.html",
    "href": "reference_info/data_standards/semantic-salmon-data-ecosystem.html",
    "title": "The Semantic Salmon Data Ecosystem",
    "section": "",
    "text": "The Semantic Salmon Data Ecosystem (Semantic Salmon System) connects standards, ontology terms, package metadata, validation tooling, and operational upload pathways so salmon data can move from local analysis into reusable systems.\n\n  Canonical Architecture\n  Active Workflows\n  Contributor Guidance\n\n\n\n\n  \n    DFO Salmon Ontology ‚Äî canonical semantics\n    Salmon data package ‚Äî package metadata structure\n    metasalmon ‚Äî validation and quality control\n    Salmon Data GPT ‚Äî optional drafting/acceleration assistant\n    SPSR ‚Äî operational intake path for many FSAR workflows\n  \n  Open the branded start page for this full path.\n\nAt a high level, the ecosystem has five working parts:\n\nDFO Salmon Ontology ‚Äî canonical terms and semantics\nSalmon data package (SDP) ‚Äî structured metadata packaging\nValidation tooling (metasalmon, related checks) ‚Äî quality control\nSPSR (Salmon Population Summary Repository) ‚Äî intake and operational publishing path for many FSAR workflows\nSalmon Data GPT / guided assistants (optional) ‚Äî acceleration layer for drafting mappings and metadata",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon Data Ecosystem"
    ]
  },
  {
    "objectID": "reference_info/data_standards/semantic-salmon-data-ecosystem.html#entry-path-what-to-use-in-order",
    "href": "reference_info/data_standards/semantic-salmon-data-ecosystem.html#entry-path-what-to-use-in-order",
    "title": "The Semantic Salmon Data Ecosystem",
    "section": "",
    "text": "DFO Salmon Ontology ‚Äî canonical semantics\n    Salmon data package ‚Äî package metadata structure\n    metasalmon ‚Äî validation and quality control\n    Salmon Data GPT ‚Äî optional drafting/acceleration assistant\n    SPSR ‚Äî operational intake path for many FSAR workflows\n  \n  Open the branded start page for this full path.\n\nAt a high level, the ecosystem has five working parts:\n\nDFO Salmon Ontology ‚Äî canonical terms and semantics\nSalmon data package (SDP) ‚Äî structured metadata packaging\nValidation tooling (metasalmon, related checks) ‚Äî quality control\nSPSR (Salmon Population Summary Repository) ‚Äî intake and operational publishing path for many FSAR workflows\nSalmon Data GPT / guided assistants (optional) ‚Äî acceleration layer for drafting mappings and metadata",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "Semantic Salmon Data Ecosystem"
    ]
  },
  {
    "objectID": "reference_info/data_standards/standardize-data.html",
    "href": "reference_info/data_standards/standardize-data.html",
    "title": "Standardize Your Data",
    "section": "",
    "text": "Active Workflow\n  Canonical Terms Required\n  FSAR + SPSR Ready",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Standardize Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/standardize-data.html#semantic-salmon-system-entry-path-start-here",
    "href": "reference_info/data_standards/standardize-data.html#semantic-salmon-system-entry-path-start-here",
    "title": "Standardize Your Data",
    "section": "Semantic Salmon System ‚Äî entry path (start here)",
    "text": "Semantic Salmon System ‚Äî entry path (start here)\n\n  Use this 5-part system in order\n  \n    DFO Salmon Ontology for canonical term IRIs\n    Salmon data package for package metadata structure\n    metasalmon for validation checks\n    Salmon Data GPT as an optional drafting assistant\n    SPSR for operational FSAR intake\n  \n\nQuick links:\n\nSemantic Salmon System ‚Äî Start Here\nGC DFO Salmon Ontology documentation\nSalmon data package specification\nSalmon data package + SPSR intake path\nFSAR to SPSR: End-to-End Example",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Standardize Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/standardize-data.html#cookbook-guide",
    "href": "reference_info/data_standards/standardize-data.html#cookbook-guide",
    "title": "Standardize Your Data",
    "section": "Cookbook Guide",
    "text": "Cookbook Guide\n\n\nStep 1: Assess Your Current Data\n\nWhat you need: - Your source dataset - A list of column names and meanings\nHow to do it:\n\nList all columns and describe each in plain language.\nMark candidate fields for semantic mapping:\n\nidentifiers (CU/SMU/population)\ntime fields (brood year, return year, event date)\nmeasurements (escapement, abundance, catch)\ncontrolled codes (run type, method, status)\n\n\n\n\n\nStep 2: Map to Canonical Terms\n\nWhat you need: - Column assessment from Step 1 - GC DFO Salmon Ontology documentation\nHow to do it:\n\nSearch for matching canonical terms.\nConfirm definitions match your intended meaning.\nRecord the full canonical IRI for each mapping.\n\nExample mapping (illustrative ‚Äî confirm exact term IRIs in WIDOCO):\ncurrent_column,standard_term_label,standard_term_iri\nCU_code,Conservation Unit,https://w3id.org/gcdfo/salmon#ConservationUnit\nBY,Brood Year,https://w3id.org/gcdfo/salmon#BroodYear\nEsc,Escapement,https://w3id.org/gcdfo/salmon#Escapement\nRule: do not use partial/shortened URIs.\n\n\n\nStep 3: Build or Update Your Data Dictionary\n\nUse one row per source column.\nMinimum recommended fields:\n\nvariable_name\nlabel\ndata_type\ndefinition\nstandard_term_iri\nunit (where applicable)\naccepted_values (for categorical fields)\n\nExample:\nvariable_name,label,data_type,definition,standard_term_iri,unit\nCU_code,Conservation Unit,string,The conservation unit identifier,https://w3id.org/gcdfo/salmon#ConservationUnit,\nBY,Brood Year,integer,The year in which spawning occurred,https://w3id.org/gcdfo/salmon#BroodYear,year\nEsc,Escapement,float,Number of fish returning to spawn,https://w3id.org/gcdfo/salmon#Escapement,number of fish\n\n\n\nStep 4: Standardize Controlled Values\n\n\nIdentify categorical columns.\nList observed values.\nMap to controlled concepts and keep a mapping table.\n\nExample:\nvariable_name,current_value,standard_value,concept_iri\nrun_type,Spring,Spring Run,https://w3id.org/gcdfo/salmon#SpringRun\nrun_type,Summer,Summer Run,https://w3id.org/gcdfo/salmon#SummerRun\nrun_type,Fall,Fall Run,https://w3id.org/gcdfo/salmon#FallRun\n\n\n\nStep 5: Apply Transformations Reproducibly\n\n\napply mappings in script (R/Python), not manually in ad-hoc spreadsheets\npreserve source column traceability\nvalidate data types and units after transformation\nversion your transformed output\n\n\n\n\nStep 6: Validate and Prepare for Intake\n\nBefore moving to package/upload:\n\nevery column is documented\nsemantic links are canonical full IRIs\ncontrolled values are mapped or flagged\nunits and assumptions are explicit\ntransformation steps are recorded\n\nIf your destination is SPSR for FSAR workflows, continue with FSAR Data Standardization Workflow.",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Standardize Your Data"
    ]
  },
  {
    "objectID": "reference_info/data_standards/standardize-data.html#next-steps",
    "href": "reference_info/data_standards/standardize-data.html#next-steps",
    "title": "Standardize Your Data",
    "section": "Next Steps",
    "text": "Next Steps\n\nSalmon Data Standards Hub\nSalmon data package specification\nSalmon data package + SPSR intake path\nData Stewardship Unit contact",
    "crumbs": [
      "Home",
      "Common Workflows",
      "Standardize Your Data"
    ]
  },
  {
    "objectID": "reference_info/glossary.html",
    "href": "reference_info/glossary.html",
    "title": "Glossary & Acronyms",
    "section": "",
    "text": "A quick reference for common terms used across the FADS Open Science Hub.",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Glossary & Acronyms"
    ]
  },
  {
    "objectID": "reference_info/glossary.html#acronyms",
    "href": "reference_info/glossary.html#acronyms",
    "title": "Glossary & Acronyms",
    "section": "Acronyms",
    "text": "Acronyms\n\n\n\nAcronym\nMeaning\n\n\n\n\nCSAS\nCanadian Science Advisory Secretariat\n\n\nDFO\nFisheries and Oceans Canada\n\n\nDSU\nData Stewardship Unit\n\n\nFADS\nFisheries and Assessment Data Section\n\n\nFSAR\nFisheries Science Advisory Report\n\n\nIRI\nInternationalized Resource Identifier\n\n\nSDP\nSalmon data package\n\n\nSKOS\nSimple Knowledge Organization System\n\n\nURI\nUniform Resource Identifier\n\n\n\nLegacy usage note: some older documents may still use SDEP; current naming in this hub is salmon data package (SDP).",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Glossary & Acronyms"
    ]
  },
  {
    "objectID": "reference_info/glossary.html#common-terms",
    "href": "reference_info/glossary.html#common-terms",
    "title": "Glossary & Acronyms",
    "section": "Common terms",
    "text": "Common terms\n\nControlled vocabulary\nA curated set of approved terms and definitions used consistently across datasets.\n\n\nOntology\nA formal representation of domain concepts, relationships, and identifiers that supports machine-readable interoperability.\n\n\nData package\nA structured bundle of data files plus metadata files describing schema, meaning, and provenance.\n\n\nColumn dictionary\nA metadata table documenting each variable (column) in a dataset, including data type, role, and semantic mapping.\n\n\nConcept scheme\nA SKOS collection organizing related concepts, often used for controlled codes.",
    "crumbs": [
      "Home",
      "Tools & Training",
      "Glossary & Acronyms"
    ]
  },
  {
    "objectID": "reference_info/ontology/conventions/full.html",
    "href": "reference_info/ontology/conventions/full.html",
    "title": "Full Conventions (Canonical Source)",
    "section": "",
    "text": "This site keeps a curated, user-first conventions guide.\nFor the complete and canonical source text, use:\n\nhttps://github.com/dfo-pacific-science/dfo-salmon-ontology/blob/main/docs/CONVENTIONS.md\n\nRepository:\n\nhttps://github.com/dfo-pacific-science/dfo-salmon-ontology\n\nIf there is any mismatch between DSU guidebook pages and the canonical file, treat the canonical file as authoritative."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html",
    "href": "reference_info/ontology/conventions/owl-classes.html",
    "title": "OWL Classes",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html#required-pattern",
    "href": "reference_info/ontology/conventions/owl-classes.html#required-pattern",
    "title": "OWL Classes",
    "section": "Required pattern",
    "text": "Required pattern\nEvery class should include:\n\na owl:Class\nrdfs:label \"...\"@en\nIAO:0000115 \"...\"@en\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt;\n\nOptional but recommended:\n\nIAO:0000119 (text citation)\ndcterms:source (resolvable source URI)\nIAO:0000112 (example usage)"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html#authoring-template",
    "href": "reference_info/ontology/conventions/owl-classes.html#authoring-template",
    "title": "OWL Classes",
    "section": "Authoring template",
    "text": "Authoring template\n:ConservationUnit a owl:Class ;\n  rdfs:label \"Conservation Unit\"@en ;\n  IAO:0000115 \"A biologically meaningful salmon management unit used for conservation and assessment.\"@en ;\n  rdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; ;\n  IAO:0000119 \"DFO policy source text\"@en ;\n  dcterms:source &lt;https://example.org/source&gt; ."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html#hierarchy-guidance",
    "href": "reference_info/ontology/conventions/owl-classes.html#hierarchy-guidance",
    "title": "OWL Classes",
    "section": "Hierarchy guidance",
    "text": "Hierarchy guidance\n\nUse rdfs:subClassOf for true ‚Äúis-a‚Äù relationships.\nUse owl:disjointWith when classes are mutually exclusive.\nUse owl:equivalentClass only when meanings are truly identical."
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html#common-mistakes-to-avoid",
    "href": "reference_info/ontology/conventions/owl-classes.html#common-mistakes-to-avoid",
    "title": "OWL Classes",
    "section": "Common mistakes to avoid",
    "text": "Common mistakes to avoid\n\ndefinition repeating label without adding meaning\ncreating duplicate classes for the same concept\nusing instance examples as class definitions\nmixing SKOS concept semantics into class axioms without design intent"
  },
  {
    "objectID": "reference_info/ontology/conventions/owl-classes.html#quick-quality-checks",
    "href": "reference_info/ontology/conventions/owl-classes.html#quick-quality-checks",
    "title": "OWL Classes",
    "section": "Quick quality checks",
    "text": "Quick quality checks\n\nclear singular label\nnon-circular definition\ncorrect parent class\nprovenance included when available"
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html",
    "title": "Provenance and Maintenance",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html#minimal-provenance-pattern",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html#minimal-provenance-pattern",
    "title": "Provenance and Maintenance",
    "section": "Minimal provenance pattern",
    "text": "Minimal provenance pattern\nUse these fields consistently:\n\nIAO:0000115 ‚Äî term definition text (required)\nIAO:0000119 ‚Äî textual source/citation for definition (optional)\ndcterms:source ‚Äî resolvable URI for source material (optional but preferred)\nrdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; (required)"
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html#keep-identifiers-and-codes-distinct",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html#keep-identifiers-and-codes-distinct",
    "title": "Provenance and Maintenance",
    "section": "Keep identifiers and codes distinct",
    "text": "Keep identifiers and codes distinct\n\ndcterms:identifier = textual internal identifier (when used)\nskos:notation = scheme code (typed literal)\nlabels are human-facing and should not carry machine IDs/codes"
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html#maintenance-cadence",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html#maintenance-cadence",
    "title": "Provenance and Maintenance",
    "section": "Maintenance cadence",
    "text": "Maintenance cadence\nFor every material ontology update:\n\nupdate/add terms with required annotations\nvalidate consistency and quality checks\nregenerate docs/release artifacts\nupdate DSU guidebook pages if conventions changed\ncommunicate downstream impact (salmon data package templates, SPSR guidance, validators)"
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html#quality-checks-before-merge",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html#quality-checks-before-merge",
    "title": "Provenance and Maintenance",
    "section": "Quality checks before merge",
    "text": "Quality checks before merge\n\nrequired annotation fields present\nprovenance fields are meaningful (not placeholders)\nno duplicate concept labels with conflicting definitions\nhierarchy/scheme placement reviewed\nrelease notes prepared for user-facing changes"
  },
  {
    "objectID": "reference_info/ontology/conventions/provenance-maintenance.html#canonical-source-links",
    "href": "reference_info/ontology/conventions/provenance-maintenance.html#canonical-source-links",
    "title": "Provenance and Maintenance",
    "section": "Canonical source links",
    "text": "Canonical source links\n\nCanonical file: https://github.com/dfo-pacific-science/dfo-salmon-ontology/blob/main/docs/CONVENTIONS.md\nRepository root: https://github.com/dfo-pacific-science/dfo-salmon-ontology"
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html",
    "href": "reference_info/ontology/conventions/schema-vs-data.html",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "",
    "text": "Note\n\n\n\nDerived from canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repo."
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#why-this-split-matters",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#why-this-split-matters",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "Why this split matters",
    "text": "Why this split matters\nThe ontology file should stay focused on schema (types, properties, definitions). Actual observations and measurements belong in data files/databases."
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#tbox-schema-belongs-in-ontology",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#tbox-schema-belongs-in-ontology",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "TBox (schema) belongs in ontology",
    "text": "TBox (schema) belongs in ontology\nExamples:\n\nclass definitions (owl:Class)\nproperty definitions (owl:ObjectProperty, owl:DatatypeProperty)\nconcept schemes (skos:ConceptScheme)\nlogical axioms and constraints"
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#abox-instance-data-belongs-in-data-graphs",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#abox-instance-data-belongs-in-data-graphs",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "ABox (instance data) belongs in data graphs",
    "text": "ABox (instance data) belongs in data graphs\nExamples:\n\nspecific survey events\nspecific count values\nyear/location-specific records\ndataset-specific observations"
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#practical-rule-of-thumb",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#practical-rule-of-thumb",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "Practical rule of thumb",
    "text": "Practical rule of thumb\nIf the triple changes every season/year/report, it is probably ABox data, not ontology schema."
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#example",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#example",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "Example",
    "text": "Example\n# TBox (schema)\n:EscapementSurveyEvent a owl:Class ;\n  rdfs:label \"Escapement Survey Event\"@en ;\n  IAO:0000115 \"A survey event used to estimate escapement.\"@en ;\n  rdfs:isDefinedBy &lt;https://w3id.org/gcdfo/salmon&gt; .\n\n# ABox (instance data)\n:SkeenaSurveyEvent2024 a :EscapementSurveyEvent ;\n  dwc:eventDate \"2024-08-15\"^^xsd:date ;\n  :measuredVisits 6 ."
  },
  {
    "objectID": "reference_info/ontology/conventions/schema-vs-data.html#implication-for-dsu-salmon-data-package-spsr-workflows",
    "href": "reference_info/ontology/conventions/schema-vs-data.html#implication-for-dsu-salmon-data-package-spsr-workflows",
    "title": "Schema vs Data (TBox vs ABox)",
    "section": "Implication for DSU salmon data package / SPSR workflows",
    "text": "Implication for DSU salmon data package / SPSR workflows\n\nontology repository defines shared semantics\nsalmon data package + SPSR workflows carry the actual dataset instances\nkeep these layers separate so standards can evolve without rewriting historical data"
  },
  {
    "objectID": "reference_info/ontology/formal-documentation.html",
    "href": "reference_info/ontology/formal-documentation.html",
    "title": "GC DFO Salmon Ontology",
    "section": "",
    "text": "The complete formal ontology documentation is published at:\nüëâ https://w3id.org/gcdfo/salmon\nThis includes:\n\nClass hierarchies and logical axioms\nObject/data property definitions\nImports and alignment structure\nRelease ontology artifacts\n\nOntology source (TTL):\n\ndfo-salmon-ontology/docs/gcdfo.ttl",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "GC DFO Salmon Ontology"
    ]
  },
  {
    "objectID": "reference_info/ontology/formal-documentation.html#formal-ontology-documentation",
    "href": "reference_info/ontology/formal-documentation.html#formal-ontology-documentation",
    "title": "GC DFO Salmon Ontology",
    "section": "",
    "text": "The complete formal ontology documentation is published at:\nüëâ https://w3id.org/gcdfo/salmon\nThis includes:\n\nClass hierarchies and logical axioms\nObject/data property definitions\nImports and alignment structure\nRelease ontology artifacts\n\nOntology source (TTL):\n\ndfo-salmon-ontology/docs/gcdfo.ttl",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "GC DFO Salmon Ontology"
    ]
  },
  {
    "objectID": "reference_info/ontology/formal-documentation.html#contributor-guidebook-user-first",
    "href": "reference_info/ontology/formal-documentation.html#contributor-guidebook-user-first",
    "title": "GC DFO Salmon Ontology",
    "section": "Contributor guidebook (user-first)",
    "text": "Contributor guidebook (user-first)\nThe user-first conventions guidebook is now integrated into this site.\nIt is based on the canonical docs/CONVENTIONS.md in the dfo-salmon-ontology repository.\n\nConventions guidebook index\nQuick reference\nSchema vs data (TBox vs ABox)\nOWL classes\nOWL properties\nSKOS vocabularies\nProvenance and maintenance\nClass vs SKOS Decision Tree\nHow to Request a New Ontology Term\nFull conventions (canonical source)",
    "crumbs": [
      "Home",
      "Standards & Vocabulary",
      "GC DFO Salmon Ontology"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html",
    "href": "reference_info/openDataPractices.html",
    "title": "Data Practices for Open Science ‚Äì 3 Quick Tips",
    "section": "",
    "text": "Sir Isaac Newton said, ‚ÄúIf I have seen further, it is by standing on the shoulders of giants.‚Äù As scientists, we aim to share our findings with colleagues who may use the information to continue to make progress on important questions. Making your data more transparent and reproducible not only contributes to a more useful body of work but provides ‚Äúshoulders‚Äù for those who come after you (other colleagues, students, and collaborators). It‚Äôs also handy for improving your own workflow. Your personal peace of mind is another added bonus to front loading the work of organizing and describing your dataset.\nYou might be wondering what are we referring to when we talk about ‚ÄúOpen Science‚Äù. A more open approach to research means making results accessible for the benefits of both scientists and broader society (UNESCO 2023). The Open Science movement aims to increase transparency and the speed of knowledge transfer. Barriers to Open Science include things like: paywalled journals, favouring knowledge produced by high-income countries, hidden or unknown source code or workflow, and supporting data that is unavailable or unusable. In this article, we‚Äôll tackle the last barrier and talk about practical ways to make your data more Open.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#use-a-tidy-format",
    "href": "reference_info/openDataPractices.html#use-a-tidy-format",
    "title": "Data Practices for Open Science ‚Äì 3 Quick Tips",
    "section": "1. Use a tidy format",
    "text": "1. Use a tidy format\n Illustration from Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst. Adapted from Wickham (2014).\nFor an excellent introduction to Tidy Data, check out Hadley Wickham‚Äôs 2014 paper in the Journal of Statistical Software (Wickham 2014). The tidy approach to data dictates that when organizing data, every observation should be represented by a row and every variable a column. This is a great approach for standardization and machine readability.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#redundancy-in-data-is-ok-sometimes",
    "href": "reference_info/openDataPractices.html#redundancy-in-data-is-ok-sometimes",
    "title": "Data Practices for Open Science ‚Äì 3 Quick Tips",
    "section": "2. Redundancy in data is OK sometimes!",
    "text": "2. Redundancy in data is OK sometimes!\nThroughout our scientific careers, we‚Äôre often told to be concise and simplify so this advice may surprise you. Strategic redundancy in data allows you to make linkages between different pieces of information. It also saves space. One way to invoke the hidden power of redundancy is using metadata and data dictionaries to link important context to your primary dataset. For example, this table contains two types of observations: song information and Billboard rankings with one entry (i.e., row) each week the song remains on the Billboard Hot 100. Look what happens when we split the information into two tables: one with the song titles, artists‚Äô names and run times; and the other with details on their Billboard rankings. This:\n\nAvoids confusion at scale ‚Äì note how there are two types of temporal data in the first table. One could conflate time and date.\nSaves space. Say there were 100 songs with an average of 7 entries each. Rather than a table with 700 entries and 7 variables (4900 pieces of data), you now have 2 tables (containing 4500 data points total). This difference in memory usage scales with the size of your dataset.\n\n Table 1. Too much information at different scales crammed into a single table. Table adapted from Wickham (2014).\n Table 2: Example of using redundancy by splitting tables in two. Note how one song (left) has multiple Billboard rankings (right). Tables adapted from Wickham (2014).",
    "crumbs": [
      "Home",
      "Getting Started",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#keep-raw-data-raw",
    "href": "reference_info/openDataPractices.html#keep-raw-data-raw",
    "title": "Data Practices for Open Science ‚Äì 3 Quick Tips",
    "section": "3. Keep raw data raw",
    "text": "3. Keep raw data raw\nSeparating your raw data from analyses is essential for reproducibility. After all, how can we reproduce an analysis if we have no access to the original dataset? In repositories, create a folder named RAW. Place unmanipulated data here. Do not open these files except to add or remove raw data. You may consider setting them to ‚Äúread-only‚Äù when viewing in Excel. Keep all analyses in your scripts and outside of Excel, which will read the data and create outputs from it. Use a descriptive file name for any outputs generated from raw data and place them in a separate folder.\nEmbracing Open Science practices doesn‚Äôt have to be overwhelming. By adopting tidy data formats, allowing for strategic redundancy, and keeping raw data untouched, you can contribute to a more transparent and collaborative scientific community and set yourself up for smoother workflows and more reproducible research. These small, intentional steps can have a big impact, helping others build on your work and accelerating the pace of discovery.",
    "crumbs": [
      "Home",
      "Getting Started",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html",
    "href": "reference_info/tools/r_packages.html",
    "title": "R Packages",
    "section": "",
    "text": "Welcome to the R Packages section of the FADS Open Science Hub! üêü\nHere you‚Äôll find a curated set of R packages that support your work as a data steward or data producer within the Pacific Region Science Branch of Fisheries and Oceans Canada. üß™üåä\n\n\ntidyr, an offering from the tidyverse is the speedy Swiffer mop to your messy data. It provides tools for following the ethos of tidy data, which holds the following tenets:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFor example, make your wide formatted data more tidy with pivot_longer() or deal with missing values with drop_na().\n\n\n\n\n\n\nTipWant to load all the tidyverse packages at once?\n\n\n\nTry library(tidyverse) to load all of those amazing tools in one line\n\n\n\n\n\nggplot2 is a powerful tool for data visualization offered under the tidyverse umbrella.\nAs any scientist knows, visualizing your data is just as important as generating it. After all, how can you communicate your results if they‚Äôre hidden away in a table?\nThe R graph gallery has countless of great examples of plots created with ggplot2. Check them out!\n\n\n\nAnother tidyverse offering, dplyr is an essential package for data manipulation. Try out generating summary stats in a breeze with group_by() |&gt; summarise() or join two datasets with a left_join().\n\n\n\narrow is an amazing tool for working with larger than memory data. R usually performs computations in RAM (your computer‚Äôs short term memory), but this can pose a problem for larger datasets. Arrow moves computations onto disk (your computer‚Äôs long term memory) to avoid this.\nFor a tutorial, see here",
    "crumbs": [
      "Home",
      "Tools & Training",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#tidyr",
    "href": "reference_info/tools/r_packages.html#tidyr",
    "title": "R Packages",
    "section": "",
    "text": "tidyr, an offering from the tidyverse is the speedy Swiffer mop to your messy data. It provides tools for following the ethos of tidy data, which holds the following tenets:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFor example, make your wide formatted data more tidy with pivot_longer() or deal with missing values with drop_na().\n\n\n\n\n\n\nTipWant to load all the tidyverse packages at once?\n\n\n\nTry library(tidyverse) to load all of those amazing tools in one line",
    "crumbs": [
      "Home",
      "Tools & Training",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#ggplot2",
    "href": "reference_info/tools/r_packages.html#ggplot2",
    "title": "R Packages",
    "section": "",
    "text": "ggplot2 is a powerful tool for data visualization offered under the tidyverse umbrella.\nAs any scientist knows, visualizing your data is just as important as generating it. After all, how can you communicate your results if they‚Äôre hidden away in a table?\nThe R graph gallery has countless of great examples of plots created with ggplot2. Check them out!",
    "crumbs": [
      "Home",
      "Tools & Training",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#dplyr",
    "href": "reference_info/tools/r_packages.html#dplyr",
    "title": "R Packages",
    "section": "",
    "text": "Another tidyverse offering, dplyr is an essential package for data manipulation. Try out generating summary stats in a breeze with group_by() |&gt; summarise() or join two datasets with a left_join().",
    "crumbs": [
      "Home",
      "Tools & Training",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#arrow",
    "href": "reference_info/tools/r_packages.html#arrow",
    "title": "R Packages",
    "section": "",
    "text": "arrow is an amazing tool for working with larger than memory data. R usually performs computations in RAM (your computer‚Äôs short term memory), but this can pose a problem for larger datasets. Arrow moves computations onto disk (your computer‚Äôs long term memory) to avoid this.\nFor a tutorial, see here",
    "crumbs": [
      "Home",
      "Tools & Training",
      "R Packages"
    ]
  }
]