[
  {
    "objectID": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html",
    "href": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html",
    "title": "Fisheries Science Reports – Data and Stats 101",
    "section": "",
    "text": "The 2025 Salmon Spring Week meetings reviewing the biological status of three stocks underscored the complex convergence of three frameworks: Canada’s Fish Stock Provisions (FSP), the Wild Salmon Policy (WSP), and the Precautionary Approach (PA).\n\n\nUnder the Fisheries Act, the Fish Stock Provisions mandate the identification of Limit Reference Points (LRPs) and the assignment of status (healthy, cautious, critical) for major stocks. This legal obligation ties directly into harvest control rules and rebuilding plan requirements when stocks fall below LRPs.\n\n\n\nThe WSP focuses on maintaining biological diversity and genetic distinctiveness by assessing stocks at the level of Conservation Units (CUs). It relies on the identification of biological benchmarks, including lower and upper benchmarks derived from stock-recruitment or percentile approaches, to classify CU status into red, amber, or green zones.\n\n\n\nThe PA emphasizes erring on the side of caution in the face of uncertainty. In assessments, this is operationalized via conservative assumptions (e.g., using lower bounds of estimates, assigning “data deficient” status) and requires clearly articulated risk statements and uncertainty quantification.\n\n\n\nKey terms—Limit Reference Point (LRP) under FSP and Lower Benchmark under WSP—are conceptually aligned but contextually distinct (Table 1). Likewise, USR (Upper Stock Reference) in PA guidance shares conceptual space with the WSP Upper Benchmark. The challenge is reconciling population-specific CUs with stock management units (SMUs) required by FSP, especially when CUs differ in status or data availability. Several inconsistencies, such as those seen in Stikine, highlight the need for an integrated data architecture and harmonized terminology.\nTable 1. Relation of Terms Across Frameworks\n\n\n\n\n\n\n\n\n\n\nWSP Term\nPA/Fisheries Act Equivalent\nPurpose\nStatistical Approach Used\nEg. Quantitative Benchmarks / Model\n\n\n\n\nLower Benchmark\nLimit Reference Point (LRP)\nAvoid serious harm\nSR-based (Ricker), Risk-based, Percentile\n\\(S_{GEN}\\), 40% \\(S_{MSY}\\), 25th percentile\n\n\nUpper Benchmark\nUpper Stock Reference (USR)\nTrigger caution, reduce removals\nSR-based (Ricker), Percentile\n80% \\(S_{MSY}\\), 50th/75th percentile\n\n\nGreen Zone\nHealthy Zone\nFull exploitation allowed\nComposite scoring, quantitative SR input\nStatus \\(\\geq\\) USR or Upper Benchmark\n\n\nAmber Zone\nCautious Zone\nManagement action to avoid critical\nComposite integration, trend analysis\nBetween LRP and USR\n\n\nRed Zone\nCritical Zone\nMinimize removals, enable rebuilding\nEscapement + trend + SR benchmarks\nStatus &lt; LRP or Lower Benchmark\n\n\nStatus Assessment\nStock Assessment\nDetermine location relative to LRP/USR\nIntegrated Indicators + SR + Escapement\nCombined benchmark comparison",
    "crumbs": [
      "FADS Internal Wiki",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html#high-level-overview-stock-assessment-and-policy-alignment",
    "href": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html#high-level-overview-stock-assessment-and-policy-alignment",
    "title": "Fisheries Science Reports – Data and Stats 101",
    "section": "",
    "text": "The 2025 Salmon Spring Week meetings reviewing the biological status of three stocks underscored the complex convergence of three frameworks: Canada’s Fish Stock Provisions (FSP), the Wild Salmon Policy (WSP), and the Precautionary Approach (PA).\n\n\nUnder the Fisheries Act, the Fish Stock Provisions mandate the identification of Limit Reference Points (LRPs) and the assignment of status (healthy, cautious, critical) for major stocks. This legal obligation ties directly into harvest control rules and rebuilding plan requirements when stocks fall below LRPs.\n\n\n\nThe WSP focuses on maintaining biological diversity and genetic distinctiveness by assessing stocks at the level of Conservation Units (CUs). It relies on the identification of biological benchmarks, including lower and upper benchmarks derived from stock-recruitment or percentile approaches, to classify CU status into red, amber, or green zones.\n\n\n\nThe PA emphasizes erring on the side of caution in the face of uncertainty. In assessments, this is operationalized via conservative assumptions (e.g., using lower bounds of estimates, assigning “data deficient” status) and requires clearly articulated risk statements and uncertainty quantification.\n\n\n\nKey terms—Limit Reference Point (LRP) under FSP and Lower Benchmark under WSP—are conceptually aligned but contextually distinct (Table 1). Likewise, USR (Upper Stock Reference) in PA guidance shares conceptual space with the WSP Upper Benchmark. The challenge is reconciling population-specific CUs with stock management units (SMUs) required by FSP, especially when CUs differ in status or data availability. Several inconsistencies, such as those seen in Stikine, highlight the need for an integrated data architecture and harmonized terminology.\nTable 1. Relation of Terms Across Frameworks\n\n\n\n\n\n\n\n\n\n\nWSP Term\nPA/Fisheries Act Equivalent\nPurpose\nStatistical Approach Used\nEg. Quantitative Benchmarks / Model\n\n\n\n\nLower Benchmark\nLimit Reference Point (LRP)\nAvoid serious harm\nSR-based (Ricker), Risk-based, Percentile\n\\(S_{GEN}\\), 40% \\(S_{MSY}\\), 25th percentile\n\n\nUpper Benchmark\nUpper Stock Reference (USR)\nTrigger caution, reduce removals\nSR-based (Ricker), Percentile\n80% \\(S_{MSY}\\), 50th/75th percentile\n\n\nGreen Zone\nHealthy Zone\nFull exploitation allowed\nComposite scoring, quantitative SR input\nStatus \\(\\geq\\) USR or Upper Benchmark\n\n\nAmber Zone\nCautious Zone\nManagement action to avoid critical\nComposite integration, trend analysis\nBetween LRP and USR\n\n\nRed Zone\nCritical Zone\nMinimize removals, enable rebuilding\nEscapement + trend + SR benchmarks\nStatus &lt; LRP or Lower Benchmark\n\n\nStatus Assessment\nStock Assessment\nDetermine location relative to LRP/USR\nIntegrated Indicators + SR + Escapement\nCombined benchmark comparison",
    "crumbs": [
      "FADS Internal Wiki",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html#deep-dive-into-reference-points-and-benchmarks-in-stock-assessment",
    "href": "deep_dives/Stock Assessment Data 101 for Fisheries Science Reports.html#deep-dive-into-reference-points-and-benchmarks-in-stock-assessment",
    "title": "Fisheries Science Reports – Data and Stats 101",
    "section": "2. Deep Dive into Reference Points and Benchmarks in Stock Assessment",
    "text": "2. Deep Dive into Reference Points and Benchmarks in Stock Assessment\n\nBenchmarks vs Reference Points\n\nWSP Benchmarks (UBM, LBM): Derived from productivity models or empirical data. They are biological in nature and specific to a Conservation Unit (CU).\n\nUBM often corresponds to spawner abundance giving Maximum Sustainable Yield (MSY) or similar.\nLBM often relates to serious harm thresholds, such as a 50% probability of recovery under management actionHolt et al. 2009 Indica….\n\nPrecautionary Approach Reference Points (USR, LRP): Defined at the Stock Management Unit (SMU) level, which aggregates multiple CUs. The LRP is legally required under the amended Fisheries Act.\n\nThe SMU-level LRP is often derived using the probability that all component CUs are above their respective LBM—commonly at ≥50% probability (Holt et al. 2023)….\nHow a Lower Benchmark Relates to the LRP\nThe CU-level LBM becomes the statistical input for deriving the SMU-level LRP. Specifically:\n\nLRPs can be derived via logistic regression models estimating the probability that all CUs within an SMU exceed their LBMs given a level of aggregate abundance (Holt et al. 2023)\nThis integrates the WSP framework into the Fisheries Act mandate for setting LRPs.\n\nIn essence, LRP ≈ aggregate abundance where there’s ≥50% chance all CUs are above their LBM (Holt et al. 2023)\n\n\nSpawner-Recruitment (SR) Based Approaches\nThe gold standard for benchmark estimation when sufficient data are available.\n\na. Ricker Model\nThe most commonly used model for Pacific salmon: - Equation:\n\\[\nR = S \\cdot e^{(a - bS + \\varepsilon)}\n\\]\n\nWhere…\n\n\\(R\\): Number of recruits (offspring that survive to a specific life stage)\n\\(S\\): Number of spawners (parents that produced the recruits)\n\\(a\\): Log-scale productivity parameter; reflects the average number of recruits per spawner at low population density\n\\(b\\): Density-dependence parameter; determines how rapidly recruitment decreases as spawner abundance increases\n\\(\\varepsilon\\): Process error term accounting for environmental or stochastic variation not explained by the model\n\n\n\nb. Benchmarks Derived From SR Models\n\nLower Benchmark (LRP):\n\nOften set at ( S_{GEN} ): spawner abundance that leads to the LRP (e.g., ( S_{MSY} )) in one generation.\nOr, alternatively, a % of ( S_{MSY} ) (e.g., 40%).\n\nUpper Benchmark (USR):\n\nOften set at 80% of ( S_{MSY} ) or similar.\n\nTarget (TRP):\n\nUsually near ( S_{MSY} ), or the escapement yielding maximum sustainable yield.\n\n\n\n\n\nAlternative Methods\n\nc. Percentile-Based Approaches\nUsed when data is limited: - Lower Benchmark: 25th percentile historical abundance. - Upper Benchmark: 50th or 75th percentile historical abundance.\n\n\nd. Risk-Based Approaches\n\nUse Monte Carlo or stochastic simulations to assess risk thresholds by repeatedly sampling from uncertainty distributions.\nSimulations quantify the likelihood of stocks falling below defined critical thresholds (LRP).\nBenchmarks are selected to ensure a low probability (e.g., ≤5%) of breaching critical thresholds under various plausible scenarios.\n\n\n\ne. Closed-Loop Simulations (Management Strategy Evaluation - MSE)\n\nUtilize simulation frameworks to test effectiveness and robustness of different management strategies and Harvest Control Rules (HCRs).\nIncorporate uncertainties explicitly:\n\nObservation errors: uncertainties in monitoring and data collection.\nImplementation variability: differences between planned and actual management actions.\nEnvironmental stochasticity: variability due to environmental factors and climate change.\n\nOutcomes evaluated include biological sustainability, economic viability, and compliance with conservation objectives.",
    "crumbs": [
      "FADS Internal Wiki",
      "Deep Dives",
      "Fisheries Science Reports -- Data and Stats 101"
    ]
  },
  {
    "objectID": "documentation_hub/how_to_guides/support_controlled_vocabs.html",
    "href": "documentation_hub/how_to_guides/support_controlled_vocabs.html",
    "title": "Support Controlled Vocabularies",
    "section": "",
    "text": "The scientist downloads and completes the data dictionary template (Excel)\n\nThey describe each variable: name, definition, method, units, etc.\nThey may include URIs from known vocabularies if possible\n\nThe spreadsheet includes:\n\nA README sheet with instructions\nA Metadata tab for project-level context\nA Data Dictionary tab with one row per variable\n\n\n\n\n\n\nThe data dictionary is passed into your R validation function (validate_data_dictionary())\nIt compares submitted variables to your reference vocabulary, hosted as JSON on GitHub Pages\nThe validator returns:\n\nMatched terms\nUnmatched terms\nSuggested mappings using fuzzy logic (e.g., stringdist or Jaro-Winkler)\n\nThe scientist or steward reviews and adjusts terms as needed\n\n\n\n\n\nFor each submitted or candidate term:\n\nUse an automated lookup tool to check if a concept already exists in the NCEAS Salmon Ontology. Start by using the BioPortal Annotator or API to match term labels and definitions. Optionally, build a lightweight script that compares local vocabulary terms to the ontology by label, synonyms, or definition text using exact or fuzzy string matching. Consider integrating the result into your pipeline for batch validation.\nIf it exists:\n\nUse skos:exactMatch or skos:closeMatch in your .ttl (Turtle) or .jsonld RDF vocab files. These mappings are typically stored in the ontology files generated from your .csv source. Add a column like exact_match_uri or close_match_uri to your .csv, your export script can incorporate those values automatically during serialization to .ttl or .jsonld.\n\nIf it doesn’t exist, but is broadly applicable:\n\nPrepare a Pull Request or issue to suggest the term to the NCEAS maintainers\nInclude:\n\nLabel, definition, synonyms\nUsage example or data sources\n\n\n\n\n\n\n\n\nMaintain DFO vocabularies in GitHub as:\n\n.csv for editing\n.json for R/python access and validation\n.jsonld and .ttl for ontology use\n\nAssign persistent URIs using https://w3id.org/dfo/spsi#term-id\nOrganize vocabularies by data domain or project (e.g., monitoring, habitat, biology)\n\n\n\n\n\nExtend your vocabularies with relationships:\n\nbroader, narrower, related\nhasUnit, hasMethod, derivedFrom\n\nUse tools like rdflib or skosify to model in Turtle or OWL\nAdd namespace info:\n\n@prefix dfo-spsi: &lt;https://w3id.org/dfo/spsi#&gt;\n\nOver time, link this to:\n\nNCEAS ontology\nNERC units\nENVO/OBI for environment/sample methods\n\n\n\n\n\n\nHost vocab on GitHub Pages + register URIs at w3id.org\nLink from your Quarto site (/data-standards/vocab-index.html)\nAllow download in multiple formats (JSON, JSON-LD, TTL)\nUse vocab in:\n\nData validation tools\nMetadata editors\nAPIs and dashboards\nAI pipelines (semantic RAG, search, integration)\n\n\n\nThis workflow will enable DFO Pacific to build a structured, version-controlled, and interoperable vocabulary and ontology infrastructure for salmon data, with clear ties to domain-wide standards and extensibility for AI use cases."
  },
  {
    "objectID": "documentation_hub/how_to_guides/support_controlled_vocabs.html#guide-end-to-end-process-for-supporting-controlled-vocabularies-in-dfo",
    "href": "documentation_hub/how_to_guides/support_controlled_vocabs.html#guide-end-to-end-process-for-supporting-controlled-vocabularies-in-dfo",
    "title": "Support Controlled Vocabularies",
    "section": "",
    "text": "The scientist downloads and completes the data dictionary template (Excel)\n\nThey describe each variable: name, definition, method, units, etc.\nThey may include URIs from known vocabularies if possible\n\nThe spreadsheet includes:\n\nA README sheet with instructions\nA Metadata tab for project-level context\nA Data Dictionary tab with one row per variable\n\n\n\n\n\n\nThe data dictionary is passed into your R validation function (validate_data_dictionary())\nIt compares submitted variables to your reference vocabulary, hosted as JSON on GitHub Pages\nThe validator returns:\n\nMatched terms\nUnmatched terms\nSuggested mappings using fuzzy logic (e.g., stringdist or Jaro-Winkler)\n\nThe scientist or steward reviews and adjusts terms as needed\n\n\n\n\n\nFor each submitted or candidate term:\n\nUse an automated lookup tool to check if a concept already exists in the NCEAS Salmon Ontology. Start by using the BioPortal Annotator or API to match term labels and definitions. Optionally, build a lightweight script that compares local vocabulary terms to the ontology by label, synonyms, or definition text using exact or fuzzy string matching. Consider integrating the result into your pipeline for batch validation.\nIf it exists:\n\nUse skos:exactMatch or skos:closeMatch in your .ttl (Turtle) or .jsonld RDF vocab files. These mappings are typically stored in the ontology files generated from your .csv source. Add a column like exact_match_uri or close_match_uri to your .csv, your export script can incorporate those values automatically during serialization to .ttl or .jsonld.\n\nIf it doesn’t exist, but is broadly applicable:\n\nPrepare a Pull Request or issue to suggest the term to the NCEAS maintainers\nInclude:\n\nLabel, definition, synonyms\nUsage example or data sources\n\n\n\n\n\n\n\n\nMaintain DFO vocabularies in GitHub as:\n\n.csv for editing\n.json for R/python access and validation\n.jsonld and .ttl for ontology use\n\nAssign persistent URIs using https://w3id.org/dfo/spsi#term-id\nOrganize vocabularies by data domain or project (e.g., monitoring, habitat, biology)\n\n\n\n\n\nExtend your vocabularies with relationships:\n\nbroader, narrower, related\nhasUnit, hasMethod, derivedFrom\n\nUse tools like rdflib or skosify to model in Turtle or OWL\nAdd namespace info:\n\n@prefix dfo-spsi: &lt;https://w3id.org/dfo/spsi#&gt;\n\nOver time, link this to:\n\nNCEAS ontology\nNERC units\nENVO/OBI for environment/sample methods\n\n\n\n\n\n\nHost vocab on GitHub Pages + register URIs at w3id.org\nLink from your Quarto site (/data-standards/vocab-index.html)\nAllow download in multiple formats (JSON, JSON-LD, TTL)\nUse vocab in:\n\nData validation tools\nMetadata editors\nAPIs and dashboards\nAI pipelines (semantic RAG, search, integration)\n\n\n\nThis workflow will enable DFO Pacific to build a structured, version-controlled, and interoperable vocabulary and ontology infrastructure for salmon data, with clear ties to domain-wide standards and extensibility for AI use cases."
  },
  {
    "objectID": "documentation_hub/how_to_guides/support_controlled_vocabs.html#why-bother-with-controlled-vocabularies-and-ontologies",
    "href": "documentation_hub/how_to_guides/support_controlled_vocabs.html#why-bother-with-controlled-vocabularies-and-ontologies",
    "title": "Support Controlled Vocabularies",
    "section": "🤔 Why Bother with Controlled Vocabularies and Ontologies?",
    "text": "🤔 Why Bother with Controlled Vocabularies and Ontologies?\nIt’s a fair question. Why invest all this time defining terms, aligning with ontologies, and assigning URIs?\nBecause it unlocks a future where your data works harder for you. Controlled vocabularies and ontologies aren’t just academic exercises — they’re the foundation for better discovery, smarter integration, and automation across the science lifecycle.\nHere are some tangible ways this work adds value:\n\n\n🔍 1. Enhanced Data Discovery\n\n“I know someone collected salmon smolt data… but where do I find it?”\n\nBy tagging datasets, columns, and metadata with terms from your SPSI vocabulary, you enable: - Keyword search that actually understands synonyms and related terms - Filters based on data type, units, methods, or ecological domain - Smart discovery interfaces (e.g., “Show me everything related to juvenile survival”)\n➡️ Example: A data catalog that lets users search by controlled term rather than inconsistent column names across spreadsheets.\n\n\n\n🔄 2. Semi-Automated Data Integration\n\n“These datasets report the same metric but use different terms, formats, or units.”\n\nUsing controlled terms with defined relationships (e.g., broader, exactMatch, hasUnit), you can: - Detect overlapping fields across submissions automatically - Align columns and units across multiple datasets - Standardize value domains (e.g., Red/Green/Amber vs Critical/Stable/Concerned)\n➡️ Example: A script that reads in new FSAR data and automatically maps it to the SPSR schema for validation and loading into a database.\n\n\n\n🧠 3. Smarter Applications (AI & Beyond)\n\n“Can’t AI just figure this stuff out?”\n\nOnly if you feed it structure.\nControlled vocabularies and ontologies allow you to: - Ground large language models (LLMs) in your domain’s specific terminology - Build Retrieval-Augmented Generation (RAG) systems for question answering - Use semantic search tools to find relevant variables, concepts, and datasets - Train AI to assist in metadata generation, anomaly detection, or dataset classification\n➡️ Example: A chatbot that helps scientists describe their data using your vocabulary, or recommends matching fields from existing standards.\n\n\n\n🔗 4. Future-Proof Interoperability\n\n“What if we want to share this with other agencies or join a broader platform?”\n\nStandardized vocabularies with persistent URIs and ontology alignments (e.g., with NCEAS, ENVO, OBO) make it easy to: - Share DFO terms with external partners - Convert metadata to international schemas (e.g., Darwin Core, ISO 19115) - Plug into federated data platforms without starting over\n➡️ Example: Publishing your vocabulary to w3id.org lets others reference and reuse your terms as global identifiers.\n\n\n\n🧰 5. Better Metadata, Better Stewardship\n\n“I just want people to fill out metadata that makes sense.”\n\nControlled vocabularies: - Reduce ambiguity - Improve machine readability - Make metadata easier to validate and automate\n➡️ Example: A dropdown menu in a data intake form linked to your vocab that auto-fills units, definitions, and example values — while storing clean, machine-readable metadata under the hood."
  },
  {
    "objectID": "documentation_hub/how_to_guides/support_controlled_vocabs.html#tldr",
    "href": "documentation_hub/how_to_guides/support_controlled_vocabs.html#tldr",
    "title": "Support Controlled Vocabularies",
    "section": "🧭 TL;DR",
    "text": "🧭 TL;DR\nControlled vocabularies and ontologies are not the end — they’re the beginning.\nThey allow you to: - Find data more easily - Integrate it more reliably - Build tools more effectively - And use AI more meaningfully\nThey turn your data into infrastructure.\nAnd they help ensure that knowledge created today can be used, re-used, and trusted — long into the future."
  },
  {
    "objectID": "documentation_hub/how_to_guides/sharepoint_r.html",
    "href": "documentation_hub/how_to_guides/sharepoint_r.html",
    "title": "FADS Open Science Documentation Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/sharepoint_r.html"
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html",
    "href": "how_to_guides/databricks_r_studio.html",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "",
    "text": "DFO’s shift to the Azure Lakehouse model integrates Azure Databricks, Data Lake Storage, Delta Lake, and Synapse Analytics, replacing siloed data storage and scattered code repositories. This transition centralizes data, standardizes workflows, and improves access to shared datasets and intermediate data products.\nWith Personal Access Tokens (PATs), analysts can now connect directly to the Lakehouse from RStudio (or other IDEs like VSCode or PyCharm), working seamlessly with cloud-stored data in their preferred environment. This guide explains how to set up that connection using RStudio for analysis, GitHub for code management, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#tldr",
    "href": "how_to_guides/databricks_r_studio.html#tldr",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "",
    "text": "DFO’s shift to the Azure Lakehouse model integrates Azure Databricks, Data Lake Storage, Delta Lake, and Synapse Analytics, replacing siloed data storage and scattered code repositories. This transition centralizes data, standardizes workflows, and improves access to shared datasets and intermediate data products.\nWith Personal Access Tokens (PATs), analysts can now connect directly to the Lakehouse from RStudio (or other IDEs like VSCode or PyCharm), working seamlessly with cloud-stored data in their preferred environment. This guide explains how to set up that connection using RStudio for analysis, GitHub for code management, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#azure-data-bricks-and-the-azure-lakehouse",
    "href": "how_to_guides/databricks_r_studio.html#azure-data-bricks-and-the-azure-lakehouse",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Azure Data Bricks and the Azure Lakehouse",
    "text": "Azure Data Bricks and the Azure Lakehouse\nAzure Databricks is a cloud-based analytics platform for big data processing, machine learning, and collaborative analysis. Built on Apache Spark, it integrates with Azure Data Lake Storage (ADLS), Delta Lake, and Synapse Analytics, the core components of the Azure Lakehouse.\nIn this model, Databricks acts as the compute engine, running code while Data Lake Storage and Delta Lake handle scalable, secure data storage. Synapse Analytics adds SQL-based analytics and data warehousing.\nFor DFO, adopting the Lakehouse model eliminates siloed data storage and fragmented code management. Previously, datasets were scattered across systems, and essential data cleaning and analysis code was split across multiple GitHub repositories, making it hard to track, reuse, or standardize. Valuable intermediate data products were often inaccessible. The Lakehouse consolidates data, transformations, and curated datasets, improving collaboration, reproducibility, and transparency.\nNow, analysts can connect directly to the Lakehouse from RStudio (or any IDE) using PATs, gaining seamless, secure access to centralized data with scalable compute. This shift removes past limitations on data access, permissions, and technology use, significantly enhancing DFO’s analytical capabilities.\nThis guide walks through the setup process, enabling you to connect to Azure Databricks and analyze Lakehouse-stored data using RStudio for code execution, GitHub for version control, and the Lakehouse for data storage and sharing.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#quick-start-guide-to-accessing-azure-lakehouse-from-r-studio",
    "href": "how_to_guides/databricks_r_studio.html#quick-start-guide-to-accessing-azure-lakehouse-from-r-studio",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Quick Start Guide to Accessing Azure Lakehouse from R Studio",
    "text": "Quick Start Guide to Accessing Azure Lakehouse from R Studio\n\nAccess Token\nCreate your user access token in Databricks WebUI.\nIMPORTANT: Treat this token as your personal password and do not share with anybody\n\nRequest an account on the DFO Azure Databricks workspace (EDH Subscription) by submitting an email to DFO.PACStrategicData-DonneesStrategiquesPAC.MPO@dfo-mpo.gc.ca Copy, Paste this template request and replace the [placeholders] with your information:\n\n\nHi Team,\n\nI'm following the guide at https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html and I'd like to be added to the DFO Azure Databricks workspace (EDH Subscription) to access the DFO Pacific Lakehouse for my work which is [briefly describe your work here so that the administrators know which types of permissions to give you: Data Analyst, Data Scientist, Data Engineer (described in section 1.1 of https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html)]. \n\n\nIn Databricks Web UI &gt; go to your profile page and navigate to\nUser &gt; Developer\nAccess tokens &gt; Manage\nGenerate new token as required. Ensure to set the expiration of the token to a suitable date (1 year from now)\nCopy and save this token somewhere safe such as an encrypted password manager\n\n\n\n\nDatabricks Token\n\n\n\n\nJDBC Driver\n\nDownload JDBC (java database connectivity) driver from https://www.databricks.com/spark/jdbc-drivers-download. Save and extract the .jar file to an accessible location on your computer (C:Users\\Your username is fine).\nSet up your Databricks JDBC connection URL. You can copy this directly from Databricks this way\nIn the Databricks Web UI, navigate to SQL Warehouses\nClick on the SQL Warehouse compute resource that you started  \nGo to ‘Connection details’ &gt; JDBC URL and copy the string\n\n\n\n\nJDBC Driver\n\n\n\n\nEstablishing a Connection to Databricks from R Studio (or any other IDE)\n\nEnsure you are on the DFO VPN\n\n\nlibrary(RJDBC)\n\njdbc_driver &lt;- JDBC(\"com.databricks.client.jdbc.Driver\", \"C:/Users/JOHNSONBRE/DatabricksJDBC42.jar\", \"\")\n\nurlsql &lt;- \"jdbc:databricks://adb....;\"\n\n# Run the code below here once to store your personal access token in your R Environment. \n# IMPORTANT! Never hard code your PAT in directly in your code script since your script will likely be shared, exposing your secret personal access token \n\nfile.edit(\"~/.Renviron\") # add DATABRICKS_PAT=\"your_personal_access_token\" to your .Renviron file\n\npat &lt;- Sys.getenv(\"DATABRICKS_PAT\")\n\nconnsql &lt;- RJDBC::dbConnect(jdbc_driver, urlsql, \"token\", pat)\n\nTest your connection to Databricks by running the following code:\n\ndbGetQuery(connsql, \"SELECT 1\")\n\nIf you see a table with a single row and a single column with the value 1, then your connection is successful\n\n\nExploring the Lakehouse\nNext, we will query the available catalogs of databases, their schemas, and their tables in the Lakehouse.\n\nCatalogs\nExplore the available catalogs in the Databricks environment. Catalogs are the top-level containers in Databricks that store databases.\nIn the DFO Pacific Lakehouse, catalogs are used to organize the medallion architecture.\nYou may be able to see multiple catalogs in your Databricks environment.\n\n# See what catalogs are available\ndbGetQuery(connsql, \"SHOW CATALOGS\")\n\n\n\nSchemas\nCheck available schemas in bronze catalogue\n\ndbGetQuery(connsql, \"SHOW SCHEMAS IN bronze_pacific_prod\")\n\n# check available schemas in bronze nuseds\ndbGetQuery(connsql, \"SHOW TABLES IN bronze_pacific_prod.nuseds_v2_0\")\n\n# Query bronze nuseds_v2_0 table\nnuseds_activity_types &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.nuseds_v2_0.activity_types LIMIT 10\")\n\n\n\nTables\nCheck out the tables in FOS_v1_1\n\ndbGetQuery(connsql, \"SHOW TABLES IN bronze_pacific_prod.FOS_v1_1\")\n\n\n\n\nDescribe Tables\nDescribe a FOS_v1_1 table to extract metadata about a specific table, like what columns are available and their data types.\n\ndbGetQuery(connsql, \"DESCRIBE bronze_pacific_prod.FOS_v1_1.stock\")\n\n\n\nRunning Queries\nRun a query to extract data from a table in the Lakehouse.\n\n# Query bronze nuseds_v2_0 smu_cu_lookup table\nsmu_cu_lookup &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.nuseds_v2_0.smu_cu_lookup LIMIT 10\")\n\nsmu_cu_lookup\n\nConnect to the FOS data and aggregrate:\n\n# Query bronze FOS_v1_1 stock table\nstock &lt;- dbGetQuery(connsql, \"SELECT * FROM bronze_pacific_prod.FOS_v1_1.stock\")\n\nTry using an aggregation function such as count and group by to get a count of the number of rows in the table.\n\n# Count the number of rows in the stock table\ndbGetQuery(connsql, \"SELECT COUNT(*) FROM bronze_pacific_prod.FOS_v1_1.stock\")",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#local-compute-versus-databricks-compute",
    "href": "how_to_guides/databricks_r_studio.html#local-compute-versus-databricks-compute",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Local Compute versus Databricks Compute",
    "text": "Local Compute versus Databricks Compute\nUsing Databricks compute should be minimized to avoid unnecessary costs. Instead, use your local computer for data exploration and analysis. Only use Databricks compute when you need to access new data, write data back to the Lakehouse, or run large-scale computations.\n\nWhen Databricks Compute is Used vs. Local Compute\nDatabricks Compute:\n\nRunning SQL queries and data processing tasks on the data stored in the Azure Lakehouse.\nAny operations that involve querying the data, such as dbGetQuery(connsql, “SELECT * FROM …”), are executed on Databricks compute resources.\n\nLocal Compute:\n\nWriting and executing R scripts that establish the connection to Databricks.\nProcessing the results returned from Databricks.\nManaging code with Git and GitHub from your local machine.\n\n\n\nTips to Minimize Databricks Compute Costs\nOptimize Queries: Write efficient SQL queries to minimize the amount of data processed and transferred to your local machine.\nUse Caching: Save data locally and don’t run dbGetQuery commands unless you are intentionally trying to get new data. Cache intermediate results when performing complex transformations to avoid redundant computations.\nFor example in R:\n\n# Query the data and save it to a variable\ndata &lt;- dbGetQuery(connsql, \"SELECT * FROM ...\") # Run the query only once or as needed for new data\n \n# Perform transformations on the data\ntransformed_data &lt;- data %&gt;% ...\n\n# Cache the transformed data\nsaveRDS(transformed_data, \"transformed_data.rds\")\n\n# Next time you need the transformed data, don't run the query again, instead load it from the cache or from your local copy\n\n# Load the transformed data from the cache\ntransformed_data &lt;- readRDS(\"transformed_data.rds\")\n\n# Continue working with the transformed data",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/databricks_r_studio.html#using-a-github-repo-in-databricks",
    "href": "how_to_guides/databricks_r_studio.html#using-a-github-repo-in-databricks",
    "title": "Connect to the Azure Lakehouse from R",
    "section": "Using a GitHub Repo in Databricks",
    "text": "Using a GitHub Repo in Databricks\nOne of the advantages of using GitHub with Databricks is that you can easily import code from a GitHub repository into a Databricks notebook. This allows you to leverage the version control and collaboration features of GitHub while working in Databricks.\nHowever, there are a few things to keep in mind when using a GitHub repo in Databricks:\n\nImporting Code: You can import code from a GitHub repository into a Databricks notebook by specifying the URL of the GitHub repository. Databricks will clone the repository and import the code into the notebook.\nCode Execution: When you import code from a GitHub repository into a Databricks notebook, the code is executed in the Databricks environment. This means that any data access or processing tasks will be performed on Databricks compute resources.\nCode Management: While you can import code from a GitHub repository into a Databricks notebook, you are not able to push code changes back to the repository directly from Databricks. If you need to make changes to the code, you will need to do so in the GitHub repository and then re-import the code into the Databricks notebook.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Connect to the Azure Lakehouse from R"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html",
    "href": "how_to_guides/openGovernmentPortalR.html",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "",
    "text": "This tutorial provides an overview for how to extract spatial data from the Open Government Portal, commonly referred to as Open Data. OGP is the Government of Canada’s official platform for publishing federal datasets, and DFO Science staff publish eligible data products there to meet open data policy requirements, ensure visibility of our work, and support reuse by partners and the public.\nThe code below shows how to leverage the resources available on OGP, making it easier to meet FAIR (Findable, Accessible, Interoperable, Reusable) principles and support reproducible science.\nThis tutorial reviews:\n\nQuerying ArcGIS REST services\nExtracting data from file geodatabases\nUsing ckanr to build more robust code\n\nThe examples cover the basics of mapping in R, including plotting both vector and raster data with ggplot(), and creating leaflet maps. Many online tutorials provide further detail for specifics of mapping in R if you need more background. For example, this tutorial from the University of Toronto provides a very good overview.\nNote that much of this content has been expanded from a similar tutorial created in the DFO Maritimes Region, but with DFO Pacific examples. Other clarifying comments have also been added.\n\n\nThe librarian package is useful for loading multiple packages in R. It allows you to load packages from CRAN, GitHub, and other sources with a single command. It also installs missing packages automatically.\n\n# Set CRAN mirror to avoid errors\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# Install librarian if not already installed\nif (!requireNamespace(\"librarian\", quietly = TRUE)) {\n  install.packages(\"librarian\")\n}\n\n# Load packages using librarian\nlibrarian::shelf(\n  arcpullr, ckanr, dplyr, devtools, ggplot2, ggspatial, leaflet, leafpop, mapview,\n  rnaturalearth, sf, stars, terra, viridisLite, ropensci/rnaturalearthhires # example for loading from GitHub\n)",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#background",
    "href": "how_to_guides/openGovernmentPortalR.html#background",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "",
    "text": "This tutorial provides an overview for how to extract spatial data from the Open Government Portal, commonly referred to as Open Data. OGP is the Government of Canada’s official platform for publishing federal datasets, and DFO Science staff publish eligible data products there to meet open data policy requirements, ensure visibility of our work, and support reuse by partners and the public.\nThe code below shows how to leverage the resources available on OGP, making it easier to meet FAIR (Findable, Accessible, Interoperable, Reusable) principles and support reproducible science.\nThis tutorial reviews:\n\nQuerying ArcGIS REST services\nExtracting data from file geodatabases\nUsing ckanr to build more robust code\n\nThe examples cover the basics of mapping in R, including plotting both vector and raster data with ggplot(), and creating leaflet maps. Many online tutorials provide further detail for specifics of mapping in R if you need more background. For example, this tutorial from the University of Toronto provides a very good overview.\nNote that much of this content has been expanded from a similar tutorial created in the DFO Maritimes Region, but with DFO Pacific examples. Other clarifying comments have also been added.\n\n\nThe librarian package is useful for loading multiple packages in R. It allows you to load packages from CRAN, GitHub, and other sources with a single command. It also installs missing packages automatically.\n\n# Set CRAN mirror to avoid errors\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# Install librarian if not already installed\nif (!requireNamespace(\"librarian\", quietly = TRUE)) {\n  install.packages(\"librarian\")\n}\n\n# Load packages using librarian\nlibrarian::shelf(\n  arcpullr, ckanr, dplyr, devtools, ggplot2, ggspatial, leaflet, leafpop, mapview,\n  rnaturalearth, sf, stars, terra, viridisLite, ropensci/rnaturalearthhires # example for loading from GitHub\n)",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#accessing-open-data-via-arcgis-rest",
    "href": "how_to_guides/openGovernmentPortalR.html#accessing-open-data-via-arcgis-rest",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Accessing Open Data via ARCGIS REST",
    "text": "Accessing Open Data via ARCGIS REST\nThe arcpullr package is a great way to access ArcGIS REST services, which are often used for storing spatial data on Open Data. You can use the get_spatial_layer() function to retrieve a spatial layer from an ArcGIS server.\n\nExample: Pacific Recreational Fishery Salmon Head Depots\nThere can many URLs associated with each dataset. The “ArcGIS Rest URL” we need to feed into the get_spatial_layer() function should be the one that includes “arcgis/rest/services”.\nFor example, with the Pacific Recreational Fishery Salmon Head dataset, the URL we are interested in is https://egisp.dfo-mpo.gc.ca/arcgis/rest/services/open_data_donnees_ouvertes/pacific_recreational_fishery_salmon_head_depots/MapServer/.\nIf you have trouble tracking this down, go to the appropriate dataset page on Open Data.\nThen, under the Data and Resources section, find the item labelled ESRI REST (there is both an English and French example). Left click on the Explore dropdown item, then right click on Go to resource, and then left click on Copy link. This then copies the link to the URL you need.\n\n\n\ngoToResource\n\n\nIt is important to ensure the appropriate layer is specified. In this example, the “0” at the end of the address denotes the English version, while a 1 represents French (see layers within the MapServer page). They also do not always correspond to language. For example, in the Pacific Commercial Salmon In Season Catch Estimates dataset, layers 0 to 3 represent gill nets, troll, seine and Pacific Fishery Management Areas, respectively.\n\nsalmonDepots = get_spatial_layer(\"https://egisp.dfo-mpo.gc.ca/arcgis/rest/services/open_data_donnees_ouvertes/pacific_recreational_fishery_salmon_head_depots/MapServer/0\")\n\n\n\nCreating a map\nThere are some great packages to get basemaps in R. The rnaturalearth package is a good option for getting country borders and other geographical features. You can use the ne_countries() function to get country borders.\nIn British Columbia, rnaturalearth may not have the level of detail required (e.g., some islands are missing). The bcmaps package has a lot of detailed basemaps and spatial data for the BC coast, and the bc_bound_hres() function is particularly useful for mapping coastline.\nSince the data used in the tutorial are for the entire BC coast, rnaturalearth is used in the examples below.\n\n# Get coastline for North America \ncoast = ne_countries(scale = 10, returnclass = c(\"sf\"), continent = \"North America\") \n\nBecause the coastline basemap data span all of North America, it may be useful to crop the plot to the bounding box of the data (BC coast). The st_bbox() function from the sf package allows you to get the bounding box of a spatial object. You can use the xlim and ylim arguments in the coord_sf() function to set the limits of the x and y axes.\n\n# Define lat/lon bounds of your dataset\nbounds = st_bbox(salmonDepots)\n\nggplot()+\n  geom_sf(data = coast, fill = \"lightgrey\")+\n  geom_sf(data = salmonDepots, size = 3.5, aes(fill = ACCESSIBILITY___ACCESSIBILITÉ), pch = 21, alpha = 0.85)+\n  coord_sf(xlim = c(bounds[\"xmin\"], bounds[\"xmax\"]),\n    ylim = c(bounds[\"ymin\"], bounds[\"ymax\"]))+\n  # I'm not always convinced maps north arrows need to be added when the lat/lon is specified but some people insist!\n  # Add north arrow to the top right (tr) of the map. It points to \"true\" north, not magnetic north\n  annotation_north_arrow(location = \"tr\", which_north = \"true\",\n                         #height = unit(0.7, \"cm\"), width = unit(0.7, \"cm\"),\n                         style = north_arrow_minimal)+\n  # Also, scale bars shouldn't be added when you're this zoomed out, but here's how to add one anyway to the bottom left (bl):\n  annotation_scale(location = \"bl\", text_cex = 0.8)+\n  scale_fill_discrete(name = \"Accessibility\")+\n  theme_bw() # tidies up the visuals",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#accessing-file-geodatabases",
    "href": "how_to_guides/openGovernmentPortalR.html#accessing-file-geodatabases",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Accessing file geodatabases",
    "text": "Accessing file geodatabases\nFile geodatabases are a proprietary file format developed by ESRI for spatial and non-spatial data. Many spatial datasets are uploaded to Open Data as file geodatabases, and it is possible to download and extract them with R!\nThe example below shows how to access the file geodatabase from the floating infrastructure dataset on Open Data\n\nCreating temporary directories\nFor exploring data, it is often helpful to create temporary directories to store and extract data. This can be easier than downloading the data directly and setting your directories.\n\n# Create a temporary directory to store the downloaded zip file\ntemp_dir = tempdir()\n\n# Define the path for the downloaded zip file inside the temp directory\nzip_file = file.path(temp_dir, \"zipPath.gdb.zip\")\n\n# Define the directory where the zip contents will be extracted\n# This is a relative path, so files will be extracted into the current working directory\nunzip_dir = file.path(\"extracted_fgdb\")\n\n# Download the dataset zip file. To get the correct link, follow the same steps as in the Pacific Recreational Fishery Salmon Head Depots example above, but with the \"FDGB/GDB\" item in Data and resources \ndownload.file(\n  \"https://api-proxy.edh-cde.dfo-mpo.gc.ca/catalogue/records/049770ef-6cb3-44ee-afc8-5d77d6200a12/attachments/floating_infrastructure-docks.gdb.zip\",\n  destfile = zip_file,\n  mode = \"wb\"\n)\n\n# Create the extraction directory if it doesn't already exist\ndir.create(unzip_dir, showWarnings = FALSE)\n\n# Unzip the downloaded file into the extraction directory\nunzip(zip_file, exdir = unzip_dir)\n\n# List the files extracted to verify contents\nlist.files(unzip_dir)\n\n[1] \"floating_infrastructure-docks.gdb\"\n\n# Load the layers available in the extracted .gdb file\n# Copy the name from the list.files() command. This should match the name of the gdb from the URL above\nlayers = st_layers(file.path(unzip_dir, \"floating_infrastructure-docks.gdb\"))\n\n# Turn layers from a list into a dataframe\n# layers has 1 entry called docks, so now a df called 'docks' will be created\nfor(l in layers$name){\n  message(\"Reading layer: \", l)\n  assign(l, st_read(file.path(unzip_dir, \"floating_infrastructure-docks.gdb\"), layer = l, quiet = TRUE))\n}\n\n\n\nMap the data\n\n# Note that the coordinate reference system of these two layers are different\ninvisible(st_crs(docks)) # NAD 83\ninvisible(st_crs(coast)) # WGS 84\n\n# To plot with ggplot, one of them has to be reprojected. I've transformed the docks CRS to match the coastline CRS \ndocks = st_transform(docks, st_crs(coast))\n\n# Redefine the bounds of the layer of interest\nboundsDocks = st_bbox(docks)\n\n# Make a map of the docks\nggplot()+\n  geom_sf(data = coast, fill = \"lightgrey\")+\n  geom_sf(data = docks, size = 3.5, pch = 21, aes(fill = region), alpha = 0.65)+\n  coord_sf(\n    xlim = c(boundsDocks[\"xmin\"], boundsDocks[\"xmax\"]),\n    ylim = c(boundsDocks[\"ymin\"], boundsDocks[\"ymax\"])\n  ) +\n  scale_fill_discrete(name = \"Region\")+\n  theme_bw()",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#plotting-raster-data",
    "href": "how_to_guides/openGovernmentPortalR.html#plotting-raster-data",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Plotting raster data",
    "text": "Plotting raster data\nAn example is shown here for accessing and plotting raster data stored in a geodatabase. You can follow the example in the Maritimes tutorial for accessing raster data from tiff files, if you are interested.\nThe example I’m using is from the recreational vessel traffic model on the BC coast.\n\n# Follow steps above to create a temporary folder to download & extract geodatabase info\ntemp_dir = tempdir()\nzip_file = file.path(temp_dir, \"bc_boating_model.zip\")\nunzip_dir = file.path(temp_dir, \"bc_boating_data\")\n\n# Download the dataset zip file. To get the correct link, follow the same steps as in the Pacific Recreational Fishery Salmon Head Depots example above, but with the \"FDGB/GDB\" item in Data and resources\ndownload.file(\n  url = \"https://api-proxy.edh-cde.dfo-mpo.gc.ca/catalogue/records/fed5f00f-7b17-4ac2-95d6-f1a73858dac0/attachments/Recreational_Boating_Data_Model.gdb.zip\",\n  destfile = zip_file,\n  mode = \"wb\"\n)\n\n# Unzip the contents\ndir.create(unzip_dir, showWarnings = FALSE)\nunzip(zip_file, exdir = unzip_dir)\n\n# List the file names you just downloaded \nboat_file = list.files(unzip_dir, full.names = TRUE)",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#mapping-the-survey-effort-data-with-ggplot",
    "href": "how_to_guides/openGovernmentPortalR.html#mapping-the-survey-effort-data-with-ggplot",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Mapping the survey effort data with ggplot",
    "text": "Mapping the survey effort data with ggplot\nUnfortunately, raster data in this format cannot be directly mapped with ggplot. But, you can convert it to a data frame and keep the coordinate positions.\nIf you read through the metadata, you’ll see that that many types of data are available in this geodatabase (e.g., point data, raster, vector grid). The data being plotted in this example below is the surveyeffort raster dataset, which is specified using the sub argument in the rast() function.\n\n# Turn the \"survey effort\" data into raster format\nsurveyRast = rast(boat_file, sub = \"surveyeffort\")\n\n# Use the project function to project to WGS 84 (i.e., to match the CRS of the coastline file)\nrast_proj = project(surveyRast, coast)\n\n# Convert it to a dataframe. Keep the coordinate info\nrast_df = as.data.frame(rast_proj, xy = TRUE) \n\n# Get the bounds for this. Using different code than above, since this is a regular data frame \nboundsRast = with(rast_df, c(xmin = min(x), xmax = max(x), ymin = min(y), ymax = max(y)))\n\n# Plot it with geom_tile and add the coast data as well\nggplot()+\n  geom_tile(data = rast_df, aes(x=x, y=y, fill = Recreational_Boating_Data_Model)) +\n  geom_sf(data = coast, fill = \"lightgrey\")+\n  scale_fill_viridis_c(option = \"magma\")+\n  coord_sf(xlim = c(boundsRast[\"xmin\"], boundsRast[\"xmax\"]),\n                ylim = c(boundsRast[\"ymin\"], boundsRast[\"ymax\"]))+\n  labs(x = NULL, y = NULL)+\n  theme_bw()\n\n\n\n\n\n\n\n\n\nMaking Leaflet Maps\nLeaflet maps are interactive maps that can be a great way to explore data. Leaflet maps can also be great for point data, since you can create popup labels where you can click and view your attribute data.\n\n# Convert raster to leaflet-compatible format\npal = colorNumeric(palette = \"magma\", domain = values(rast_proj), na.color = \"transparent\")\n\n# Create leaflet map\nleaflet() %&gt;%\n  addProviderTiles(providers$Esri.OceanBasemap) %&gt;%\n  addRasterImage(rast_proj, colors = pal, opacity = 0.7, project = T) %&gt;%\n  addLegend(pal = pal, values = values(rast_proj),\n            title = \"Survey Effort\",\n            position = \"bottomright\")",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "how_to_guides/openGovernmentPortalR.html#using-ckanr-to-build-more-robust-code",
    "href": "how_to_guides/openGovernmentPortalR.html#using-ckanr-to-build-more-robust-code",
    "title": "Accessing Spatial Data from the Open Government Portal",
    "section": "Using ckanr to build more robust code",
    "text": "Using ckanr to build more robust code\nSometimes, IT needs to upgrade server hardware or software, which can result in changes to the ArcGIS REST service URLs (i.e., the portion like this: https://egisp.dfo-mpo.gc.ca/arcgis/rest/services). However, the Universally Unique Identifier (UUID) associated with each dataset remains unchanged.\nIn the Open Data portal, the UUID is the final part of the URL after “dataset”. For example, in the Pacific Recreational Salmon Head Depots dataset (https://open.canada.ca/data/dataset/3cc03bbf-d59f-4812-996e-ddc52e0ba99e), the UUID is 3cc03bbf-d59f-4812-996e-ddc52e0ba99e.\nTo make our script more robust, we can use the CKAN API. CKAN is a data management system that supports publishing, sharing, and discovering datasets. The ckanr package in R allows us to interact with CKAN-based portals, such as Open Data. You can use the ckanr_setup() function to establish a connection.\nInstead of hardcoding the ArcGIS REST URL in our script (which may change) we can use the CKAN API to reference the stable UUID. This allows us to dynamically retrieve the current ArcGIS REST URL associated with that UUID, ensuring our code remains functional even if updates occur.\n\nDownloading data\nWe’ll use the Pacific Recreational Fishery Salmon Head Depots data as an example. We can use the ckanr package to extract the ArcGIS REST URL by referencing the UUID. This may seem like an extra step while you’re writing your code, but it will save you time in the long run if the URLs change (it can happen!!)\n\nckanr_setup(\"https://open.canada.ca/data\")\nuuid = \"3cc03bbf-d59f-4812-996e-ddc52e0ba99e\"\npkg = ckanr::package_show(uuid)\nresources = pkg$resources\ndf_resources = as.data.frame(do.call(rbind, resources))\n\n# Now you can grab the correct ESRI Rest URL (\"url\"). Make sure to search for that format, and also specify you want the English (\"En\") version and not French\nsalmon_url = unlist(df_resources[df_resources$format == \"ESRI REST\" & sapply(df_resources$language, function(x) \"en\" %in% x), \"url\"])\n\n\n\nSearching for data\nYou can also use the ckanr package to search for data using the package_search() function. You can use the q argument to search for key words, and the fq argument to specify file types.\n\nckanr_setup(\"https://open.canada.ca/data\")\nsearch_results = package_search(q = \"Pacific salmon\")\nsearch_results # Note that UUID also shows up here after the &lt;CKAN Package&gt; text\n\n# You  can also search specifically for spatial data (e.g., shapefiles, geodatabases, etc.) by specifying the fq argument\nsearch_results = package_search(\n  q = \"Pacific salmon\",\n  fq = 'res_format:(SHP OR GDB OR GeoJSON OR KML OR CSV)' # CSVs sometimes contain spatial data, sometimes not\n)",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Accessing Spatial Data from the Open Government Portal"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FADS Open Science Documentation Hub 🐟💾",
    "section": "",
    "text": "FADS Open Science Documentation Hub 🐟💾\nWelcome to the Fishery & Assessment Data Section (FADS) Open Science Documentation Hub! This is the public facing portion of our internal FADS Wiki on SharePoint.\nHere, we work in the open and share our knowledge to enable open data science collaboration with a broad community not only within DFO but also externally with academics, NGOs, and Indigenous groups.\nWhether you’re a seasoned data steward or just starting out, this hub is designed to provide you with the resources you need to navigate the world of salmon data stewardship and management within DFO 📚✨\nTo keep documentation organized, useful, and easy to navigate docs are grouped into four clear categories:\n\n\n️1. How-to Guides 🛠\nQuick, practical instructions for specific tasks.\nPerfect for when you’re asking:\n&gt; “How do I do this specific thing?”\nExamples:\n\nHow to request access to datasets stored in the cloud 🔐\n\nHow to submit a metadata record 📤\n\nHow to clean up an Excel file before uploading 🧼\n\n\n\n\n2. Reference Info 📘\nThe deep, detailed, no-fluff facts about tools, terms, and standards.\nUse these when you need precision or technical definitions.\nExamples:\n\nUsing terms from controlled vocabularies like Darwin Core 🧾\nOn hosting data and code 🫙\nTools, including reccomended R packages 🧑🏾‍💻\n\n\n\n\n3. Tutorials 🎓\nStructured, step-by-step lessons that help you learn by doing.\nIdeal for onboarding or skill-building.\nExamples:\n\nA walk through on using Power BI with our datasets 📊\nIntro to working with Synapse Notebooks 🧠\nHow to support controlled vocabularies 🔤\n\n\n\n\n4. Deep dives 🧠\nThoughtful writeups and explanations on the why behind the work.\nGreat for context, decision-making, or just satisfying your inner data philosopher.\nExamples:\n\nWhy metadata quality matters 🧼\nData lifecycle management explained 🌱➡️🌳\nThe theory behind our access protocols 🚦\n\n\n\n\nGetting Started 🛠️\n🧭 Start exploring using the navigation menu on your left to dive in!\n❔ Learn more about what resources are available on the About page.\n🔍 Use the search bar at the top right to find specific topics or resources.\n\n\n\nContribute & Collaborate! 🤝\nThis documentation is a living resource — always evolving and improving.\nWe’d love your feedback, suggestions, and ideas! 🗣️\nYou can even contribute directly to the documentation! See our Contributing Guide for more details. Spotted something confusing? Missing a topic? Got an awesome tip to share?\n➡️ Reach out to us anytime — we’re listening 👂 and always happy to help!\nHappy stewarding! 🐟✨\n\n↩︎️ Return to the Internal FADS SharePoint Wiki"
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html",
    "href": "reference_info/dwc-obis-gbif.html",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "This guide introduces the Darwin Core (DwC) standard and its role in supporting data interoperability and publication for Fisheries and Oceans Canada (DFO). It also provides an overview of the Unified GBIF Data Model and the Ocean Biodiversity Information System (OBIS). The focus is on how commonly used fields in DFO fisheries datasets map to Darwin Core terms, enabling integration across internal and external systems.\n\n\n\nDarwin Core is a biodiversity data standard maintained by Biodiversity Information Standards (TDWG). It provides a stable, referenceable set of terms designed to facilitate sharing information about biological diversity — particularly the occurrence of taxa in nature, along with associated metadata like location, time, method, and taxonomy.\nDarwin Core is intentionally broad, supporting data from natural history collections, ecological surveys, tagging programs, and more.\n\n\n\n\nIn the DFO Pacific Region, diverse data formats and schemas are used across programs, divisions, and systems. Darwin Core provides a common data standard that enables these internal formats to be mapped to:\n\nOther DFO systems\nNational open data platforms\nInternational biodiversity repositories (e.g., GBIF, OBIS)\n\nBy treating Darwin Core as a semantic translation layer, we can harmonize our internal datasets for interoperability, discoverability, and reuse, without requiring all groups to adopt the same database design.\n\n\n\n\nThe GBIF Unified Data Model extends Darwin Core to support complex biodiversity datasets, including:\n\nEvents (e.g., sampling campaigns, cruises, transects)\nOccurrences (e.g., individual specimens, observations)\nMeasurements or Facts (e.g., biological measurements, environmental conditions)\n\nThis model aligns with DFO data practices that involve nested relationships between surveys, gear deployments, catches, and individual biological samples.\n\n\n\n\nOBIS builds on Darwin Core and the GBIF model to support marine biodiversity data. It uses a Darwin Core Archive format with specific extensions for:\n\nSampling Effort\nEnvironmental Parameters\nTaxonomic Verification\n\nOBIS is the global node for marine biodiversity under the UN Ocean Decade and is highly relevant for DFO’s coastal and offshore research.\n\n\n\n\nBelow are examples of how commonly used columns in DFO data systems align with Darwin Core terms.\n\n\n\n\n\n\n\n\nDFO Field / Concept\nDarwin Core Term\nNotes\n\n\n\n\nSpecies_Code or Species_Name\nscientificName\nCan be resolved via WoRMS or NCBI\n\n\nSample_ID\noccurrenceID\nA globally unique identifier for the specimen or observation\n\n\nCatch_Date\neventDate\nIdeally formatted as ISO 8601\n\n\nSurvey_Name\neventID\nUse hierarchical eventIDs for nested events (e.g., cruise → tow → sample)\n\n\nLatitude, Longitude\ndecimalLatitude, decimalLongitude\nUse WGS84 datum\n\n\nDepth_m\nminimumDepthInMeters, maximumDepthInMeters\nUse both where applicable\n\n\nSex, Maturity, Length, Weight\nmeasurementType, measurementValue, measurementUnit\nRecorded under the MeasurementOrFact extension\n\n\nGear_Type\nsamplingProtocol\nUse consistent terminology for methods\n\n\nArea_Code, Subarea\nlocationID, locality\nAlign to spatial standards like NAFO/DFO zones\n\n\nData_Source, Project_Name\ndatasetName, recordedBy\nIdentify project or responsible team\n\n\n\n\n\n\n\nDFO’s long-term goal is to develop controlled vocabularies and ontology modules that align with:\n\nDarwin Core term values\nThe NERC Vocabulary Server (NVS) for environmental parameters\nWorld Register of Marine Species (WoRMS) for taxonomic resolution\nCF Conventions for oceanographic and climate data\nThe Salmon Ontology developed by NCEAS\n\nThese efforts will allow for semantic alignment, improving both machine-readability and clarity for analysts and stakeholders.\n\n\n\n\n\nWe are actively building a DFO metadata crosswalk using Darwin Core as the central translation layer\nLook for updates on term definitions, controlled value lists, and validation tools\nJoin our discussions on standardizing data fields and contributing to the controlled vocabulary development\n\nFor questions, feedback, or help mapping your dataset, contact the Data Stewardship Unit.\n\nDarwin Core is not a database format. It’s a lingua franca for biodiversity data. Use it to describe, translate, and publish your data — not to force everything into the same structure.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#what-is-darwin-core",
    "href": "reference_info/dwc-obis-gbif.html#what-is-darwin-core",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "Darwin Core is a biodiversity data standard maintained by Biodiversity Information Standards (TDWG). It provides a stable, referenceable set of terms designed to facilitate sharing information about biological diversity — particularly the occurrence of taxa in nature, along with associated metadata like location, time, method, and taxonomy.\nDarwin Core is intentionally broad, supporting data from natural history collections, ecological surveys, tagging programs, and more.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#darwin-core-as-a-translation-layer",
    "href": "reference_info/dwc-obis-gbif.html#darwin-core-as-a-translation-layer",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "In the DFO Pacific Region, diverse data formats and schemas are used across programs, divisions, and systems. Darwin Core provides a common data standard that enables these internal formats to be mapped to:\n\nOther DFO systems\nNational open data platforms\nInternational biodiversity repositories (e.g., GBIF, OBIS)\n\nBy treating Darwin Core as a semantic translation layer, we can harmonize our internal datasets for interoperability, discoverability, and reuse, without requiring all groups to adopt the same database design.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#the-unified-gbif-data-model",
    "href": "reference_info/dwc-obis-gbif.html#the-unified-gbif-data-model",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "The GBIF Unified Data Model extends Darwin Core to support complex biodiversity datasets, including:\n\nEvents (e.g., sampling campaigns, cruises, transects)\nOccurrences (e.g., individual specimens, observations)\nMeasurements or Facts (e.g., biological measurements, environmental conditions)\n\nThis model aligns with DFO data practices that involve nested relationships between surveys, gear deployments, catches, and individual biological samples.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#obis-ocean-biodiversity-information-system",
    "href": "reference_info/dwc-obis-gbif.html#obis-ocean-biodiversity-information-system",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "OBIS builds on Darwin Core and the GBIF model to support marine biodiversity data. It uses a Darwin Core Archive format with specific extensions for:\n\nSampling Effort\nEnvironmental Parameters\nTaxonomic Verification\n\nOBIS is the global node for marine biodiversity under the UN Ocean Decade and is highly relevant for DFO’s coastal and offshore research.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#mapping-dfo-fisheries-data-to-darwin-core",
    "href": "reference_info/dwc-obis-gbif.html#mapping-dfo-fisheries-data-to-darwin-core",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "Below are examples of how commonly used columns in DFO data systems align with Darwin Core terms.\n\n\n\n\n\n\n\n\nDFO Field / Concept\nDarwin Core Term\nNotes\n\n\n\n\nSpecies_Code or Species_Name\nscientificName\nCan be resolved via WoRMS or NCBI\n\n\nSample_ID\noccurrenceID\nA globally unique identifier for the specimen or observation\n\n\nCatch_Date\neventDate\nIdeally formatted as ISO 8601\n\n\nSurvey_Name\neventID\nUse hierarchical eventIDs for nested events (e.g., cruise → tow → sample)\n\n\nLatitude, Longitude\ndecimalLatitude, decimalLongitude\nUse WGS84 datum\n\n\nDepth_m\nminimumDepthInMeters, maximumDepthInMeters\nUse both where applicable\n\n\nSex, Maturity, Length, Weight\nmeasurementType, measurementValue, measurementUnit\nRecorded under the MeasurementOrFact extension\n\n\nGear_Type\nsamplingProtocol\nUse consistent terminology for methods\n\n\nArea_Code, Subarea\nlocationID, locality\nAlign to spatial standards like NAFO/DFO zones\n\n\nData_Source, Project_Name\ndatasetName, recordedBy\nIdentify project or responsible team",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#toward-controlled-vocabularies-and-ontologies",
    "href": "reference_info/dwc-obis-gbif.html#toward-controlled-vocabularies-and-ontologies",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "DFO’s long-term goal is to develop controlled vocabularies and ontology modules that align with:\n\nDarwin Core term values\nThe NERC Vocabulary Server (NVS) for environmental parameters\nWorld Register of Marine Species (WoRMS) for taxonomic resolution\nCF Conventions for oceanographic and climate data\nThe Salmon Ontology developed by NCEAS\n\nThese efforts will allow for semantic alignment, improving both machine-readability and clarity for analysts and stakeholders.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/dwc-obis-gbif.html#next-steps",
    "href": "reference_info/dwc-obis-gbif.html#next-steps",
    "title": "Darwin Core, GBIF, and OBIS Reference Guide",
    "section": "",
    "text": "We are actively building a DFO metadata crosswalk using Darwin Core as the central translation layer\nLook for updates on term definitions, controlled value lists, and validation tools\nJoin our discussions on standardizing data fields and contributing to the controlled vocabulary development\n\nFor questions, feedback, or help mapping your dataset, contact the Data Stewardship Unit.\n\nDarwin Core is not a database format. It’s a lingua franca for biodiversity data. Use it to describe, translate, and publish your data — not to force everything into the same structure.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Darwin Core, GBIF, and OBIS Reference Guide"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html",
    "href": "reference_info/openDataPractices.html",
    "title": "Data Practices for Open Science – 3 Quick Tips",
    "section": "",
    "text": "Sir Isaac Newton said, “If I have seen further, it is by standing on the shoulders of giants.” As scientists, we aim to share our findings with colleagues who may use the information to continue to make progress on important questions. Making your data more transparent and reproducible not only contributes to a more useful body of work but provides “shoulders” for those who come after you (other colleagues, students, and collaborators). It’s also handy for improving your own workflow. Your personal peace of mind is another added bonus to front loading the work of organizing and describing your dataset.\nYou might be wondering what are we referring to when we talk about “Open Science”. A more open approach to research means making results accessible for the benefits of both scientists and broader society (UNESCO 2023). The Open Science movement aims to increase transparency and the speed of knowledge transfer. Barriers to Open Science include things like: paywalled journals, favouring knowledge produced by high-income countries, hidden or unknown source code or workflow, and supporting data that is unavailable or unusable. In this article, we’ll tackle the last barrier and talk about practical ways to make your data more Open.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#use-a-tidy-format",
    "href": "reference_info/openDataPractices.html#use-a-tidy-format",
    "title": "Data Practices for Open Science – 3 Quick Tips",
    "section": "1. Use a tidy format",
    "text": "1. Use a tidy format\n Illustration from Tidy Data for reproducibility, efficiency, and collaboration by Julia Lowndes and Allison Horst. Adapted from Wickham (2014).\nFor an excellent introduction to Tidy Data, check out Hadley Wickham’s 2014 paper in the Journal of Statistical Software (Wickham 2014). The tidy approach to data dictates that when organizing data, every observation should be represented by a row and every variable a column. This is a great approach for standardization and machine readability.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#redundancy-in-data-is-ok-sometimes",
    "href": "reference_info/openDataPractices.html#redundancy-in-data-is-ok-sometimes",
    "title": "Data Practices for Open Science – 3 Quick Tips",
    "section": "2. Redundancy in data is OK sometimes!",
    "text": "2. Redundancy in data is OK sometimes!\nThroughout our scientific careers, we’re often told to be concise and simplify so this advice may surprise you. Strategic redundancy in data allows you to make linkages between different pieces of information. It also saves space. One way to invoke the hidden power of redundancy is using metadata and data dictionaries to link important context to your primary dataset. For example, this table contains two types of observations: song information and Billboard rankings with one entry (i.e., row) each week the song remains on the Billboard Hot 100. Look what happens when we split the information into two tables: one with the song titles, artists’ names and run times; and the other with details on their Billboard rankings. This:\n\nAvoids confusion at scale – note how there are two types of temporal data in the first table. One could conflate time and date.\nSaves space. Say there were 100 songs with an average of 7 entries each. Rather than a table with 700 entries and 7 variables (4900 pieces of data), you now have 2 tables (containing 4500 data points total). This difference in memory usage scales with the size of your dataset.\n\n Table 1. Too much information at different scales crammed into a single table. Table adapted from Wickham (2014).\n Table 2: Example of using redundancy by splitting tables in two. Note how one song (left) has multiple Billboard rankings (right). Tables adapted from Wickham (2014).",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "reference_info/openDataPractices.html#keep-raw-data-raw",
    "href": "reference_info/openDataPractices.html#keep-raw-data-raw",
    "title": "Data Practices for Open Science – 3 Quick Tips",
    "section": "3. Keep raw data raw",
    "text": "3. Keep raw data raw\nSeparating your raw data from analyses is essential for reproducibility. After all, how can we reproduce an analysis if we have no access to the original dataset? In repositories, create a folder named RAW. Place unmanipulated data here. Do not open these files except to add or remove raw data. You may consider setting them to “read-only” when viewing in Excel. Keep all analyses in your scripts and outside of Excel, which will read the data and create outputs from it. Use a descriptive file name for any outputs generated from raw data and place them in a separate folder.\nEmbracing Open Science practices doesn’t have to be overwhelming. By adopting tidy data formats, allowing for strategic redundancy, and keeping raw data untouched, you can contribute to a more transparent and collaborative scientific community and yourself up for smoother workflows and more reproducible research. These small, intentional steps can have a big impact, helping others build on your work and accelerating the pace of discovery.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Practices for Open Science -- 3 Quick Tips"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "🤝 Contributing to the DSU Docs & Tools",
    "section": "",
    "text": "Thanks for helping improve the Data Stewardship Unit (DSU) site! 🎉\nWhether you’re adding a how-to guide, writing up a tutorial, or sharing a helpful tool, your contribution helps make data stewardship easier for everyone in the Pacific Region Science Branch. 🐟🌲\n\n\n\nIf you’re already a member of the dfo-pacific-science organization, you can edit the repo directly. If not, you can still contribute by creating a pull request (PR) from your forked copy of the repo or creating an issue to request a change.\nWant to add a new documentation or tools page? Here’s how!\nYou can open the repo after cloning it in an IDE like RStudio or VSCode depending on your preference.\n\n\n\nAn IDE (RStudio or VSCode)\nGit SCM\nQuarto\n\n\n\n\nTo open the website locally, clone the repository with git by running git clone https://github.com/dfo-pacific-science/data-stewardship-unit.git in terminal in your IDE.\n\n\n\nMake a new branch so that you can test things out without messing things up in the main branch. Run git branch {branchName} in the terminal in your IDE to make a new branch and git switch {branchName} to switch to it.\n\n\n\nYou can create a new .qmd file in one of the four folders (how_to_guides, reference_info, tutorials, deep_dives) to get it displayed under the navbar menu, which grabs .qmd files automatically.\n\n\n\nIn terminal, run quarto preview. A browser window will pop up with a locally-hosted version of the website. Make sure your changes look good and progress to the next step. Make edits if needed.\n\n\n\nIn terminal, run quarto render. The complete site will be rendered in the _site folder for Github actions to deploy.\n\n\n\nRun git add {fileName} for all of the files you want to stage. Make sure to not forget the _free folder, which contains all of the information related to R code that runs in some of the .qmd’s.\nRun git commit -m \"commit message here\".\nRun git push. If you’ve never pushed on your current branch, git will prompt you to use something like git push --set-upstream origin {branchName}.\n\n\n\nCreate a pull request from your branch and assign a team member to review it. There are protections on the main branch that caution you away from pushing without a review. Once the PR has been merged to main, the Github Action will be automatically triggered. Check the actions tab to see the deployment progress.\n\n\n\n\n\n\n\n\nUse feature branches off the main branch if working on a larger update (e.g. fix/typo, add/new-tool-guide)\nOtherwise for small updates, commit directly to main or create your Pull Request directly from main.\nGroup related changes into one PR\nIf you’re fixing an Issue, mention it in your PR:\nCloses #42 will auto-link and close the issue on merge\nKeep commit messages short but clear and in present tense (\"Add how-to for bulk metadata upload\")\n\n\n\n\n\n\nUse GitHub Issues to: - Request a new topic 📌 - Report a problem 🐛 - Share an idea 💡\nWe tag and triage regularly — feel free to assign yourself or ask for help!\n\n\n\n\nOpen an issue, ping someone on the DSU team, or check the README for more guidance. We’re here to help! Thanks again for contributing — every little improvement adds up."
  },
  {
    "objectID": "CONTRIBUTING.html#how-to-write-a-new-page",
    "href": "CONTRIBUTING.html#how-to-write-a-new-page",
    "title": "🤝 Contributing to the DSU Docs & Tools",
    "section": "",
    "text": "If you’re already a member of the dfo-pacific-science organization, you can edit the repo directly. If not, you can still contribute by creating a pull request (PR) from your forked copy of the repo or creating an issue to request a change.\nWant to add a new documentation or tools page? Here’s how!\nYou can open the repo after cloning it in an IDE like RStudio or VSCode depending on your preference.\n\n\n\nAn IDE (RStudio or VSCode)\nGit SCM\nQuarto\n\n\n\n\nTo open the website locally, clone the repository with git by running git clone https://github.com/dfo-pacific-science/data-stewardship-unit.git in terminal in your IDE.\n\n\n\nMake a new branch so that you can test things out without messing things up in the main branch. Run git branch {branchName} in the terminal in your IDE to make a new branch and git switch {branchName} to switch to it.\n\n\n\nYou can create a new .qmd file in one of the four folders (how_to_guides, reference_info, tutorials, deep_dives) to get it displayed under the navbar menu, which grabs .qmd files automatically.\n\n\n\nIn terminal, run quarto preview. A browser window will pop up with a locally-hosted version of the website. Make sure your changes look good and progress to the next step. Make edits if needed.\n\n\n\nIn terminal, run quarto render. The complete site will be rendered in the _site folder for Github actions to deploy.\n\n\n\nRun git add {fileName} for all of the files you want to stage. Make sure to not forget the _free folder, which contains all of the information related to R code that runs in some of the .qmd’s.\nRun git commit -m \"commit message here\".\nRun git push. If you’ve never pushed on your current branch, git will prompt you to use something like git push --set-upstream origin {branchName}.\n\n\n\nCreate a pull request from your branch and assign a team member to review it. There are protections on the main branch that caution you away from pushing without a review. Once the PR has been merged to main, the Github Action will be automatically triggered. Check the actions tab to see the deployment progress."
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-tips",
    "href": "CONTRIBUTING.html#pull-request-tips",
    "title": "🤝 Contributing to the DSU Docs & Tools",
    "section": "",
    "text": "Use feature branches off the main branch if working on a larger update (e.g. fix/typo, add/new-tool-guide)\nOtherwise for small updates, commit directly to main or create your Pull Request directly from main.\nGroup related changes into one PR\nIf you’re fixing an Issue, mention it in your PR:\nCloses #42 will auto-link and close the issue on merge\nKeep commit messages short but clear and in present tense (\"Add how-to for bulk metadata upload\")"
  },
  {
    "objectID": "CONTRIBUTING.html#working-with-issues",
    "href": "CONTRIBUTING.html#working-with-issues",
    "title": "🤝 Contributing to the DSU Docs & Tools",
    "section": "",
    "text": "Use GitHub Issues to: - Request a new topic 📌 - Report a problem 🐛 - Share an idea 💡\nWe tag and triage regularly — feel free to assign yourself or ask for help!"
  },
  {
    "objectID": "CONTRIBUTING.html#need-help",
    "href": "CONTRIBUTING.html#need-help",
    "title": "🤝 Contributing to the DSU Docs & Tools",
    "section": "",
    "text": "Open an issue, ping someone on the DSU team, or check the README for more guidance. We’re here to help! Thanks again for contributing — every little improvement adds up."
  },
  {
    "objectID": "reference_info/data_standards/vocab-index.html",
    "href": "reference_info/data_standards/vocab-index.html",
    "title": "DFO Salmon Data Controlled Vocabulary",
    "section": "",
    "text": "A growing challenge in salmon research, management, and data stewardship is the proliferation of inconsistent terms, acronyms, and definitions across documents, programs, and even within the same organization—leading to confusion, miscommunication, and inefficiencies in analysis and reporting.\nTo address this, the Data Stewardship Unit (DSU) has initiated the DFO Salmon Data Controlled Vocabulary—a standardized, community-curated list of key terms and definitions related to salmon data collection, analysis, and policy within Fisheries and Oceans Canada (DFO) Pacific Region.\nThe table below consolidates definitions from regulatory documents, scientific literature, internal guidelines, and other sources into a single, searchable, and filterable reference designed to improve semantic clarity across DFO Pacific.\nThe table is more than a simple glossary: it is a formally structured controlled vocabulary that uses the W3C-endorsed Simple Knowledge Organization System (SKOS), enabling machine-readable formats for integration into data systems, APIs, and web applications.For a deep-dive on the how and why of controlled vocabularies, see our guide: Support Controlled Vocabularies.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Standards",
      "DFO Salmon Data Controlled Vocabulary"
    ]
  },
  {
    "objectID": "reference_info/data_standards/vocab-index.html#dfo-salmon-controlled-vocabulary-terms-under-construction",
    "href": "reference_info/data_standards/vocab-index.html#dfo-salmon-controlled-vocabulary-terms-under-construction",
    "title": "DFO Salmon Data Controlled Vocabulary",
    "section": "🗂️DFO Salmon – Controlled Vocabulary Terms (🚧Under Construction🚧)",
    "text": "🗂️DFO Salmon – Controlled Vocabulary Terms (🚧Under Construction🚧)\n\n\n\n\n\n\nUnder the hood, the vocabulary is formatted in SKOS (Simple Knowledge Organization System), a W3C-recommended framework for representing controlled vocabularies and thesauri. By modeling terms and their relationships (broader, narrower, related), SKOS makes it easy to:\n\nMaintain consistent labels, definitions, and concept hierarchies.\nShare and integrate the vocabulary with other systems that understand RDF.\n\nThe vocabulary is intended to be a living document that evolves with the needs of the community. It is maintained by the Data Stewardship Unit (DSU) within the Fisheries and Assessment Data Section (FADS) in the Pacific Region of Fisheries and Oceans Canada (DFO).\nTo maximize accessibility, we publish three serializations:\n\nJSON – A lightweight format ideal for most web and desktop applications that need to load term lists and definitions without RDF tooling.\nJSON-LD – Embeds linked-data context so that terms become addressable URIs; perfect for web pages, APIs, and any environment where you want automatic context resolution and richer interoperability.\nTurtle (.ttl) – A compact, human-readable RDF syntax for semantic-web tools, triple stores, and SPARQL queries, enabling advanced semantic reasoning or integration into broader data ecosystems.\n\nYou can download or query any of these formats directly from our GitHub repo for programmatic access, metadata validation, integration, or automated workflows.\nFile a GitHub Issue at https://github.com/dfo-pacific-science/data-stewardship-unit/issues\nOr contact the Data Stewardship Unit.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Data Standards",
      "DFO Salmon Data Controlled Vocabulary"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html",
    "href": "reference_info/publishing_data_externally.html",
    "title": "Hosting Data and Code",
    "section": "",
    "text": "This guide is intended for scientists and technical staff in the DFO Pacific Region Science Branch. It outlines what is allowed, restricted, or prohibited when using internal and external platforms (e.g., Sharepoint, GitHub, Zenodo, Google Drive) to manage and share DFO Science data, code, and metadata, including during Fishery Science Advisory Report (FSAR) workflows.\nFor FSAR workflows, we recommend authors save their data in the Sharepoint for the Pacific Salmon Data MS Team. All data should be accompanied by a data dictionary with a README. Github can be used to host the code associated with analyses.\nTable 1. External Platform Use Guidelines for three data classifications of both data and code: Unclassified, Protected A, and Protected B. Note that this table does not cover Protected C or Classified data which are not permitted on external platforms.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#data-classification-quick-reference",
    "href": "reference_info/publishing_data_externally.html#data-classification-quick-reference",
    "title": "Hosting Data and Code",
    "section": "🔐 Data Classification Quick Reference",
    "text": "🔐 Data Classification Quick Reference\n\nUnclassified: No expected harm from disclosure. Includes open data, public metadata, and code that contains no sensitive logic or credentials.\n\nExamples: Published R packages, fish counts aggregated by region and year, habitat model source code without raw data. Note, however, even unclassified data has to be approved for release by the Regional Director of Science.\n\nProtected A: Could cause low-level injury if disclosed (e.g., minor confidentiality breach, operational disruption).\n\nExamples: Unpublished but non-sensitive biological metrics tied to location or project; fishing effort by vessel class if potentially re-identifiable; unreviewed site-specific model outputs.\n\nProtected B: Could cause serious injury if disclosed (e.g., violation of legal obligations, significant economic harm).\n\nExamples: Commercial catch by vessel or license number; sensitive species occurrences; data provided under confidentiality agreements.\n\n\n\n🔗 Refer to the Policy on Government Security for classification definitions and examples.\n🔎 Unsure if data is protected? Consult your divisional data contact or ATIP office.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#platform-by-platform-guidance",
    "href": "reference_info/publishing_data_externally.html#platform-by-platform-guidance",
    "title": "Hosting Data and Code",
    "section": "🧭 Platform-by-Platform Guidance",
    "text": "🧭 Platform-by-Platform Guidance\n\n✅ GitHub (Public and Private)\n\nRecommended for: Non-sensitive, unclassified code, metadata schemas, documentation, workflows.\nAllowed: Unclassified data/code only. Data must go through the Approvals and Internal publishing process before being posted anywhere public.\nPrivate repos: Use for drafting and collaboration before public release. Do not use for long-term data storage.\nFSAR Guidance: Releasing data in a public GitHub repo before approval by CSAS and before the release has been approved by the Regional Director of Science is a breach of the Access to Information Act and/or Privacy Act.\n\n\n🔗 TBS Directive on Open Government, Section 6.1 – Open information by default, subject to security/privacy. 🔗 DFO Policy for Scientific Data - “it is the responsibility of the Regional Director of Science to designate specific Data as classified for preventing its open sharing.”\n\n\n\n\n🚫 GitHub Codespaces / Copilot\n\nCodespaces: Cloud-hosted dev environments. Not GC-approved for sensitive data. Fine for non-sensitive code.\nCopilot: Do not enter Protected data/code. Avoid unless using for generic logic.\n\n\n🔗 TBS Guide on Generative AI\n\n\n\n\n✅ Canadian Integrated Ocean Observing System, Zenodo, Dryad, Figshare\n\nUse after FSAR or publication approval.\nGood for creating DOIs and enabling citation and disseminating broadly.\nData must be fully unclassified, reviewed, and approved.\n\n\n\n\n✅ GC Open Data Portal\n\nOfficial repository for open DFO datasets.\nRequires metadata, data steward approval, and Open Government Licence.\n\n\n🔗 Open Government Licence\n🔗 Open Data Portal Submission Guide",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#internal-dfo-requirements",
    "href": "reference_info/publishing_data_externally.html#internal-dfo-requirements",
    "title": "Hosting Data and Code",
    "section": "🧾 Internal DFO Requirements",
    "text": "🧾 Internal DFO Requirements\n\n📤 Internal release process\n\nManaged by the Marine Spatial Data Infrastructure System and the Enterprise Data Hub Follow the official steps here\n\nFOr the steps specific to Approvals and Publishing see step 4 for the Release Criteria Checklist\n\n\n⏳ When to Release?\n\nFor FSAR-related data: Only after advice is finalized and report is approved.\nFor code/metadata tools: May be shared publicly once confirmed to contain no sensitive content.\n\n\n🔗 CSAS Procedural Manuals – Supporting material must not be released until advice is finalized.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/publishing_data_externally.html#need-help",
    "href": "reference_info/publishing_data_externally.html#need-help",
    "title": "Hosting Data and Code",
    "section": "📬 Need Help?",
    "text": "📬 Need Help?\n\nData Classification or Release Questions: DFO.PACIFIC.SCIENCE.DATA@dfo-mpo.gc.ca\nCSAS Advice Process Questions: Contact your regional CSAS Coordinator\nOpen Science & GitHub Use: Data Stewardship Unit\n\n\n\nℹ️ This document will be updated periodically. Please check back for revised guidance as policies evolve.",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Hosting Data and Code"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html",
    "href": "reference_info/tools/r_packages.html",
    "title": "R Packages",
    "section": "",
    "text": "Welcome to the R Packages section of the DSU site! 🐟\nHere you’ll find a curated set of R packages that support your work as a data steward or data producer within the Pacific Region Science Branch of Fisheries and Oceans Canada. 🧪🌊\n\n\ntidyr, an offering from the tidyverse is the speedy Swiffer mop to your messy data. It provides tools for following the ethos of tidy data, which holds the following tenets:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFor example, make your wide formatted data more tidy with pivot_longer() or deal with missing values with drop_na().\n\n\n\n\n\n\nTipWant to load all the tidyverse packages at once?\n\n\n\nTry library(tidyverse) to load all of those amazing tools in one line\n\n\n\n\n\nggplot2 is a powerful tool for data visualization offered under the tidyverse umbrella.\nAs any scientist knows, visualizing your data is just as important as generating it. After all, how can you communicate your results if they’re hidden away in a table?\nThe R graph gallery has countless of great examples of plots created with ggplot2. Check them out!\n\n\n\nAnother tidyverse offering, dplyr is an essential package for data manipulation. Try out generating summary stats in a breeze with group_by() |&gt; summarise() or join two datasets with a left_join().\n\n\n\narrow is an amazing tool for working with larger than memory data. R usually performs computations in RAM (your computer’s short term memory), but this can pose a problem for larger datasets. Arrow moves computations onto disk (your computer’s long term memory) to avoid this.\nFor a tutorial, see here",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Tools",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#tidyr",
    "href": "reference_info/tools/r_packages.html#tidyr",
    "title": "R Packages",
    "section": "",
    "text": "tidyr, an offering from the tidyverse is the speedy Swiffer mop to your messy data. It provides tools for following the ethos of tidy data, which holds the following tenets:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\nFor example, make your wide formatted data more tidy with pivot_longer() or deal with missing values with drop_na().\n\n\n\n\n\n\nTipWant to load all the tidyverse packages at once?\n\n\n\nTry library(tidyverse) to load all of those amazing tools in one line",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Tools",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#ggplot2",
    "href": "reference_info/tools/r_packages.html#ggplot2",
    "title": "R Packages",
    "section": "",
    "text": "ggplot2 is a powerful tool for data visualization offered under the tidyverse umbrella.\nAs any scientist knows, visualizing your data is just as important as generating it. After all, how can you communicate your results if they’re hidden away in a table?\nThe R graph gallery has countless of great examples of plots created with ggplot2. Check them out!",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Tools",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#dplyr",
    "href": "reference_info/tools/r_packages.html#dplyr",
    "title": "R Packages",
    "section": "",
    "text": "Another tidyverse offering, dplyr is an essential package for data manipulation. Try out generating summary stats in a breeze with group_by() |&gt; summarise() or join two datasets with a left_join().",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Tools",
      "R Packages"
    ]
  },
  {
    "objectID": "reference_info/tools/r_packages.html#arrow",
    "href": "reference_info/tools/r_packages.html#arrow",
    "title": "R Packages",
    "section": "",
    "text": "arrow is an amazing tool for working with larger than memory data. R usually performs computations in RAM (your computer’s short term memory), but this can pose a problem for larger datasets. Arrow moves computations onto disk (your computer’s long term memory) to avoid this.\nFor a tutorial, see here",
    "crumbs": [
      "FADS Internal Wiki",
      "Reference Info",
      "Tools",
      "R Packages"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "🧰 Tools for Data Stewardship ⚙️📦",
    "section": "",
    "text": "Welcome to the Tools section of the DSU site! 🐟\nHere you’ll find a curated set of apps, platforms, scripts, and templates that support your work as a data steward or data producer within the Pacific Region Science Branch of Fisheries and Oceans Canada. 🧪🌊\n\n\n\nWe’re actively gathering, building, and documenting the most useful tools for the job — so stay tuned! In the meantime, here’s a sneak peek at what’s coming:\n\n\n\n🗂 Data Dictionary and Metadata Templates (Excel + JSON)\n🤝 Data Management Agreement Templates\n🧮 R & Python Functions, Scripts, and Packages for data cleaning and reporting\n🔗 Links to External Tools (See Internal DSU SharePoint Wiki for internal tools)\n🧠 Toolkits for Data Stewards (checklists, walkthroughs, best practices)\n\n\n\n\n\nIf you’ve built or used something you think could help others — whether it’s a handy Excel macro or a full-blown Shiny app — we’d love to include it here!\nSend us a note and let’s make it easier for everyone to work smarter. 🤝💡\n\n🧭 Return to the Internal DSU Wiki\n\nTools don’t solve problems — people do. But the right tool helps a lot. 🛠️🙂"
  },
  {
    "objectID": "tools.html#this-page-is-under-construction",
    "href": "tools.html#this-page-is-under-construction",
    "title": "🧰 Tools for Data Stewardship ⚙️📦",
    "section": "",
    "text": "We’re actively gathering, building, and documenting the most useful tools for the job — so stay tuned! In the meantime, here’s a sneak peek at what’s coming:\n\n\n\n🗂 Data Dictionary and Metadata Templates (Excel + JSON)\n🤝 Data Management Agreement Templates\n🧮 R & Python Functions, Scripts, and Packages for data cleaning and reporting\n🔗 Links to External Tools (See Internal DSU SharePoint Wiki for internal tools)\n🧠 Toolkits for Data Stewards (checklists, walkthroughs, best practices)\n\n\n\n\n\nIf you’ve built or used something you think could help others — whether it’s a handy Excel macro or a full-blown Shiny app — we’d love to include it here!\nSend us a note and let’s make it easier for everyone to work smarter. 🤝💡\n\n🧭 Return to the Internal DSU Wiki\n\nTools don’t solve problems — people do. But the right tool helps a lot. 🛠️🙂"
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html",
    "href": "how_to_guides/sharepoint_r.html",
    "title": "Access Data in SharePoint from R",
    "section": "",
    "text": "Connecting R to Sharepoint enables secure, programmatic data access for reproducible analysis workflows. This is especially useful to data stewards and scientists who want to move away from email and personal hard-drive based data storage and sharing and instead use a cloud service as a central repository for data.\nThis is particularly useful for data management tasks in Sharepoint, such as:\n\nDownloading data files for analysis\nUploading processed data files\nManaging document libraries and folders\nAutomating data workflows\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind, to access the DFO Sharepoint from R using the methods below you need to be on the DFO network or using the VPN.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html#conclusion",
    "href": "how_to_guides/sharepoint_r.html#conclusion",
    "title": "Access Data in SharePoint from R",
    "section": "Conclusion",
    "text": "Conclusion\nThis enables secure, programmatic data access for reproducible analysis workflows. Using the Microsoft365R package, you can easily access and manage data in SharePoint from R. This allows for a more streamlined and efficient workflow, especially for data stewards and scientists who work with large datasets and need to collaborate with others.\nThis approach can help you automate data management tasks, reduce reliance on email and personal storage, and improve collaboration within your team.",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "how_to_guides/sharepoint_r.html#additional-resources",
    "href": "how_to_guides/sharepoint_r.html#additional-resources",
    "title": "Access Data in SharePoint from R",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nMicrosoft365R Documentation",
    "crumbs": [
      "FADS Internal Wiki",
      "How to Guides",
      "Access Data in SharePoint from R"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the DSU",
    "section": "",
    "text": "The Data Stewardship Unit (DSU) is situated within the Fisheries and Assessment Data Section (FADS), of the Stock Assessment and Research Division of the Pacific Region Science Branch of Fisheries and Oceans Canada (DFO).\nWe are a dedicated team committed to enhancing data management and stewardship practices within DFO. Our primary focus is on data related to Pacific Salmon within the DFO Pacific Region.\nOur goal is to provide guidance and assistance throughout the entire lifecycle of data management—from planning to publication. We strive to ensure our practices align with modern data standards, such as those outlined in the DFO Data Strategy and the Government of Canada’s digital data strategy. Our work is guided by the principles of making data findable, accessible, interoperable, and reusable (FAIR). Additionally, we highly value the principles of Indigenous data governance, such as ownership, control, access, and possession (OCAP), and we are committed to helping our collaborators respect and implement these principles in their work."
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html",
    "href": "tutorials/support_controlled_vocabs.html",
    "title": "Support Controlled Vocabularies",
    "section": "",
    "text": "The scientist downloads and completes the data dictionary template (Excel)\n\nThey describe each variable: name, definition, method, units, etc.\nThey may include URIs from known vocabularies if possible\n\nThe spreadsheet includes:\n\nA README sheet with instructions\nA Metadata tab for project-level context\nA Data Dictionary tab with one row per variable\n\n\n\n\n\n\nThe data dictionary is passed into your R validation function (validate_data_dictionary())\nIt compares submitted variables to your reference vocabulary, hosted as JSON on GitHub Pages\nThe validator returns:\n\nMatched terms\nUnmatched terms\nSuggested mappings using fuzzy logic (e.g., stringdist or Jaro-Winkler)\n\nThe scientist or steward reviews and adjusts terms as needed\n\n\n\n\n\nFor each submitted or candidate term:\n\nUse an automated lookup tool to check if a concept already exists in the NCEAS Salmon Ontology. Start by using the BioPortal Annotator or API to match term labels and definitions. Optionally, build a lightweight script that compares local vocabulary terms to the ontology by label, synonyms, or definition text using exact or fuzzy string matching. Consider integrating the result into your pipeline for batch validation.\nIf it exists:\n\nUse skos:exactMatch or skos:closeMatch in your .ttl (Turtle) or .jsonld RDF vocab files. These mappings are typically stored in the ontology files generated from your .csv source. Add a column like exact_match_uri or close_match_uri to your .csv, your export script can incorporate those values automatically during serialization to .ttl or .jsonld.\n\nIf it doesn’t exist, but is broadly applicable:\n\nPrepare a Pull Request or issue to suggest the term to the NCEAS maintainers\nInclude:\n\nLabel, definition, synonyms\nUsage example or data sources\n\n\n\n\n\n\n\n\nMaintain DFO vocabularies in GitHub as:\n\n.csv for editing\n.json for R/python access and validation\n.jsonld and .ttl for ontology use\n\nAssign persistent URIs using https://w3id.org/dfo/spsi#term-id\nOrganize vocabularies by data domain or project (e.g., monitoring, habitat, biology)\n\n\n\n\n\nExtend your vocabularies with relationships:\n\nbroader, narrower, related\nhasUnit, hasMethod, derivedFrom\n\nUse tools like rdflib or skosify to model in Turtle or OWL\nAdd namespace info:\n\n@prefix dfo-spsi: &lt;https://w3id.org/dfo/spsi#&gt;\n\nOver time, link this to:\n\nNCEAS ontology\nNERC units\nENVO/OBI for environment/sample methods\n\n\n\n\n\n\nHost vocab on GitHub Pages + register URIs at w3id.org\nLink from your Quarto site (/data-standards/vocab-index.html)\nAllow download in multiple formats (JSON, JSON-LD, TTL)\nUse vocab in:\n\nData validation tools\nMetadata editors\nAPIs and dashboards\nAI pipelines (semantic RAG, search, integration)\n\n\n\nThis workflow will enable DFO Pacific to build a structured, version-controlled, and interoperable vocabulary and ontology infrastructure for salmon data, with clear ties to domain-wide standards and extensibility for AI use cases.",
    "crumbs": [
      "FADS Internal Wiki",
      "Tutorials",
      "Support Controlled Vocabularies"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#guide-end-to-end-process-for-supporting-controlled-vocabularies-in-dfo",
    "href": "tutorials/support_controlled_vocabs.html#guide-end-to-end-process-for-supporting-controlled-vocabularies-in-dfo",
    "title": "Support Controlled Vocabularies",
    "section": "",
    "text": "The scientist downloads and completes the data dictionary template (Excel)\n\nThey describe each variable: name, definition, method, units, etc.\nThey may include URIs from known vocabularies if possible\n\nThe spreadsheet includes:\n\nA README sheet with instructions\nA Metadata tab for project-level context\nA Data Dictionary tab with one row per variable\n\n\n\n\n\n\nThe data dictionary is passed into your R validation function (validate_data_dictionary())\nIt compares submitted variables to your reference vocabulary, hosted as JSON on GitHub Pages\nThe validator returns:\n\nMatched terms\nUnmatched terms\nSuggested mappings using fuzzy logic (e.g., stringdist or Jaro-Winkler)\n\nThe scientist or steward reviews and adjusts terms as needed\n\n\n\n\n\nFor each submitted or candidate term:\n\nUse an automated lookup tool to check if a concept already exists in the NCEAS Salmon Ontology. Start by using the BioPortal Annotator or API to match term labels and definitions. Optionally, build a lightweight script that compares local vocabulary terms to the ontology by label, synonyms, or definition text using exact or fuzzy string matching. Consider integrating the result into your pipeline for batch validation.\nIf it exists:\n\nUse skos:exactMatch or skos:closeMatch in your .ttl (Turtle) or .jsonld RDF vocab files. These mappings are typically stored in the ontology files generated from your .csv source. Add a column like exact_match_uri or close_match_uri to your .csv, your export script can incorporate those values automatically during serialization to .ttl or .jsonld.\n\nIf it doesn’t exist, but is broadly applicable:\n\nPrepare a Pull Request or issue to suggest the term to the NCEAS maintainers\nInclude:\n\nLabel, definition, synonyms\nUsage example or data sources\n\n\n\n\n\n\n\n\nMaintain DFO vocabularies in GitHub as:\n\n.csv for editing\n.json for R/python access and validation\n.jsonld and .ttl for ontology use\n\nAssign persistent URIs using https://w3id.org/dfo/spsi#term-id\nOrganize vocabularies by data domain or project (e.g., monitoring, habitat, biology)\n\n\n\n\n\nExtend your vocabularies with relationships:\n\nbroader, narrower, related\nhasUnit, hasMethod, derivedFrom\n\nUse tools like rdflib or skosify to model in Turtle or OWL\nAdd namespace info:\n\n@prefix dfo-spsi: &lt;https://w3id.org/dfo/spsi#&gt;\n\nOver time, link this to:\n\nNCEAS ontology\nNERC units\nENVO/OBI for environment/sample methods\n\n\n\n\n\n\nHost vocab on GitHub Pages + register URIs at w3id.org\nLink from your Quarto site (/data-standards/vocab-index.html)\nAllow download in multiple formats (JSON, JSON-LD, TTL)\nUse vocab in:\n\nData validation tools\nMetadata editors\nAPIs and dashboards\nAI pipelines (semantic RAG, search, integration)\n\n\n\nThis workflow will enable DFO Pacific to build a structured, version-controlled, and interoperable vocabulary and ontology infrastructure for salmon data, with clear ties to domain-wide standards and extensibility for AI use cases.",
    "crumbs": [
      "FADS Internal Wiki",
      "Tutorials",
      "Support Controlled Vocabularies"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#why-bother-with-controlled-vocabularies-and-ontologies",
    "href": "tutorials/support_controlled_vocabs.html#why-bother-with-controlled-vocabularies-and-ontologies",
    "title": "Support Controlled Vocabularies",
    "section": "🤔 Why Bother with Controlled Vocabularies and Ontologies?",
    "text": "🤔 Why Bother with Controlled Vocabularies and Ontologies?\nIt’s a fair question. Why invest all this time defining terms, aligning with ontologies, and assigning URIs?\nBecause it unlocks a future where your data works harder for you. Controlled vocabularies and ontologies aren’t just academic exercises — they’re the foundation for better discovery, smarter integration, and automation across the science lifecycle.\nHere are some tangible ways this work adds value:\n\n\n🔍 1. Enhanced Data Discovery\n\n“I know someone collected salmon smolt data… but where do I find it?”\n\nBy tagging datasets, columns, and metadata with terms from your SPSI vocabulary, you enable: - Keyword search that actually understands synonyms and related terms - Filters based on data type, units, methods, or ecological domain - Smart discovery interfaces (e.g., “Show me everything related to juvenile survival”)\n➡️ Example: A data catalog that lets users search by controlled term rather than inconsistent column names across spreadsheets.\n\n\n\n🔄 2. Semi-Automated Data Integration\n\n“These datasets report the same metric but use different terms, formats, or units.”\n\nUsing controlled terms with defined relationships (e.g., broader, exactMatch, hasUnit), you can: - Detect overlapping fields across submissions automatically - Align columns and units across multiple datasets - Standardize value domains (e.g., Red/Green/Amber vs Critical/Stable/Concerned)\n➡️ Example: A script that reads in new FSAR data and automatically maps it to the SPSR schema for validation and loading into a database.\n\n\n\n🧠 3. Smarter Applications (AI & Beyond)\n\n“Can’t AI just figure this stuff out?”\n\nOnly if you feed it structure.\nControlled vocabularies and ontologies allow you to: - Ground large language models (LLMs) in your domain’s specific terminology - Build Retrieval-Augmented Generation (RAG) systems for question answering - Use semantic search tools to find relevant variables, concepts, and datasets - Train AI to assist in metadata generation, anomaly detection, or dataset classification\n➡️ Example: A chatbot that helps scientists describe their data using your vocabulary, or recommends matching fields from existing standards.\n\n\n\n🔗 4. Future-Proof Interoperability\n\n“What if we want to share this with other agencies or join a broader platform?”\n\nStandardized vocabularies with persistent URIs and ontology alignments (e.g., with NCEAS, ENVO, OBO) make it easy to: - Share DFO terms with external partners - Convert metadata to international schemas (e.g., Darwin Core, ISO 19115) - Plug into federated data platforms without starting over\n➡️ Example: Publishing your vocabulary to w3id.org lets others reference and reuse your terms as global identifiers.\n\n\n\n🧰 5. Better Metadata, Better Stewardship\n\n“I just want people to fill out metadata that makes sense.”\n\nControlled vocabularies: - Reduce ambiguity - Improve machine readability - Make metadata easier to validate and automate\n➡️ Example: A dropdown menu in a data intake form linked to your vocab that auto-fills units, definitions, and example values — while storing clean, machine-readable metadata under the hood.",
    "crumbs": [
      "FADS Internal Wiki",
      "Tutorials",
      "Support Controlled Vocabularies"
    ]
  },
  {
    "objectID": "tutorials/support_controlled_vocabs.html#tldr",
    "href": "tutorials/support_controlled_vocabs.html#tldr",
    "title": "Support Controlled Vocabularies",
    "section": "🧭 TL;DR",
    "text": "🧭 TL;DR\nControlled vocabularies and ontologies are not the end — they’re the beginning.\nThey allow you to: - Find data more easily - Integrate it more reliably - Build tools more effectively - And use AI more meaningfully\nThey turn your data into infrastructure.\nAnd they help ensure that knowledge created today can be used, re-used, and trusted — long into the future.",
    "crumbs": [
      "FADS Internal Wiki",
      "Tutorials",
      "Support Controlled Vocabularies"
    ]
  },
  {
    "objectID": "documentation_hub/how_to_guides/databricks_r_studio.html",
    "href": "documentation_hub/how_to_guides/databricks_r_studio.html",
    "title": "FADS Open Science Documentation Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/how_to_guides/databricks_r_studio.html"
  },
  {
    "objectID": "dsu_documentation.html",
    "href": "dsu_documentation.html",
    "title": "FADS Open Science Documentation Hub",
    "section": "",
    "text": "Redirecting to https://dfo-pacific-science.github.io/data-stewardship-unit/index.html"
  }
]