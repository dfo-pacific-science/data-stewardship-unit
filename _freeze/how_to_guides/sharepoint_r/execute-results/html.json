{
  "hash": "b492b0b11859541304a022c0ef2fc25f",
  "result": {
    "engine": "knitr",
    "markdown": "::: {.cell}\n\n```{.r .cell-code}\nlibrary('Microsoft365R')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'Microsoft365R' was built under R version 4.4.3\n```\n\n\n:::\n\n```{.r .cell-code}\nlist_sharepoint_sites()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading Microsoft Graph login for default tenant\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nAccess token has expired or is no longer valid; refreshing\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n<Sharepoint site 'Fishery & Assessment Data Section Wiki-Pacific Salmon Data Community of Practice'>\n  directory id: 086gc.sharepoint.com,7dbad11f-37f1-425d-89a5-494db3b115d6,e006fda1-3eba-47d7-808a-5e49d28259ee \n  web link: https://086gc.sharepoint.com/sites/FisheryAssessmentDataSectionWiki-PacificSalmonDataCommunityofPractice \n  description:  \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[2]]\n<Sharepoint site 'Fishery & Assessment Data Section Wiki'>\n  directory id: 086gc.sharepoint.com,de3aa7ea-4d76-4401-afe5-2821d69203f6,5223c0be-402e-4760-9576-16a53459c796 \n  web link: https://086gc.sharepoint.com/sites/FisheryAssessmentDataSectionWiki \n  description: A public Team and Sharepoint to widely distribute and communicate the services available from the Fishery & Assessment Data Section along with how to guides, code examples and documentation. \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[3]]\n<Sharepoint site 'Cultus Lake Labs'>\n  directory id: 086gc.sharepoint.com,4923bcf1-96dc-450d-a0a3-11b7b540272d,5e32aa3a-8576-4c81-8f04-0d47426d5316 \n  web link: https://086gc.sharepoint.com/sites/CultusLakeLabs \n  description: Teams site for everything related to Cultus Lake Laboratories  \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[4]]\n<Sharepoint site 'Pacific Region Data Community-PSDCoP - Pacific Salmon Data Community of Practice'>\n  directory id: 086gc.sharepoint.com,982cbc2f-f902-4890-8e44-f2a6f00593cf,251b2a53-cc2a-42ea-8aac-c5ebb6f1691e \n  web link: https://086gc.sharepoint.com/sites/PacificRegionDataCommunity-PacificSalmonDataCommunityofPracticePSDCoP \n  description:  \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[5]]\n<Sharepoint site 'Generative AI Pilots'>\n  directory id: 086gc.sharepoint.com,1d640383-0ef9-4176-8bc9-68a3cbe781c6,dff43d12-a184-4702-89aa-60448a5aa5db \n  web link: https://086gc.sharepoint.com/sites/GenerativeAIPilots \n  description: A number of pilots are running in DFO cloud that allow citizen data scientists, data scientists, ML engineers and other hands-on users to work various GenAI tools. This is the place for them share ideas and discuss issues. \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[6]]\n<Sharepoint site 'EDH Zone/Zone CDE'>\n  directory id: 086gc.sharepoint.com,196f65c0-0142-486e-8be9-aef3e2bd9fa6,6368f14c-b849-4295-81ba-b640c0031c4b \n  web link: https://086gc.sharepoint.com/sites/EDHCentreCentreCDE \n  description: EDH Zone/Zone CDE \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[7]]\n<Sharepoint site 'Pacific Region Data Community'>\n  directory id: 086gc.sharepoint.com,a4504e28-7e9d-461d-8c35-d94c70b53823,046fef14-20c1-4c33-8a18-7a127163e53a \n  web link: https://086gc.sharepoint.com/sites/StrategicDataCommittee \n  description: Pacific Region Data Community bring together the data community across the Pacific region. \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n\n[[8]]\n<Sharepoint site 'Fishery & Assessment Data Section - Data Stewardship Unit'>\n  directory id: 086gc.sharepoint.com,488e38a1-9d63-4f1f-bafc-4136a7021908,5700330b-a811-4e74-b480-f519eda229c1 \n  web link: https://086gc.sharepoint.com/sites/Fishery-Assessment_Data_Section-DataStewardshipUnit2 \n  description:  \n---\n  Methods:\n    delete, do_operation, get_drive, get_group, get_list,\n    get_list_pager, get_lists, list_drives, list_subsites,\n    sync_fields, update\n```\n\n\n:::\n\n```{.r .cell-code}\n# Delete cached token manually if you get an error about MFA\n#unlink(\"~/.Microsoft365R\", recursive = TRUE)\n\nsite <- get_sharepoint_site('Fishery & Assessment Data Section Wiki')\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading Microsoft Graph login for default tenant\n```\n\n\n:::\n\n```{.r .cell-code}\n# document libraries\nsite$list_drives()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n<Document library 'Documents'>\n  directory id: b!6qc63nZNAUSv5Sgh1pID9r7AI1IuQGBHlXYWpTRZx5ZWf10oNpdEQrxI7pHuwIys \n  web link: https://086gc.sharepoint.com/sites/FisheryAssessmentDataSectionWiki/Shared%20Documents \n  description:  \n---\n  Methods:\n    copy_item, create_folder, create_share_link, delete,\n    delete_item, do_operation, download_file, download_folder,\n    get_item, get_item_properties, get_list_pager, list_files,\n    list_items, list_shared_files, list_shared_items,\n    load_dataframe, load_rdata, load_rds, move_item, open_item,\n    save_dataframe, save_rdata, save_rds, set_item_properties,\n    sync_fields, update, upload_file, upload_folder\n```\n\n\n:::\n\n```{.r .cell-code}\ndrv <- site$get_drive()\n\n# Note: I manually put this data into the FADS Wiki SharePoint to illustrate the workflow of FSAR authors uploading data here\n\n# Download file from FADS Wiki SharePoint\ndrv$download_file(\"General/FSAR Data/Fraser Pinks/data/generated/benchmarks.csv\", overwrite = TRUE)\n\nfr_pk_bms <- readr::read_csv(\"benchmarks.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\n• `` -> `...1`\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 3 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): ...1\ndbl (3): median, lower 95% CI, upper 95% CI\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n\n\n# Write this raw data file to Bronze\n\nJust kidding this has to be done in a Databricks Notebook if I want to write a .csv. I can only write delta tables into a database schema from R locally using RJDBC. Which might also work. Using a Databrick notebook would be annoying because I can't read from SharePoint in DataBricks. I could set up pipeline in Azure Data Factory to read from SharePoint, transform, and write to Lakehouse but it seems complicated and maybe overkill.\n\nWe could also just read from SharePoint and write to SharePoint silver folder... Makes provisioning access to silver data easier perhaps...\n\nSee miro diagram https://miro.com/app/board/uXjVIJyjBWk=/?share_link_id=763283318229\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(RJDBC)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: DBI\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: rJava\n```\n\n\n:::\n\n```{.r .cell-code}\njdbc_driver <- JDBC(\"com.databricks.client.jdbc.Driver\", \"C:/Users/JOHNSONBRE/DatabricksJDBC42.jar\", \"\")\n\nurlsql <- \"jdbc:databricks://adb-553282681420861.1.azuredatabricks.net:443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/613056ec98d47d29;\"\n\npat <- Sys.getenv(\"DATABRICKS_PAT\")\n\nconnsql <- RJDBC::dbConnect(jdbc_driver, urlsql, \"token\", pat)\ndbGetQuery(connsql, \"SELECT 1\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  1\n1 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# see what catalog are available\ndbGetQuery(connsql, \"SHOW CATALOGS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                          catalog\n1             ais_service_catalog\n2             bronze_pacific_prod\n3   bronze_pacific_prod_dataverse\n4      configuration_pacific_prod\n5               gold_pacific_prod\n6                  hive_metastore\n7                            main\n8                         samples\n9             silver_pacific_prod\n10 staging_pacific_prod_dataverse\n11                         system\n```\n\n\n:::\n\n```{.r .cell-code}\ndbGetQuery(connsql, \"SHOW SCHEMAS IN bronze_pacific_prod\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        databaseName\n1            default\n2           fos_v1_1\n3 information_schema\n4              krest\n5        nuseds_v2_0\n```\n\n\n:::\n:::\n\n\n\n# Read data from Bronze\n\n# Transform data to meet SPSR schema\n\n# Write to silver\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}